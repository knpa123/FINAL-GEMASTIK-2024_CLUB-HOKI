2024-09-23 13:52:36,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 13:52:36,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 13:52:36,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 13:52:36,991:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 13:52:38,626:INFO:PyCaret ClassificationExperiment
2024-09-23 13:52:38,626:INFO:Logging name: clf-default-name
2024-09-23 13:52:38,627:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 13:52:38,627:INFO:version 3.3.1
2024-09-23 13:52:38,627:INFO:Initializing setup()
2024-09-23 13:52:38,627:INFO:self.USI: f51c
2024-09-23 13:52:38,627:INFO:self._variable_keys: {'_ml_usecase', 'y_test', 'X_test', 'is_multiclass', 'target_param', 'USI', 'exp_name_log', 'logging_param', 'idx', 'gpu_n_jobs_param', 'pipeline', 'fold_generator', 'seed', 'html_param', 'data', 'y', 'fold_groups_param', 'X_train', 'log_plots_param', 'fix_imbalance', 'exp_id', 'fold_shuffle_param', 'memory', 'n_jobs_param', 'y_train', '_available_plots', 'X', 'gpu_param'}
2024-09-23 13:52:38,628:INFO:Checking environment
2024-09-23 13:52:38,628:INFO:python_version: 3.11.9
2024-09-23 13:52:38,628:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 13:52:38,628:INFO:machine: AMD64
2024-09-23 13:52:38,628:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 13:52:38,628:INFO:Memory: svmem(total=16853458944, available=7166267392, percent=57.5, used=9687191552, free=7166267392)
2024-09-23 13:52:38,628:INFO:Physical Core: 16
2024-09-23 13:52:38,628:INFO:Logical Core: 24
2024-09-23 13:52:38,628:INFO:Checking libraries
2024-09-23 13:52:38,629:INFO:System:
2024-09-23 13:52:38,629:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 13:52:38,629:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 13:52:38,629:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 13:52:38,630:INFO:PyCaret required dependencies:
2024-09-23 13:52:38,690:INFO:                 pip: 24.2
2024-09-23 13:52:38,706:INFO:          setuptools: 75.1.0
2024-09-23 13:52:38,706:INFO:             pycaret: 3.3.1
2024-09-23 13:52:38,706:INFO:             IPython: 8.27.0
2024-09-23 13:52:38,706:INFO:          ipywidgets: 8.1.2
2024-09-23 13:52:38,706:INFO:                tqdm: 4.66.5
2024-09-23 13:52:38,706:INFO:               numpy: 1.26.4
2024-09-23 13:52:38,706:INFO:              pandas: 2.1.4
2024-09-23 13:52:38,706:INFO:              jinja2: 3.1.4
2024-09-23 13:52:38,707:INFO:               scipy: 1.11.4
2024-09-23 13:52:38,707:INFO:              joblib: 1.2.0
2024-09-23 13:52:38,707:INFO:             sklearn: 1.4.2
2024-09-23 13:52:38,707:INFO:                pyod: 2.0.2
2024-09-23 13:52:38,707:INFO:            imblearn: 0.12.3
2024-09-23 13:52:38,707:INFO:   category_encoders: 2.6.3
2024-09-23 13:52:38,707:INFO:            lightgbm: 4.5.0
2024-09-23 13:52:38,707:INFO:               numba: 0.60.0
2024-09-23 13:52:38,707:INFO:            requests: 2.32.3
2024-09-23 13:52:38,707:INFO:          matplotlib: 3.9.2
2024-09-23 13:52:38,707:INFO:          scikitplot: 0.3.7
2024-09-23 13:52:38,707:INFO:         yellowbrick: 1.5
2024-09-23 13:52:38,707:INFO:              plotly: 5.24.1
2024-09-23 13:52:38,707:INFO:    plotly-resampler: Not installed
2024-09-23 13:52:38,707:INFO:             kaleido: 0.2.1
2024-09-23 13:52:38,707:INFO:           schemdraw: 0.15
2024-09-23 13:52:38,707:INFO:         statsmodels: 0.14.2
2024-09-23 13:52:38,707:INFO:              sktime: 0.26.0
2024-09-23 13:52:38,707:INFO:               tbats: 1.1.3
2024-09-23 13:52:38,707:INFO:            pmdarima: 2.0.4
2024-09-23 13:52:38,707:INFO:              psutil: 5.9.0
2024-09-23 13:52:38,707:INFO:          markupsafe: 2.1.3
2024-09-23 13:52:38,707:INFO:             pickle5: Not installed
2024-09-23 13:52:38,707:INFO:         cloudpickle: 3.0.0
2024-09-23 13:52:38,707:INFO:         deprecation: 2.1.0
2024-09-23 13:52:38,707:INFO:              xxhash: 2.0.2
2024-09-23 13:52:38,707:INFO:           wurlitzer: 3.1.1
2024-09-23 13:52:38,707:INFO:PyCaret optional dependencies:
2024-09-23 13:52:38,773:INFO:                shap: Not installed
2024-09-23 13:52:38,773:INFO:           interpret: Not installed
2024-09-23 13:52:38,773:INFO:                umap: Not installed
2024-09-23 13:52:38,773:INFO:     ydata_profiling: Not installed
2024-09-23 13:52:38,773:INFO:  explainerdashboard: Not installed
2024-09-23 13:52:38,773:INFO:             autoviz: Not installed
2024-09-23 13:52:38,773:INFO:           fairlearn: Not installed
2024-09-23 13:52:38,773:INFO:          deepchecks: Not installed
2024-09-23 13:52:38,773:INFO:             xgboost: 2.1.1
2024-09-23 13:52:38,773:INFO:            catboost: 1.2.7
2024-09-23 13:52:38,773:INFO:              kmodes: Not installed
2024-09-23 13:52:38,773:INFO:             mlxtend: Not installed
2024-09-23 13:52:38,773:INFO:       statsforecast: Not installed
2024-09-23 13:52:38,773:INFO:        tune_sklearn: Not installed
2024-09-23 13:52:38,773:INFO:                 ray: Not installed
2024-09-23 13:52:38,773:INFO:            hyperopt: Not installed
2024-09-23 13:52:38,773:INFO:              optuna: Not installed
2024-09-23 13:52:38,773:INFO:               skopt: Not installed
2024-09-23 13:52:38,773:INFO:              mlflow: Not installed
2024-09-23 13:52:38,773:INFO:              gradio: Not installed
2024-09-23 13:52:38,773:INFO:             fastapi: Not installed
2024-09-23 13:52:38,773:INFO:             uvicorn: Not installed
2024-09-23 13:52:38,773:INFO:              m2cgen: Not installed
2024-09-23 13:52:38,773:INFO:           evidently: Not installed
2024-09-23 13:52:38,773:INFO:               fugue: Not installed
2024-09-23 13:52:38,773:INFO:           streamlit: Not installed
2024-09-23 13:52:38,773:INFO:             prophet: Not installed
2024-09-23 13:52:38,773:INFO:None
2024-09-23 13:52:38,773:INFO:Set up data.
2024-09-23 13:52:38,805:INFO:Set up folding strategy.
2024-09-23 13:52:38,805:INFO:Set up train/test split.
2024-09-23 13:52:38,836:INFO:Set up index.
2024-09-23 13:52:38,836:INFO:Assigning column types.
2024-09-23 13:52:38,836:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 13:52:38,871:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 13:52:38,875:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 13:52:38,891:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:38,891:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:39,002:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 13:52:39,002:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 13:52:39,018:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:39,018:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:39,018:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 13:52:39,034:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 13:52:39,049:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:39,049:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:39,085:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 13:52:39,100:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:39,101:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:39,102:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 13:52:39,140:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:39,140:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:39,218:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:39,218:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:39,227:INFO:Preparing preprocessing pipeline...
2024-09-23 13:52:39,234:INFO:Set up simple imputation.
2024-09-23 13:52:39,242:INFO:Set up encoding of ordinal features.
2024-09-23 13:52:39,250:INFO:Set up encoding of categorical features.
2024-09-23 13:52:39,250:INFO:Set up column name cleaning.
2024-09-23 13:52:39,689:INFO:Finished creating preprocessing pipeline.
2024-09-23 13:52:39,727:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Jumlah Tanggungan dalam Keluarga',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Jumlah (Perkiraan Harga Pasar) '
                                             'Kendaraan yang Dimiliki',
                                             'Daya Listrik (Watt)',
                                             'Luas Tanah Rumah (m2)',
                                             'Luas Bangunan Rumah (m2)',
                                             'NJOP per Meter',
                                             'Rerat...
                                                                    'Rumah',
                                                                    'Bahan '
                                                                    'Lantai '
                                                                    'Rumah',
                                                                    'Program '
                                                                    'Studi',
                                                                    'Kota'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 13:52:39,728:INFO:Creating final display dataframe.
2024-09-23 13:52:40,435:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 28)
4        Transformed data shape       (30380, 48)
5   Transformed train set shape       (21266, 48)
6    Transformed test set shape        (9114, 48)
7              Numeric features                11
8          Categorical features                16
9      Rows with missing values             24.6%
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              f51c
2024-09-23 13:52:40,493:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:40,495:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:40,524:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 13:52:40,524:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 13:52:40,524:INFO:setup() successfully completed in 1.91s...............
2024-09-23 13:52:48,811:INFO:Initializing compare_models()
2024-09-23 13:52:48,811:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 13:52:48,811:INFO:Checking exceptions
2024-09-23 13:52:48,829:INFO:Preparing display monitor
2024-09-23 13:52:48,843:INFO:Initializing Logistic Regression
2024-09-23 13:52:48,843:INFO:Total runtime is 0.0 minutes
2024-09-23 13:52:48,845:INFO:SubProcess create_model() called ==================================
2024-09-23 13:52:48,845:INFO:Initializing create_model()
2024-09-23 13:52:48,845:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:52:48,845:INFO:Checking exceptions
2024-09-23 13:52:48,845:INFO:Importing libraries
2024-09-23 13:52:48,845:INFO:Copying training dataset
2024-09-23 13:52:48,848:INFO:Defining folds
2024-09-23 13:52:48,848:INFO:Declaring metric variables
2024-09-23 13:52:48,848:INFO:Importing untrained model
2024-09-23 13:52:48,858:INFO:Logistic Regression Imported successfully
2024-09-23 13:52:48,861:INFO:Starting cross validation
2024-09-23 13:52:48,863:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:52:59,860:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:52:59,938:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:52:59,938:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,116:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,179:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,179:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,194:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,280:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,462:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,478:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,532:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,537:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,542:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,542:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,575:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,575:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,708:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,757:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,758:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,841:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,873:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:00,888:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,891:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:00,908:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:00,908:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:01,007:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-23 13:53:01,051:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:01,056:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:01,060:INFO:Calculating mean and std
2024-09-23 13:53:01,060:INFO:Creating metrics dataframe
2024-09-23 13:53:01,060:INFO:Uploading results into container
2024-09-23 13:53:01,060:INFO:Uploading model into container now
2024-09-23 13:53:01,060:INFO:_master_model_container: 1
2024-09-23 13:53:01,060:INFO:_display_container: 2
2024-09-23 13:53:01,060:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 13:53:01,060:INFO:create_model() successfully completed......................................
2024-09-23 13:53:01,126:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:01,126:INFO:Creating metrics dataframe
2024-09-23 13:53:01,126:INFO:Initializing K Neighbors Classifier
2024-09-23 13:53:01,126:INFO:Total runtime is 0.2047174533208211 minutes
2024-09-23 13:53:01,126:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:01,126:INFO:Initializing create_model()
2024-09-23 13:53:01,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:01,126:INFO:Checking exceptions
2024-09-23 13:53:01,126:INFO:Importing libraries
2024-09-23 13:53:01,126:INFO:Copying training dataset
2024-09-23 13:53:01,145:INFO:Defining folds
2024-09-23 13:53:01,145:INFO:Declaring metric variables
2024-09-23 13:53:01,145:INFO:Importing untrained model
2024-09-23 13:53:01,145:INFO:K Neighbors Classifier Imported successfully
2024-09-23 13:53:01,145:INFO:Starting cross validation
2024-09-23 13:53:01,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:03,723:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:03,802:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:03,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:03,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:03,849:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:03,888:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:03,890:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:03,936:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:03,936:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:03,936:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,003:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,003:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,003:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:04,003:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,003:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:04,015:INFO:Calculating mean and std
2024-09-23 13:53:04,015:INFO:Creating metrics dataframe
2024-09-23 13:53:04,015:INFO:Uploading results into container
2024-09-23 13:53:04,015:INFO:Uploading model into container now
2024-09-23 13:53:04,015:INFO:_master_model_container: 2
2024-09-23 13:53:04,015:INFO:_display_container: 2
2024-09-23 13:53:04,015:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 13:53:04,015:INFO:create_model() successfully completed......................................
2024-09-23 13:53:04,073:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:04,073:INFO:Creating metrics dataframe
2024-09-23 13:53:04,089:INFO:Initializing Naive Bayes
2024-09-23 13:53:04,089:INFO:Total runtime is 0.2541017413139343 minutes
2024-09-23 13:53:04,089:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:04,089:INFO:Initializing create_model()
2024-09-23 13:53:04,089:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:04,089:INFO:Checking exceptions
2024-09-23 13:53:04,089:INFO:Importing libraries
2024-09-23 13:53:04,089:INFO:Copying training dataset
2024-09-23 13:53:04,093:INFO:Defining folds
2024-09-23 13:53:04,093:INFO:Declaring metric variables
2024-09-23 13:53:04,093:INFO:Importing untrained model
2024-09-23 13:53:04,093:INFO:Naive Bayes Imported successfully
2024-09-23 13:53:04,107:INFO:Starting cross validation
2024-09-23 13:53:04,109:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:04,683:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,729:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,741:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,776:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:04,790:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:06,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:06,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:06,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:06,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:06,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:06,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:06,138:INFO:Calculating mean and std
2024-09-23 13:53:06,138:INFO:Creating metrics dataframe
2024-09-23 13:53:06,138:INFO:Uploading results into container
2024-09-23 13:53:06,138:INFO:Uploading model into container now
2024-09-23 13:53:06,138:INFO:_master_model_container: 3
2024-09-23 13:53:06,138:INFO:_display_container: 2
2024-09-23 13:53:06,138:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 13:53:06,138:INFO:create_model() successfully completed......................................
2024-09-23 13:53:06,207:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:06,207:INFO:Creating metrics dataframe
2024-09-23 13:53:06,207:INFO:Initializing Decision Tree Classifier
2024-09-23 13:53:06,207:INFO:Total runtime is 0.2893944541613261 minutes
2024-09-23 13:53:06,207:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:06,207:INFO:Initializing create_model()
2024-09-23 13:53:06,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:06,207:INFO:Checking exceptions
2024-09-23 13:53:06,207:INFO:Importing libraries
2024-09-23 13:53:06,207:INFO:Copying training dataset
2024-09-23 13:53:06,227:INFO:Defining folds
2024-09-23 13:53:06,227:INFO:Declaring metric variables
2024-09-23 13:53:06,227:INFO:Importing untrained model
2024-09-23 13:53:06,227:INFO:Decision Tree Classifier Imported successfully
2024-09-23 13:53:06,227:INFO:Starting cross validation
2024-09-23 13:53:06,242:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:07,019:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,058:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,080:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,080:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,124:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,140:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,140:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,173:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,204:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,251:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:07,251:INFO:Calculating mean and std
2024-09-23 13:53:07,251:INFO:Creating metrics dataframe
2024-09-23 13:53:07,251:INFO:Uploading results into container
2024-09-23 13:53:07,251:INFO:Uploading model into container now
2024-09-23 13:53:07,251:INFO:_master_model_container: 4
2024-09-23 13:53:07,251:INFO:_display_container: 2
2024-09-23 13:53:07,251:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 13:53:07,251:INFO:create_model() successfully completed......................................
2024-09-23 13:53:07,324:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:07,324:INFO:Creating metrics dataframe
2024-09-23 13:53:07,324:INFO:Initializing SVM - Linear Kernel
2024-09-23 13:53:07,324:INFO:Total runtime is 0.3080099105834961 minutes
2024-09-23 13:53:07,324:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:07,324:INFO:Initializing create_model()
2024-09-23 13:53:07,339:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:07,339:INFO:Checking exceptions
2024-09-23 13:53:07,339:INFO:Importing libraries
2024-09-23 13:53:07,339:INFO:Copying training dataset
2024-09-23 13:53:07,345:INFO:Defining folds
2024-09-23 13:53:07,345:INFO:Declaring metric variables
2024-09-23 13:53:07,345:INFO:Importing untrained model
2024-09-23 13:53:07,345:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 13:53:07,345:INFO:Starting cross validation
2024-09-23 13:53:07,358:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:08,343:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,343:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,343:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,359:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,392:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,392:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,483:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,485:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,490:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,490:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,490:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,521:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,521:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,537:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,537:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:08,537:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,537:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:08,537:INFO:Calculating mean and std
2024-09-23 13:53:08,553:INFO:Creating metrics dataframe
2024-09-23 13:53:08,553:INFO:Uploading results into container
2024-09-23 13:53:08,553:INFO:Uploading model into container now
2024-09-23 13:53:08,553:INFO:_master_model_container: 5
2024-09-23 13:53:08,553:INFO:_display_container: 2
2024-09-23 13:53:08,553:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 13:53:08,553:INFO:create_model() successfully completed......................................
2024-09-23 13:53:08,599:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:08,599:INFO:Creating metrics dataframe
2024-09-23 13:53:08,615:INFO:Initializing Ridge Classifier
2024-09-23 13:53:08,615:INFO:Total runtime is 0.3295343518257141 minutes
2024-09-23 13:53:08,624:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:08,624:INFO:Initializing create_model()
2024-09-23 13:53:08,625:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:08,625:INFO:Checking exceptions
2024-09-23 13:53:08,625:INFO:Importing libraries
2024-09-23 13:53:08,625:INFO:Copying training dataset
2024-09-23 13:53:08,625:INFO:Defining folds
2024-09-23 13:53:08,625:INFO:Declaring metric variables
2024-09-23 13:53:08,640:INFO:Importing untrained model
2024-09-23 13:53:08,642:INFO:Ridge Classifier Imported successfully
2024-09-23 13:53:08,645:INFO:Starting cross validation
2024-09-23 13:53:08,647:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:09,164:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=5.14151e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,195:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.85958e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,218:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,220:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,220:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,224:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,242:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,242:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=5.26791e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,242:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,284:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.86083e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,321:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,321:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,337:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=5.43754e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,337:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,353:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.87961e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,353:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,353:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.91551e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,368:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.79362e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,368:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\linear_model\_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=5.00246e-21): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2024-09-23 13:53:09,384:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,400:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,400:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,400:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,415:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,415:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,415:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,415:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,431:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:09,431:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:09,431:INFO:Calculating mean and std
2024-09-23 13:53:09,431:INFO:Creating metrics dataframe
2024-09-23 13:53:09,431:INFO:Uploading results into container
2024-09-23 13:53:09,431:INFO:Uploading model into container now
2024-09-23 13:53:09,431:INFO:_master_model_container: 6
2024-09-23 13:53:09,431:INFO:_display_container: 2
2024-09-23 13:53:09,431:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 13:53:09,431:INFO:create_model() successfully completed......................................
2024-09-23 13:53:09,494:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:09,494:INFO:Creating metrics dataframe
2024-09-23 13:53:09,494:INFO:Initializing Random Forest Classifier
2024-09-23 13:53:09,494:INFO:Total runtime is 0.3441800872484843 minutes
2024-09-23 13:53:09,507:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:09,507:INFO:Initializing create_model()
2024-09-23 13:53:09,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:09,507:INFO:Checking exceptions
2024-09-23 13:53:09,507:INFO:Importing libraries
2024-09-23 13:53:09,507:INFO:Copying training dataset
2024-09-23 13:53:09,509:INFO:Defining folds
2024-09-23 13:53:09,509:INFO:Declaring metric variables
2024-09-23 13:53:09,523:INFO:Importing untrained model
2024-09-23 13:53:09,526:INFO:Random Forest Classifier Imported successfully
2024-09-23 13:53:09,530:INFO:Starting cross validation
2024-09-23 13:53:09,530:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:12,858:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,863:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,864:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:12,872:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:12,984:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,984:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,990:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,991:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,991:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:12,991:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:12,991:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:12,991:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:12,991:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:13,009:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:13,009:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:13,030:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:13,047:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:13,054:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:13,141:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:53:13,145:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:13,152:INFO:Calculating mean and std
2024-09-23 13:53:13,152:INFO:Creating metrics dataframe
2024-09-23 13:53:13,153:INFO:Uploading results into container
2024-09-23 13:53:13,153:INFO:Uploading model into container now
2024-09-23 13:53:13,154:INFO:_master_model_container: 7
2024-09-23 13:53:13,154:INFO:_display_container: 2
2024-09-23 13:53:13,154:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 13:53:13,154:INFO:create_model() successfully completed......................................
2024-09-23 13:53:13,207:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:13,207:INFO:Creating metrics dataframe
2024-09-23 13:53:13,207:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 13:53:13,207:INFO:Total runtime is 0.4060730576515198 minutes
2024-09-23 13:53:13,223:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:13,223:INFO:Initializing create_model()
2024-09-23 13:53:13,223:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:13,223:INFO:Checking exceptions
2024-09-23 13:53:13,226:INFO:Importing libraries
2024-09-23 13:53:13,226:INFO:Copying training dataset
2024-09-23 13:53:13,234:INFO:Defining folds
2024-09-23 13:53:13,234:INFO:Declaring metric variables
2024-09-23 13:53:13,236:INFO:Importing untrained model
2024-09-23 13:53:13,238:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 13:53:13,241:INFO:Starting cross validation
2024-09-23 13:53:13,244:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:13,822:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:13,908:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:13,908:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:13,923:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:13,923:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:13,923:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:13,939:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:14,008:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:14,008:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,025:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,041:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,041:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:14,041:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,057:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:14,072:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 13:53:14,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,137:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:53:14,137:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:14,153:INFO:Calculating mean and std
2024-09-23 13:53:14,153:INFO:Creating metrics dataframe
2024-09-23 13:53:14,153:INFO:Uploading results into container
2024-09-23 13:53:14,153:INFO:Uploading model into container now
2024-09-23 13:53:14,153:INFO:_master_model_container: 8
2024-09-23 13:53:14,153:INFO:_display_container: 2
2024-09-23 13:53:14,153:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 13:53:14,153:INFO:create_model() successfully completed......................................
2024-09-23 13:53:14,224:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:14,224:INFO:Creating metrics dataframe
2024-09-23 13:53:14,224:INFO:Initializing Ada Boost Classifier
2024-09-23 13:53:14,224:INFO:Total runtime is 0.423015038172404 minutes
2024-09-23 13:53:14,224:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:14,224:INFO:Initializing create_model()
2024-09-23 13:53:14,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:14,224:INFO:Checking exceptions
2024-09-23 13:53:14,224:INFO:Importing libraries
2024-09-23 13:53:14,224:INFO:Copying training dataset
2024-09-23 13:53:14,244:INFO:Defining folds
2024-09-23 13:53:14,244:INFO:Declaring metric variables
2024-09-23 13:53:14,244:INFO:Importing untrained model
2024-09-23 13:53:14,244:INFO:Ada Boost Classifier Imported successfully
2024-09-23 13:53:14,244:INFO:Starting cross validation
2024-09-23 13:53:14,244:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:53:14,768:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,808:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,808:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,838:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,859:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,906:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,937:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,968:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,968:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:14,984:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 13:53:16,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,585:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,647:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,694:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,694:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,757:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,788:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,850:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:53:16,858:INFO:Calculating mean and std
2024-09-23 13:53:16,858:INFO:Creating metrics dataframe
2024-09-23 13:53:16,858:INFO:Uploading results into container
2024-09-23 13:53:16,858:INFO:Uploading model into container now
2024-09-23 13:53:16,858:INFO:_master_model_container: 9
2024-09-23 13:53:16,858:INFO:_display_container: 2
2024-09-23 13:53:16,858:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 13:53:16,858:INFO:create_model() successfully completed......................................
2024-09-23 13:53:16,925:INFO:SubProcess create_model() end ==================================
2024-09-23 13:53:16,925:INFO:Creating metrics dataframe
2024-09-23 13:53:16,925:INFO:Initializing Gradient Boosting Classifier
2024-09-23 13:53:16,925:INFO:Total runtime is 0.4680305282274882 minutes
2024-09-23 13:53:16,925:INFO:SubProcess create_model() called ==================================
2024-09-23 13:53:16,925:INFO:Initializing create_model()
2024-09-23 13:53:16,925:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:53:16,925:INFO:Checking exceptions
2024-09-23 13:53:16,925:INFO:Importing libraries
2024-09-23 13:53:16,925:INFO:Copying training dataset
2024-09-23 13:53:16,946:INFO:Defining folds
2024-09-23 13:53:16,946:INFO:Declaring metric variables
2024-09-23 13:53:16,948:INFO:Importing untrained model
2024-09-23 13:53:16,948:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 13:53:16,948:INFO:Starting cross validation
2024-09-23 13:53:16,957:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:54:05,092:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:05,217:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:05,217:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:05,405:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:06,032:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:06,234:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:06,437:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:06,840:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:07,038:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:07,219:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:07,639:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:07,647:INFO:Calculating mean and std
2024-09-23 13:54:07,648:INFO:Creating metrics dataframe
2024-09-23 13:54:07,649:INFO:Uploading results into container
2024-09-23 13:54:07,649:INFO:Uploading model into container now
2024-09-23 13:54:07,650:INFO:_master_model_container: 10
2024-09-23 13:54:07,650:INFO:_display_container: 2
2024-09-23 13:54:07,650:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 13:54:07,650:INFO:create_model() successfully completed......................................
2024-09-23 13:54:07,715:INFO:SubProcess create_model() end ==================================
2024-09-23 13:54:07,715:INFO:Creating metrics dataframe
2024-09-23 13:54:07,722:INFO:Initializing Linear Discriminant Analysis
2024-09-23 13:54:07,722:INFO:Total runtime is 1.3146434426307678 minutes
2024-09-23 13:54:07,723:INFO:SubProcess create_model() called ==================================
2024-09-23 13:54:07,723:INFO:Initializing create_model()
2024-09-23 13:54:07,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:54:07,723:INFO:Checking exceptions
2024-09-23 13:54:07,723:INFO:Importing libraries
2024-09-23 13:54:07,723:INFO:Copying training dataset
2024-09-23 13:54:07,731:INFO:Defining folds
2024-09-23 13:54:07,731:INFO:Declaring metric variables
2024-09-23 13:54:07,731:INFO:Importing untrained model
2024-09-23 13:54:07,737:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 13:54:07,741:INFO:Starting cross validation
2024-09-23 13:54:07,742:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:54:08,364:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,476:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,523:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,523:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,539:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,541:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,571:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,571:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:54:08,586:INFO:Calculating mean and std
2024-09-23 13:54:08,586:INFO:Creating metrics dataframe
2024-09-23 13:54:08,586:INFO:Uploading results into container
2024-09-23 13:54:08,586:INFO:Uploading model into container now
2024-09-23 13:54:08,586:INFO:_master_model_container: 11
2024-09-23 13:54:08,586:INFO:_display_container: 2
2024-09-23 13:54:08,586:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 13:54:08,586:INFO:create_model() successfully completed......................................
2024-09-23 13:54:08,656:INFO:SubProcess create_model() end ==================================
2024-09-23 13:54:08,656:INFO:Creating metrics dataframe
2024-09-23 13:54:08,656:INFO:Initializing Extra Trees Classifier
2024-09-23 13:54:08,656:INFO:Total runtime is 1.3302239020665487 minutes
2024-09-23 13:54:08,656:INFO:SubProcess create_model() called ==================================
2024-09-23 13:54:08,656:INFO:Initializing create_model()
2024-09-23 13:54:08,656:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:54:08,656:INFO:Checking exceptions
2024-09-23 13:54:08,656:INFO:Importing libraries
2024-09-23 13:54:08,656:INFO:Copying training dataset
2024-09-23 13:54:08,675:INFO:Defining folds
2024-09-23 13:54:08,675:INFO:Declaring metric variables
2024-09-23 13:54:08,676:INFO:Importing untrained model
2024-09-23 13:54:08,676:INFO:Extra Trees Classifier Imported successfully
2024-09-23 13:54:08,682:INFO:Starting cross validation
2024-09-23 13:54:08,682:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:54:10,609:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:10,625:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,483:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,492:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,508:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,508:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,586:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,586:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,601:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,617:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,617:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,633:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,633:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,633:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,633:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:11,648:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:11,664:INFO:Calculating mean and std
2024-09-23 13:54:11,664:INFO:Creating metrics dataframe
2024-09-23 13:54:11,664:INFO:Uploading results into container
2024-09-23 13:54:11,664:INFO:Uploading model into container now
2024-09-23 13:54:11,664:INFO:_master_model_container: 12
2024-09-23 13:54:11,664:INFO:_display_container: 2
2024-09-23 13:54:11,664:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 13:54:11,664:INFO:create_model() successfully completed......................................
2024-09-23 13:54:11,755:INFO:SubProcess create_model() end ==================================
2024-09-23 13:54:11,755:INFO:Creating metrics dataframe
2024-09-23 13:54:11,760:INFO:Initializing Extreme Gradient Boosting
2024-09-23 13:54:11,760:INFO:Total runtime is 1.3819497028986614 minutes
2024-09-23 13:54:11,760:INFO:SubProcess create_model() called ==================================
2024-09-23 13:54:11,760:INFO:Initializing create_model()
2024-09-23 13:54:11,760:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:54:11,760:INFO:Checking exceptions
2024-09-23 13:54:11,760:INFO:Importing libraries
2024-09-23 13:54:11,760:INFO:Copying training dataset
2024-09-23 13:54:11,776:INFO:Defining folds
2024-09-23 13:54:11,776:INFO:Declaring metric variables
2024-09-23 13:54:11,778:INFO:Importing untrained model
2024-09-23 13:54:11,780:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 13:54:11,784:INFO:Starting cross validation
2024-09-23 13:54:11,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:54:16,280:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,280:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:16,312:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,312:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:16,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,437:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,546:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,546:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:16,593:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,593:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:16,733:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,733:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:16,812:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,852:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:16,855:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:16,861:INFO:Calculating mean and std
2024-09-23 13:54:16,862:INFO:Creating metrics dataframe
2024-09-23 13:54:16,863:INFO:Uploading results into container
2024-09-23 13:54:16,864:INFO:Uploading model into container now
2024-09-23 13:54:16,864:INFO:_master_model_container: 13
2024-09-23 13:54:16,864:INFO:_display_container: 2
2024-09-23 13:54:16,865:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 13:54:16,865:INFO:create_model() successfully completed......................................
2024-09-23 13:54:16,920:INFO:SubProcess create_model() end ==================================
2024-09-23 13:54:16,920:INFO:Creating metrics dataframe
2024-09-23 13:54:16,929:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 13:54:16,929:INFO:Total runtime is 1.4681019465128582 minutes
2024-09-23 13:54:16,929:INFO:SubProcess create_model() called ==================================
2024-09-23 13:54:16,929:INFO:Initializing create_model()
2024-09-23 13:54:16,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:54:16,929:INFO:Checking exceptions
2024-09-23 13:54:16,929:INFO:Importing libraries
2024-09-23 13:54:16,929:INFO:Copying training dataset
2024-09-23 13:54:16,944:INFO:Defining folds
2024-09-23 13:54:16,944:INFO:Declaring metric variables
2024-09-23 13:54:16,944:INFO:Importing untrained model
2024-09-23 13:54:16,944:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 13:54:16,944:INFO:Starting cross validation
2024-09-23 13:54:16,956:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:54:34,197:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:34,203:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:36,845:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:36,845:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:36,902:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:36,924:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,008:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,013:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:37,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,252:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,257:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:37,390:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,440:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:54:37,455:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:54:37,455:INFO:Calculating mean and std
2024-09-23 13:54:37,455:INFO:Creating metrics dataframe
2024-09-23 13:54:37,455:INFO:Uploading results into container
2024-09-23 13:54:37,455:INFO:Uploading model into container now
2024-09-23 13:54:37,455:INFO:_master_model_container: 14
2024-09-23 13:54:37,455:INFO:_display_container: 2
2024-09-23 13:54:37,455:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 13:54:37,455:INFO:create_model() successfully completed......................................
2024-09-23 13:54:37,518:INFO:SubProcess create_model() end ==================================
2024-09-23 13:54:37,518:INFO:Creating metrics dataframe
2024-09-23 13:54:37,534:INFO:Initializing CatBoost Classifier
2024-09-23 13:54:37,534:INFO:Total runtime is 1.8115086476008098 minutes
2024-09-23 13:54:37,534:INFO:SubProcess create_model() called ==================================
2024-09-23 13:54:37,534:INFO:Initializing create_model()
2024-09-23 13:54:37,534:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:54:37,534:INFO:Checking exceptions
2024-09-23 13:54:37,534:INFO:Importing libraries
2024-09-23 13:54:37,534:INFO:Copying training dataset
2024-09-23 13:54:37,567:INFO:Defining folds
2024-09-23 13:54:37,567:INFO:Declaring metric variables
2024-09-23 13:54:37,571:INFO:Importing untrained model
2024-09-23 13:54:37,574:INFO:CatBoost Classifier Imported successfully
2024-09-23 13:54:37,580:INFO:Starting cross validation
2024-09-23 13:54:37,583:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:55:33,115:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,130:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:33,130:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,177:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,177:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:33,255:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,271:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:33,318:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,365:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,365:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:33,396:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,412:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:33,412:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:33,412:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:33,427:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
2 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
2 fits failed with the following error:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 276, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\joblib\memory.py", line 349, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 69, in _fit_one
    transformer.fit(*args)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\catboost\core.py", line 5245, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\catboost\core.py", line 2410, in _fit
    self._train(
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\catboost\core.py", line 1790, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 5017, in _catboost._CatBoost._train
  File "_catboost.pyx", line 5066, in _catboost._CatBoost._train
_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2024-09-23 13:55:33,427:INFO:Calculating mean and std
2024-09-23 13:55:33,427:INFO:Creating metrics dataframe
2024-09-23 13:55:33,427:INFO:Uploading results into container
2024-09-23 13:55:33,427:INFO:Uploading model into container now
2024-09-23 13:55:33,427:INFO:_master_model_container: 15
2024-09-23 13:55:33,427:INFO:_display_container: 2
2024-09-23 13:55:33,427:INFO:<catboost.core.CatBoostClassifier object at 0x00000181AD312950>
2024-09-23 13:55:33,427:INFO:create_model() successfully completed......................................
2024-09-23 13:55:33,521:INFO:SubProcess create_model() end ==================================
2024-09-23 13:55:33,521:INFO:Creating metrics dataframe
2024-09-23 13:55:33,537:INFO:Initializing Dummy Classifier
2024-09-23 13:55:33,537:INFO:Total runtime is 2.7448947628339133 minutes
2024-09-23 13:55:33,552:INFO:SubProcess create_model() called ==================================
2024-09-23 13:55:33,552:INFO:Initializing create_model()
2024-09-23 13:55:33,552:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B139FB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:55:33,552:INFO:Checking exceptions
2024-09-23 13:55:33,552:INFO:Importing libraries
2024-09-23 13:55:33,552:INFO:Copying training dataset
2024-09-23 13:55:33,565:INFO:Defining folds
2024-09-23 13:55:33,565:INFO:Declaring metric variables
2024-09-23 13:55:33,567:INFO:Importing untrained model
2024-09-23 13:55:33,568:INFO:Dummy Classifier Imported successfully
2024-09-23 13:55:33,571:INFO:Starting cross validation
2024-09-23 13:55:33,573:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:55:34,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,169:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,169:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,184:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,184:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,216:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,216:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,262:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,309:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,309:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,341:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,341:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,356:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_response.py", line 210, in _get_response_values
    y_pred = prediction_method(X)
             ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pipeline.py", line 341, in predict_proba
    Xt = transform.transform(Xt)
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_set_output.py", line 295, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\preprocess\transformers.py", line 248, in transform
    args.append(X[self._include])
                ~^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\frame.py", line 3899, in __getitem__
    indexer = self.columns._get_indexer_strict(key, "columns")[1]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6115, in _get_indexer_strict
    self._raise_if_missing(keyarr, indexer, axis_name)
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pandas\core\indexes\base.py", line 6179, in _raise_if_missing
    raise KeyError(f"{not_found} not in index")
KeyError: "['Pendapatan Keluarga', 'Pekerjaan Orang Tua/Wali', 'Kepemilikan Aset', 'Status Kepesertaan Program Bantuan Pemerintah', 'Riwayat Beasiswa atau Bantuan Pendidikan', 'Status Kepemilikan Tempat Tinggal'] not in index"

  warnings.warn(

2024-09-23 13:55:34,356:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:55:34,372:INFO:Calculating mean and std
2024-09-23 13:55:34,372:INFO:Creating metrics dataframe
2024-09-23 13:55:34,372:INFO:Uploading results into container
2024-09-23 13:55:34,372:INFO:Uploading model into container now
2024-09-23 13:55:34,372:INFO:_master_model_container: 16
2024-09-23 13:55:34,372:INFO:_display_container: 2
2024-09-23 13:55:34,372:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 13:55:34,372:INFO:create_model() successfully completed......................................
2024-09-23 13:55:34,434:INFO:SubProcess create_model() end ==================================
2024-09-23 13:55:34,434:INFO:Creating metrics dataframe
2024-09-23 13:55:34,452:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 13:55:34,458:INFO:Initializing create_model()
2024-09-23 13:55:34,458:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:55:34,458:INFO:Checking exceptions
2024-09-23 13:55:34,460:INFO:Importing libraries
2024-09-23 13:55:34,460:INFO:Copying training dataset
2024-09-23 13:55:34,472:INFO:Defining folds
2024-09-23 13:55:34,472:INFO:Declaring metric variables
2024-09-23 13:55:34,472:INFO:Importing untrained model
2024-09-23 13:55:34,473:INFO:Declaring custom model
2024-09-23 13:55:34,473:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 13:55:34,475:INFO:Cross validation set to False
2024-09-23 13:55:34,475:INFO:Fitting Model
2024-09-23 13:56:09,268:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 13:56:09,268:INFO:create_model() successfully completed......................................
2024-09-23 13:56:09,338:INFO:_master_model_container: 16
2024-09-23 13:56:09,338:INFO:_display_container: 2
2024-09-23 13:56:09,338:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 13:56:09,338:INFO:compare_models() successfully completed......................................
2024-09-23 13:57:33,779:INFO:Initializing create_model()
2024-09-23 13:57:33,779:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 13:57:33,779:INFO:Checking exceptions
2024-09-23 13:57:33,799:INFO:Importing libraries
2024-09-23 13:57:33,799:INFO:Copying training dataset
2024-09-23 13:57:33,814:INFO:Defining folds
2024-09-23 13:57:33,814:INFO:Declaring metric variables
2024-09-23 13:57:33,824:INFO:Importing untrained model
2024-09-23 13:57:33,827:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 13:57:33,827:INFO:Starting cross validation
2024-09-23 13:57:33,827:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 13:58:14,084:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,177:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,381:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,599:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,662:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,725:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:14,725:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 13:58:14,757:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:15,117:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:15,789:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 13:58:15,805:INFO:Calculating mean and std
2024-09-23 13:58:15,805:INFO:Creating metrics dataframe
2024-09-23 13:58:15,805:INFO:Finalizing model
2024-09-23 13:58:50,224:INFO:Uploading results into container
2024-09-23 13:58:50,232:INFO:Uploading model into container now
2024-09-23 13:58:50,239:INFO:_master_model_container: 17
2024-09-23 13:58:50,239:INFO:_display_container: 3
2024-09-23 13:58:50,239:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 13:58:50,239:INFO:create_model() successfully completed......................................
2024-09-23 14:03:42,281:INFO:Initializing tune_model()
2024-09-23 14:03:42,281:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-23 14:03:42,281:INFO:Checking exceptions
2024-09-23 14:03:42,308:INFO:Copying training dataset
2024-09-23 14:03:42,326:INFO:Checking base model
2024-09-23 14:03:42,326:INFO:Base model : Gradient Boosting Classifier
2024-09-23 14:03:42,326:INFO:Declaring metric variables
2024-09-23 14:03:42,340:INFO:Defining Hyperparameters
2024-09-23 14:03:42,433:INFO:Tuning with n_jobs=-1
2024-09-23 14:03:42,433:INFO:Initializing RandomizedSearchCV
2024-09-23 14:09:08,668:INFO:best_params: {'actual_estimator__subsample': 0.35, 'actual_estimator__n_estimators': 140, 'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 2, 'actual_estimator__min_impurity_decrease': 0.05, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.01}
2024-09-23 14:09:08,684:INFO:Hyperparameter search completed
2024-09-23 14:09:08,684:INFO:SubProcess create_model() called ==================================
2024-09-23 14:09:08,684:INFO:Initializing create_model()
2024-09-23 14:09:08,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000181B04A9D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.35, 'n_estimators': 140, 'min_samples_split': 2, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.05, 'max_features': 'sqrt', 'max_depth': 7, 'learning_rate': 0.01})
2024-09-23 14:09:08,684:INFO:Checking exceptions
2024-09-23 14:09:08,684:INFO:Importing libraries
2024-09-23 14:09:08,684:INFO:Copying training dataset
2024-09-23 14:09:08,684:INFO:Defining folds
2024-09-23 14:09:08,684:INFO:Declaring metric variables
2024-09-23 14:09:08,684:INFO:Importing untrained model
2024-09-23 14:09:08,684:INFO:Declaring custom model
2024-09-23 14:09:08,684:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 14:09:08,700:INFO:Starting cross validation
2024-09-23 14:09:08,700:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:09:21,750:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:21,750:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:22,562:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:22,562:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:23,172:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:23,172:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:23,250:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:23,250:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:23,907:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:23,907:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:24,360:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:24,360:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:24,438:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:24,438:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:24,922:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:24,922:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:25,485:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:25,501:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:25,594:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:09:25,594:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:09:25,610:INFO:Calculating mean and std
2024-09-23 14:09:25,610:INFO:Creating metrics dataframe
2024-09-23 14:09:25,610:INFO:Finalizing model
2024-09-23 14:09:36,330:INFO:Uploading results into container
2024-09-23 14:09:36,346:INFO:Uploading model into container now
2024-09-23 14:09:36,346:INFO:_master_model_container: 18
2024-09-23 14:09:36,346:INFO:_display_container: 4
2024-09-23 14:09:36,346:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.01, loss='log_loss', max_depth=7,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.05, min_samples_leaf=2,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=140, n_iter_no_change=None,
                           random_state=123, subsample=0.35, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 14:09:36,346:INFO:create_model() successfully completed......................................
2024-09-23 14:09:36,409:INFO:SubProcess create_model() end ==================================
2024-09-23 14:09:36,409:INFO:choose_better activated
2024-09-23 14:09:36,409:INFO:SubProcess create_model() called ==================================
2024-09-23 14:09:36,409:INFO:Initializing create_model()
2024-09-23 14:09:36,409:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:09:36,409:INFO:Checking exceptions
2024-09-23 14:09:36,409:INFO:Importing libraries
2024-09-23 14:09:36,409:INFO:Copying training dataset
2024-09-23 14:09:36,425:INFO:Defining folds
2024-09-23 14:09:36,425:INFO:Declaring metric variables
2024-09-23 14:09:36,425:INFO:Importing untrained model
2024-09-23 14:09:36,425:INFO:Declaring custom model
2024-09-23 14:09:36,425:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 14:09:36,425:INFO:Starting cross validation
2024-09-23 14:09:36,425:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:10:18,266:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:19,906:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:19,922:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:20,354:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:20,361:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:10:20,564:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:21,931:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:22,525:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:22,854:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:23,479:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:23,963:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:10:23,963:INFO:Calculating mean and std
2024-09-23 14:10:23,963:INFO:Creating metrics dataframe
2024-09-23 14:10:23,979:INFO:Finalizing model
2024-09-23 14:10:57,875:INFO:Uploading results into container
2024-09-23 14:10:57,875:INFO:Uploading model into container now
2024-09-23 14:10:57,875:INFO:_master_model_container: 19
2024-09-23 14:10:57,875:INFO:_display_container: 5
2024-09-23 14:10:57,875:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 14:10:57,875:INFO:create_model() successfully completed......................................
2024-09-23 14:10:57,937:INFO:SubProcess create_model() end ==================================
2024-09-23 14:10:57,937:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.5457
2024-09-23 14:10:57,937:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.01, loss='log_loss', max_depth=7,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.05, min_samples_leaf=2,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=140, n_iter_no_change=None,
                           random_state=123, subsample=0.35, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.5376
2024-09-23 14:10:57,937:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2024-09-23 14:10:57,937:INFO:choose_better completed
2024-09-23 14:10:57,937:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-23 14:10:57,937:INFO:_master_model_container: 19
2024-09-23 14:10:57,937:INFO:_display_container: 4
2024-09-23 14:10:57,937:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 14:10:57,937:INFO:tune_model() successfully completed......................................
2024-09-23 14:12:16,195:INFO:Initializing evaluate_model()
2024-09-23 14:12:16,195:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 14:12:16,216:INFO:Initializing plot_model()
2024-09-23 14:12:16,216:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:16,216:INFO:Checking exceptions
2024-09-23 14:12:16,224:INFO:Preloading libraries
2024-09-23 14:12:16,247:INFO:Copying training dataset
2024-09-23 14:12:16,247:INFO:Plot type: pipeline
2024-09-23 14:12:16,406:INFO:Visual Rendered Successfully
2024-09-23 14:12:16,472:INFO:plot_model() successfully completed......................................
2024-09-23 14:12:18,241:INFO:Initializing plot_model()
2024-09-23 14:12:18,241:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:18,241:INFO:Checking exceptions
2024-09-23 14:12:18,259:INFO:Preloading libraries
2024-09-23 14:12:18,289:INFO:Copying training dataset
2024-09-23 14:12:18,289:INFO:Plot type: learning
2024-09-23 14:12:18,465:INFO:Fitting Model
2024-09-23 14:12:41,723:INFO:Initializing evaluate_model()
2024-09-23 14:12:41,723:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 14:12:41,726:INFO:Initializing plot_model()
2024-09-23 14:12:41,726:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:41,726:INFO:Checking exceptions
2024-09-23 14:12:41,726:INFO:Preloading libraries
2024-09-23 14:12:41,777:INFO:Copying training dataset
2024-09-23 14:12:41,777:INFO:Plot type: pipeline
2024-09-23 14:12:42,041:INFO:Initializing plot_model()
2024-09-23 14:12:42,041:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:42,043:INFO:Checking exceptions
2024-09-23 14:12:42,043:INFO:Preloading libraries
2024-09-23 14:12:42,074:INFO:Copying training dataset
2024-09-23 14:12:42,074:INFO:Plot type: confusion_matrix
2024-09-23 14:12:44,147:INFO:Initializing evaluate_model()
2024-09-23 14:12:44,147:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 14:12:44,175:INFO:Initializing plot_model()
2024-09-23 14:12:44,175:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:44,175:INFO:Checking exceptions
2024-09-23 14:12:44,175:INFO:Preloading libraries
2024-09-23 14:12:44,208:INFO:Copying training dataset
2024-09-23 14:12:44,208:INFO:Plot type: pipeline
2024-09-23 14:12:44,378:INFO:Visual Rendered Successfully
2024-09-23 14:12:44,469:INFO:plot_model() successfully completed......................................
2024-09-23 14:12:48,601:INFO:Initializing plot_model()
2024-09-23 14:12:48,601:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:48,601:INFO:Checking exceptions
2024-09-23 14:12:48,609:INFO:Preloading libraries
2024-09-23 14:12:48,641:INFO:Copying training dataset
2024-09-23 14:12:48,641:INFO:Plot type: parameter
2024-09-23 14:12:48,644:INFO:Visual Rendered Successfully
2024-09-23 14:12:48,704:INFO:plot_model() successfully completed......................................
2024-09-23 14:12:52,194:INFO:Initializing plot_model()
2024-09-23 14:12:52,194:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:12:52,195:INFO:Checking exceptions
2024-09-23 14:12:52,209:INFO:Preloading libraries
2024-09-23 14:12:52,256:INFO:Copying training dataset
2024-09-23 14:12:52,256:INFO:Plot type: confusion_matrix
2024-09-23 14:12:52,412:INFO:Fitting Model
2024-09-23 14:12:52,415:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 14:12:52,416:INFO:Scoring test/hold-out set
2024-09-23 14:12:52,657:INFO:Visual Rendered Successfully
2024-09-23 14:12:52,734:INFO:plot_model() successfully completed......................................
2024-09-23 14:13:05,691:INFO:Initializing plot_model()
2024-09-23 14:13:05,691:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:13:05,691:INFO:Checking exceptions
2024-09-23 14:13:05,710:INFO:Preloading libraries
2024-09-23 14:13:05,750:INFO:Copying training dataset
2024-09-23 14:13:05,750:INFO:Plot type: feature
2024-09-23 14:13:05,751:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 14:13:05,866:INFO:Visual Rendered Successfully
2024-09-23 14:13:05,955:INFO:plot_model() successfully completed......................................
2024-09-23 14:13:27,978:INFO:Initializing plot_model()
2024-09-23 14:13:27,979:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:13:27,979:INFO:Checking exceptions
2024-09-23 14:13:27,993:INFO:Preloading libraries
2024-09-23 14:13:28,030:INFO:Copying training dataset
2024-09-23 14:13:28,030:INFO:Plot type: feature_all
2024-09-23 14:13:28,065:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 14:13:28,381:INFO:Visual Rendered Successfully
2024-09-23 14:13:28,466:INFO:plot_model() successfully completed......................................
2024-09-23 14:13:33,249:INFO:Initializing plot_model()
2024-09-23 14:13:33,250:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:13:33,250:INFO:Checking exceptions
2024-09-23 14:13:33,257:INFO:Preloading libraries
2024-09-23 14:13:33,310:INFO:Copying training dataset
2024-09-23 14:13:33,310:INFO:Plot type: pr
2024-09-23 14:13:33,457:INFO:Fitting Model
2024-09-23 14:14:03,318:INFO:Initializing plot_model()
2024-09-23 14:14:03,318:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:03,318:INFO:Checking exceptions
2024-09-23 14:14:03,325:INFO:Preloading libraries
2024-09-23 14:14:03,341:INFO:Copying training dataset
2024-09-23 14:14:03,341:INFO:Plot type: boundary
2024-09-23 14:14:03,470:INFO:Fitting StandardScaler()
2024-09-23 14:14:05,774:INFO:Initializing evaluate_model()
2024-09-23 14:14:05,775:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 14:14:05,793:INFO:Initializing plot_model()
2024-09-23 14:14:05,793:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:05,793:INFO:Checking exceptions
2024-09-23 14:14:05,795:INFO:Preloading libraries
2024-09-23 14:14:05,825:INFO:Copying training dataset
2024-09-23 14:14:05,825:INFO:Plot type: pipeline
2024-09-23 14:14:05,906:INFO:Visual Rendered Successfully
2024-09-23 14:14:05,997:INFO:plot_model() successfully completed......................................
2024-09-23 14:14:08,446:INFO:Initializing plot_model()
2024-09-23 14:14:08,446:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=tree, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:08,447:INFO:Checking exceptions
2024-09-23 14:14:11,395:INFO:Initializing plot_model()
2024-09-23 14:14:11,395:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=lift, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:11,395:INFO:Checking exceptions
2024-09-23 14:14:11,424:INFO:Preloading libraries
2024-09-23 14:14:11,453:INFO:Copying training dataset
2024-09-23 14:14:11,453:INFO:Plot type: lift
2024-09-23 14:14:11,453:INFO:Generating predictions / predict_proba on X_test
2024-09-23 14:14:12,611:INFO:Initializing plot_model()
2024-09-23 14:14:12,611:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:12,611:INFO:Checking exceptions
2024-09-23 14:14:12,626:INFO:Preloading libraries
2024-09-23 14:14:12,674:INFO:Copying training dataset
2024-09-23 14:14:12,674:INFO:Plot type: boundary
2024-09-23 14:14:12,788:INFO:Fitting StandardScaler()
2024-09-23 14:14:12,802:INFO:Fitting PCA()
2024-09-23 14:14:12,983:INFO:Fitting Model
2024-09-23 14:14:26,179:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\yellowbrick\contrib\classifier\boundaries.py:467: UserWarning: You passed a edgecolor/edgecolors ('black') for an unfilled marker ('+').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.
  self.ax.scatter(

2024-09-23 14:14:26,867:INFO:Visual Rendered Successfully
2024-09-23 14:14:27,029:INFO:plot_model() successfully completed......................................
2024-09-23 14:14:36,979:INFO:Initializing plot_model()
2024-09-23 14:14:36,979:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:36,979:INFO:Checking exceptions
2024-09-23 14:14:36,992:INFO:Preloading libraries
2024-09-23 14:14:37,023:INFO:Copying training dataset
2024-09-23 14:14:37,023:INFO:Plot type: feature_all
2024-09-23 14:14:37,085:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 14:14:37,356:INFO:Visual Rendered Successfully
2024-09-23 14:14:37,452:INFO:plot_model() successfully completed......................................
2024-09-23 14:14:39,523:INFO:Initializing plot_model()
2024-09-23 14:14:39,523:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=class_report, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:14:39,523:INFO:Checking exceptions
2024-09-23 14:14:39,539:INFO:Preloading libraries
2024-09-23 14:14:39,573:INFO:Copying training dataset
2024-09-23 14:14:39,573:INFO:Plot type: class_report
2024-09-23 14:14:39,740:INFO:Fitting Model
2024-09-23 14:14:39,740:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 14:14:39,741:INFO:Scoring test/hold-out set
2024-09-23 14:14:39,975:INFO:Visual Rendered Successfully
2024-09-23 14:14:40,071:INFO:plot_model() successfully completed......................................
2024-09-23 14:15:00,826:INFO:Initializing plot_model()
2024-09-23 14:15:00,826:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:15:00,826:INFO:Checking exceptions
2024-09-23 14:15:00,853:INFO:Preloading libraries
2024-09-23 14:15:00,886:INFO:Copying training dataset
2024-09-23 14:15:00,886:INFO:Plot type: pr
2024-09-23 14:15:01,076:INFO:Fitting Model
2024-09-23 14:15:35,825:INFO:Scoring test/hold-out set
2024-09-23 14:15:36,106:INFO:Visual Rendered Successfully
2024-09-23 14:15:36,206:INFO:plot_model() successfully completed......................................
2024-09-23 14:15:44,041:INFO:Initializing plot_model()
2024-09-23 14:15:44,042:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:15:44,042:INFO:Checking exceptions
2024-09-23 14:15:44,059:INFO:Preloading libraries
2024-09-23 14:15:44,102:INFO:Copying training dataset
2024-09-23 14:15:44,102:INFO:Plot type: confusion_matrix
2024-09-23 14:15:44,286:INFO:Fitting Model
2024-09-23 14:15:44,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 14:15:44,286:INFO:Scoring test/hold-out set
2024-09-23 14:15:44,503:INFO:Visual Rendered Successfully
2024-09-23 14:15:44,581:INFO:plot_model() successfully completed......................................
2024-09-23 14:15:54,536:INFO:Initializing plot_model()
2024-09-23 14:15:54,536:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:15:54,539:INFO:Checking exceptions
2024-09-23 14:15:54,541:INFO:Preloading libraries
2024-09-23 14:15:54,575:INFO:Copying training dataset
2024-09-23 14:15:54,575:INFO:Plot type: parameter
2024-09-23 14:15:54,577:INFO:Visual Rendered Successfully
2024-09-23 14:15:54,667:INFO:plot_model() successfully completed......................................
2024-09-23 14:26:16,242:INFO:Initializing get_config()
2024-09-23 14:26:16,242:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, variable=X)
2024-09-23 14:26:16,264:INFO:Variable:  returned as            id             Pendapatan Keluarga  \
12385   77651      kurang dari Rp 1.000.000,-   
27539   76385  Rp 4.000.000 s.d. Rp 5.000.000   
17473   62396  Rp 3.000.000 s.d. Rp 4.000.000   
259     68391  Rp 3.000.000 s.d. Rp 4.000.000   
27337   83643         lebih dari Rp 5.000.000   
...       ...                             ...   
11567   45626  Rp 3.000.000 s.d. Rp 4.000.000   
4984    75945  Rp 1.000.000 s.d. Rp 2.000.000   
19529  107798         lebih dari Rp 5.000.000   
21575  149359  Rp 3.000.000 s.d. Rp 4.000.000   
20630   42359         lebih dari Rp 5.000.000   

       Jumlah Tanggungan dalam Keluarga Pekerjaan Orang Tua/Wali  \
12385                               1.0                    Buruh   
27539                               2.0           Pegawai Swasta   
17473                               2.0               Wiraswasta   
259                                 1.0                    Buruh   
27337                               2.0     Pegawai Pemerintahan   
...                                 ...                      ...   
11567                               2.0     Pegawai Pemerintahan   
4984                                0.0                   Petani   
19529                               2.0           Pegawai Swasta   
21575                               1.0            Tidak Bekerja   
20630                               2.0                    Buruh   

      Pendidikan Orang Tua/Wali Kepemilikan Aset Lokasi Tempat Tinggal  \
12385                      SLTP    Milik Sendiri   Kabupaten Wonogiri    
27539                    S1/D-4    Milik Sendiri        Kota Semarang    
17473                       SMA    Milik Sendiri     Kabupaten Sragen    
259                      S-1/D4    Milik Sendiri    Kabupaten Kebumen    
27337                       S-2    Milik Sendiri      Kabupaten Demak    
...                         ...              ...                   ...   
11567                        D4    Milik Sendiri       Kota Palembang    
4984            SD/MI Sederajat    Milik Sendiri    Kabupaten Cilacap    
19529                        MA    Milik Sendiri            Kab. Kudus   
21575            SLTP Sederajat        Menumpang                   NaN   
20630                        MI    Milik Sendiri      Kabupaten Tegal    

       Pengeluaran Bulanan Keluarga  \
12385                      500000.0   
27539                     1000000.0   
17473                     1500000.0   
259                        500000.0   
27337                     7600000.0   
...                             ...   
11567                     1000000.0   
4984                      1300000.0   
19529                     2500000.0   
21575                     2000000.0   
20630                      900000.0   

      Status Kepesertaan Program Bantuan Pemerintah  \
12385                                  KIS/JKS/BPJS   
27539                                  KIS/JKS/BPJS   
17473                                           NaN   
259                                    KIS/JKS/BPJS   
27337                                  KIS/JKS/BPJS   
...                                             ...   
11567                                           NaN   
4984                                   KIS/JKS/BPJS   
19529                                  KIS/JKS/BPJS   
21575                                  KIS/JKS/BPJS   
20630                                           NaN   

      Riwayat Beasiswa atau Bantuan Pendidikan  ... NJOP per Meter  \
12385                                        -  ...        14000.0   
27539                              Tidak Dapat  ...       285000.0   
17473                              Tidak Dapat  ...      1032000.0   
259                                          -  ...        36000.0   
27337                                Tidak Ada  ...       160000.0   
...                                        ...  ...            ...   
11567                                    Tidak  ...       595000.0   
4984                                 Tidak Ada  ...        11628.0   
19529                                Tidak Ada  ...       464000.0   
21575                                     KIPK  ...       750000.0   
20630                                        -  ...            0.0   

                   Bahan Atap Rumah  \
12385                       Genting   
27539            GENTING TANAH LIAT   
17473                       Genteng   
259                         GENTING   
27337            Genteng tanah liat   
...                             ...   
11567                       genting   
4984   Genting tanah liat dan asbes   
19529            Genting tanah liat   
21575                       Genting   
20630                          Seng   

                               Bahan Tembok Rumah  Bahan Lantai Rumah  \
12385                                   Batu Bata             Keramik   
27539                                     DINDING             KERAMIK   
17473                         Batu Bata dan Semen             Keramik   
259                                     BATU BATA             PLESTER   
27337  Sebagian Kayu sebagian batu bata dan semen             Keramik   
...                                           ...                 ...   
11567                                   Batu Bata             Keramik   
4984                                   Bata Merah             Keramik   
19529                                   Batu bata             Keramik   
21575                                   Batu bata             Keramik   
20630                                        Kayu               Tanah   

       Rerata Pengeluaran Listrik & Air per Bulan  Total Hutang  \
12385                                     25000.0           0.0   
27539                                    260000.0           0.0   
17473                                    240000.0    20000000.0   
259                                       51480.0    35000000.0   
27337                                   1091000.0           0.0   
...                                           ...           ...   
11567                                   1200000.0           0.0   
4984                                     180000.0           0.0   
19529                                    120000.0           0.0   
21575                                    300000.0           0.0   
20630                                     60000.0     1000000.0   

       Cicilan Hutang per Bulan  \
12385                       0.0   
27539                       0.0   
17473                  900000.0   
259                    500000.0   
27337                       0.0   
...                         ...   
11567                       0.0   
4984                        0.0   
19529                       0.0   
21575                       0.0   
20630                   50000.0   

                                         Program Studi                 Kota  \
12385           Pendidikan Seni Drama, Tari, dan Musik  Kabupaten Wonogiri    
27539        Pendidikan Guru Pendidikan Anak Usia Dini       Kota Semarang    
17473           Pendidikan Seni Drama, Tari, dan Musik    Kabupaten Sragen    
259                                       Ilmu Politik   Kabupaten Kebumen    
27337                                              NaN     Kabupaten Demak    
...                                                ...                  ...   
11567                             Kesehatan Masyarakat      Kota Palembang    
4984                                        Ilmu Hukum   Kabupaten Cilacap    
19529                            Pendidikan Matematika           Kab. Kudus   
21575  Pendidikan Bahasa, Sastra Indonesia, dan Daerah        Kab. Semarang   
20630                                Ilmu Keolahragaan     Kabupaten Tegal    

      Jalur Masuk  
12385        SNBP  
27539        SNBP  
17473        SNBP  
259          SNBT  
27337        SNBT  
...           ...  
11567        SNBT  
4984         SNBP  
19529        SNBT  
21575        SNBT  
20630        SNBP  

[30380 rows x 27 columns]
2024-09-23 14:26:16,264:INFO:get_config() successfully completed......................................
2024-09-23 14:26:38,674:INFO:Initializing get_config()
2024-09-23 14:26:38,674:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, variable=X)
2024-09-23 14:26:38,705:INFO:Variable:  returned as            id             Pendapatan Keluarga  \
12385   77651      kurang dari Rp 1.000.000,-   
27539   76385  Rp 4.000.000 s.d. Rp 5.000.000   
17473   62396  Rp 3.000.000 s.d. Rp 4.000.000   
259     68391  Rp 3.000.000 s.d. Rp 4.000.000   
27337   83643         lebih dari Rp 5.000.000   
...       ...                             ...   
11567   45626  Rp 3.000.000 s.d. Rp 4.000.000   
4984    75945  Rp 1.000.000 s.d. Rp 2.000.000   
19529  107798         lebih dari Rp 5.000.000   
21575  149359  Rp 3.000.000 s.d. Rp 4.000.000   
20630   42359         lebih dari Rp 5.000.000   

       Jumlah Tanggungan dalam Keluarga Pekerjaan Orang Tua/Wali  \
12385                               1.0                    Buruh   
27539                               2.0           Pegawai Swasta   
17473                               2.0               Wiraswasta   
259                                 1.0                    Buruh   
27337                               2.0     Pegawai Pemerintahan   
...                                 ...                      ...   
11567                               2.0     Pegawai Pemerintahan   
4984                                0.0                   Petani   
19529                               2.0           Pegawai Swasta   
21575                               1.0            Tidak Bekerja   
20630                               2.0                    Buruh   

      Pendidikan Orang Tua/Wali Kepemilikan Aset Lokasi Tempat Tinggal  \
12385                      SLTP    Milik Sendiri   Kabupaten Wonogiri    
27539                    S1/D-4    Milik Sendiri        Kota Semarang    
17473                       SMA    Milik Sendiri     Kabupaten Sragen    
259                      S-1/D4    Milik Sendiri    Kabupaten Kebumen    
27337                       S-2    Milik Sendiri      Kabupaten Demak    
...                         ...              ...                   ...   
11567                        D4    Milik Sendiri       Kota Palembang    
4984            SD/MI Sederajat    Milik Sendiri    Kabupaten Cilacap    
19529                        MA    Milik Sendiri            Kab. Kudus   
21575            SLTP Sederajat        Menumpang                   NaN   
20630                        MI    Milik Sendiri      Kabupaten Tegal    

       Pengeluaran Bulanan Keluarga  \
12385                      500000.0   
27539                     1000000.0   
17473                     1500000.0   
259                        500000.0   
27337                     7600000.0   
...                             ...   
11567                     1000000.0   
4984                      1300000.0   
19529                     2500000.0   
21575                     2000000.0   
20630                      900000.0   

      Status Kepesertaan Program Bantuan Pemerintah  \
12385                                  KIS/JKS/BPJS   
27539                                  KIS/JKS/BPJS   
17473                                           NaN   
259                                    KIS/JKS/BPJS   
27337                                  KIS/JKS/BPJS   
...                                             ...   
11567                                           NaN   
4984                                   KIS/JKS/BPJS   
19529                                  KIS/JKS/BPJS   
21575                                  KIS/JKS/BPJS   
20630                                           NaN   

      Riwayat Beasiswa atau Bantuan Pendidikan  ... NJOP per Meter  \
12385                                        -  ...        14000.0   
27539                              Tidak Dapat  ...       285000.0   
17473                              Tidak Dapat  ...      1032000.0   
259                                          -  ...        36000.0   
27337                                Tidak Ada  ...       160000.0   
...                                        ...  ...            ...   
11567                                    Tidak  ...       595000.0   
4984                                 Tidak Ada  ...        11628.0   
19529                                Tidak Ada  ...       464000.0   
21575                                     KIPK  ...       750000.0   
20630                                        -  ...            0.0   

                   Bahan Atap Rumah  \
12385                       Genting   
27539            GENTING TANAH LIAT   
17473                       Genteng   
259                         GENTING   
27337            Genteng tanah liat   
...                             ...   
11567                       genting   
4984   Genting tanah liat dan asbes   
19529            Genting tanah liat   
21575                       Genting   
20630                          Seng   

                               Bahan Tembok Rumah  Bahan Lantai Rumah  \
12385                                   Batu Bata             Keramik   
27539                                     DINDING             KERAMIK   
17473                         Batu Bata dan Semen             Keramik   
259                                     BATU BATA             PLESTER   
27337  Sebagian Kayu sebagian batu bata dan semen             Keramik   
...                                           ...                 ...   
11567                                   Batu Bata             Keramik   
4984                                   Bata Merah             Keramik   
19529                                   Batu bata             Keramik   
21575                                   Batu bata             Keramik   
20630                                        Kayu               Tanah   

       Rerata Pengeluaran Listrik & Air per Bulan  Total Hutang  \
12385                                     25000.0           0.0   
27539                                    260000.0           0.0   
17473                                    240000.0    20000000.0   
259                                       51480.0    35000000.0   
27337                                   1091000.0           0.0   
...                                           ...           ...   
11567                                   1200000.0           0.0   
4984                                     180000.0           0.0   
19529                                    120000.0           0.0   
21575                                    300000.0           0.0   
20630                                     60000.0     1000000.0   

       Cicilan Hutang per Bulan  \
12385                       0.0   
27539                       0.0   
17473                  900000.0   
259                    500000.0   
27337                       0.0   
...                         ...   
11567                       0.0   
4984                        0.0   
19529                       0.0   
21575                       0.0   
20630                   50000.0   

                                         Program Studi                 Kota  \
12385           Pendidikan Seni Drama, Tari, dan Musik  Kabupaten Wonogiri    
27539        Pendidikan Guru Pendidikan Anak Usia Dini       Kota Semarang    
17473           Pendidikan Seni Drama, Tari, dan Musik    Kabupaten Sragen    
259                                       Ilmu Politik   Kabupaten Kebumen    
27337                                              NaN     Kabupaten Demak    
...                                                ...                  ...   
11567                             Kesehatan Masyarakat      Kota Palembang    
4984                                        Ilmu Hukum   Kabupaten Cilacap    
19529                            Pendidikan Matematika           Kab. Kudus   
21575  Pendidikan Bahasa, Sastra Indonesia, dan Daerah        Kab. Semarang   
20630                                Ilmu Keolahragaan     Kabupaten Tegal    

      Jalur Masuk  
12385        SNBP  
27539        SNBP  
17473        SNBP  
259          SNBT  
27337        SNBT  
...           ...  
11567        SNBT  
4984         SNBP  
19529        SNBT  
21575        SNBT  
20630        SNBP  

[30380 rows x 27 columns]
2024-09-23 14:26:38,706:INFO:get_config() successfully completed......................................
2024-09-23 14:38:27,150:INFO:Initializing evaluate_model()
2024-09-23 14:38:27,150:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 14:38:27,172:INFO:Initializing plot_model()
2024-09-23 14:38:27,172:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:38:27,172:INFO:Checking exceptions
2024-09-23 14:38:27,172:INFO:Preloading libraries
2024-09-23 14:38:27,206:INFO:Copying training dataset
2024-09-23 14:38:27,206:INFO:Plot type: pipeline
2024-09-23 14:38:27,294:INFO:Visual Rendered Successfully
2024-09-23 14:38:27,404:INFO:plot_model() successfully completed......................................
2024-09-23 14:38:33,073:INFO:Initializing plot_model()
2024-09-23 14:38:33,073:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:38:33,073:INFO:Checking exceptions
2024-09-23 14:38:33,092:INFO:Preloading libraries
2024-09-23 14:38:33,129:INFO:Copying training dataset
2024-09-23 14:38:33,129:INFO:Plot type: parameter
2024-09-23 14:38:33,131:INFO:Visual Rendered Successfully
2024-09-23 14:38:33,218:INFO:plot_model() successfully completed......................................
2024-09-23 14:38:56,641:INFO:Initializing plot_model()
2024-09-23 14:38:56,641:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:38:56,641:INFO:Checking exceptions
2024-09-23 14:38:56,659:INFO:Preloading libraries
2024-09-23 14:38:56,705:INFO:Copying training dataset
2024-09-23 14:38:56,705:INFO:Plot type: confusion_matrix
2024-09-23 14:38:56,904:INFO:Fitting Model
2024-09-23 14:38:56,904:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 14:38:56,904:INFO:Scoring test/hold-out set
2024-09-23 14:38:57,168:INFO:Visual Rendered Successfully
2024-09-23 14:38:57,265:INFO:plot_model() successfully completed......................................
2024-09-23 14:39:16,922:INFO:Initializing plot_model()
2024-09-23 14:39:16,922:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:39:16,922:INFO:Checking exceptions
2024-09-23 14:39:16,939:INFO:Preloading libraries
2024-09-23 14:39:16,970:INFO:Copying training dataset
2024-09-23 14:39:16,970:INFO:Plot type: pr
2024-09-23 14:39:17,142:INFO:Fitting Model
2024-09-23 14:39:51,954:INFO:Scoring test/hold-out set
2024-09-23 14:39:52,220:INFO:Visual Rendered Successfully
2024-09-23 14:39:52,313:INFO:plot_model() successfully completed......................................
2024-09-23 14:39:56,641:INFO:Initializing plot_model()
2024-09-23 14:39:56,641:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=class_report, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:39:56,641:INFO:Checking exceptions
2024-09-23 14:39:56,658:INFO:Preloading libraries
2024-09-23 14:39:56,690:INFO:Copying training dataset
2024-09-23 14:39:56,690:INFO:Plot type: class_report
2024-09-23 14:39:56,845:INFO:Fitting Model
2024-09-23 14:39:56,845:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 14:39:56,846:INFO:Scoring test/hold-out set
2024-09-23 14:39:57,074:INFO:Visual Rendered Successfully
2024-09-23 14:39:57,169:INFO:plot_model() successfully completed......................................
2024-09-23 14:44:49,100:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 14:44:49,101:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 14:44:49,101:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 14:44:49,101:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 14:46:29,541:INFO:PyCaret ClassificationExperiment
2024-09-23 14:46:29,541:INFO:Logging name: clf-default-name
2024-09-23 14:46:29,542:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 14:46:29,542:INFO:version 3.3.1
2024-09-23 14:46:29,542:INFO:Initializing setup()
2024-09-23 14:46:29,542:INFO:self.USI: 8d2f
2024-09-23 14:46:29,542:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 14:46:29,542:INFO:Checking environment
2024-09-23 14:46:29,542:INFO:python_version: 3.11.9
2024-09-23 14:46:29,542:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 14:46:29,542:INFO:machine: AMD64
2024-09-23 14:46:29,543:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 14:46:29,543:INFO:Memory: svmem(total=16853458944, available=7132889088, percent=57.7, used=9720569856, free=7132889088)
2024-09-23 14:46:29,543:INFO:Physical Core: 16
2024-09-23 14:46:29,543:INFO:Logical Core: 24
2024-09-23 14:46:29,543:INFO:Checking libraries
2024-09-23 14:46:29,543:INFO:System:
2024-09-23 14:46:29,543:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 14:46:29,543:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 14:46:29,543:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 14:46:29,543:INFO:PyCaret required dependencies:
2024-09-23 14:46:29,575:INFO:                 pip: 24.2
2024-09-23 14:46:29,575:INFO:          setuptools: 75.1.0
2024-09-23 14:46:29,575:INFO:             pycaret: 3.3.1
2024-09-23 14:46:29,575:INFO:             IPython: 8.27.0
2024-09-23 14:46:29,575:INFO:          ipywidgets: 8.1.2
2024-09-23 14:46:29,575:INFO:                tqdm: 4.66.5
2024-09-23 14:46:29,575:INFO:               numpy: 1.26.4
2024-09-23 14:46:29,575:INFO:              pandas: 2.1.4
2024-09-23 14:46:29,575:INFO:              jinja2: 3.1.4
2024-09-23 14:46:29,575:INFO:               scipy: 1.11.4
2024-09-23 14:46:29,575:INFO:              joblib: 1.2.0
2024-09-23 14:46:29,575:INFO:             sklearn: 1.4.2
2024-09-23 14:46:29,575:INFO:                pyod: 2.0.2
2024-09-23 14:46:29,575:INFO:            imblearn: 0.12.3
2024-09-23 14:46:29,575:INFO:   category_encoders: 2.6.3
2024-09-23 14:46:29,575:INFO:            lightgbm: 4.5.0
2024-09-23 14:46:29,575:INFO:               numba: 0.60.0
2024-09-23 14:46:29,575:INFO:            requests: 2.32.3
2024-09-23 14:46:29,575:INFO:          matplotlib: 3.9.2
2024-09-23 14:46:29,575:INFO:          scikitplot: 0.3.7
2024-09-23 14:46:29,575:INFO:         yellowbrick: 1.5
2024-09-23 14:46:29,575:INFO:              plotly: 5.24.1
2024-09-23 14:46:29,575:INFO:    plotly-resampler: Not installed
2024-09-23 14:46:29,575:INFO:             kaleido: 0.2.1
2024-09-23 14:46:29,575:INFO:           schemdraw: 0.15
2024-09-23 14:46:29,575:INFO:         statsmodels: 0.14.2
2024-09-23 14:46:29,575:INFO:              sktime: 0.26.0
2024-09-23 14:46:29,575:INFO:               tbats: 1.1.3
2024-09-23 14:46:29,575:INFO:            pmdarima: 2.0.4
2024-09-23 14:46:29,575:INFO:              psutil: 5.9.0
2024-09-23 14:46:29,575:INFO:          markupsafe: 2.1.3
2024-09-23 14:46:29,575:INFO:             pickle5: Not installed
2024-09-23 14:46:29,575:INFO:         cloudpickle: 3.0.0
2024-09-23 14:46:29,575:INFO:         deprecation: 2.1.0
2024-09-23 14:46:29,575:INFO:              xxhash: 2.0.2
2024-09-23 14:46:29,575:INFO:           wurlitzer: 3.1.1
2024-09-23 14:46:29,575:INFO:PyCaret optional dependencies:
2024-09-23 14:46:29,593:INFO:                shap: Not installed
2024-09-23 14:46:29,593:INFO:           interpret: Not installed
2024-09-23 14:46:29,593:INFO:                umap: Not installed
2024-09-23 14:46:29,593:INFO:     ydata_profiling: Not installed
2024-09-23 14:46:29,593:INFO:  explainerdashboard: Not installed
2024-09-23 14:46:29,593:INFO:             autoviz: Not installed
2024-09-23 14:46:29,593:INFO:           fairlearn: Not installed
2024-09-23 14:46:29,593:INFO:          deepchecks: Not installed
2024-09-23 14:46:29,593:INFO:             xgboost: 2.1.1
2024-09-23 14:46:29,593:INFO:            catboost: 1.2.7
2024-09-23 14:46:29,593:INFO:              kmodes: Not installed
2024-09-23 14:46:29,593:INFO:             mlxtend: Not installed
2024-09-23 14:46:29,593:INFO:       statsforecast: Not installed
2024-09-23 14:46:29,593:INFO:        tune_sklearn: Not installed
2024-09-23 14:46:29,593:INFO:                 ray: Not installed
2024-09-23 14:46:29,593:INFO:            hyperopt: Not installed
2024-09-23 14:46:29,593:INFO:              optuna: Not installed
2024-09-23 14:46:29,593:INFO:               skopt: Not installed
2024-09-23 14:46:29,593:INFO:              mlflow: Not installed
2024-09-23 14:46:29,593:INFO:              gradio: Not installed
2024-09-23 14:46:29,593:INFO:             fastapi: Not installed
2024-09-23 14:46:29,593:INFO:             uvicorn: Not installed
2024-09-23 14:46:29,593:INFO:              m2cgen: Not installed
2024-09-23 14:46:29,593:INFO:           evidently: Not installed
2024-09-23 14:46:29,593:INFO:               fugue: Not installed
2024-09-23 14:46:29,593:INFO:           streamlit: Not installed
2024-09-23 14:46:29,593:INFO:             prophet: Not installed
2024-09-23 14:46:29,593:INFO:None
2024-09-23 14:46:29,593:INFO:Set up data.
2024-09-23 14:46:29,606:INFO:Set up folding strategy.
2024-09-23 14:46:29,606:INFO:Set up train/test split.
2024-09-23 14:46:29,631:INFO:Set up index.
2024-09-23 14:46:29,632:INFO:Assigning column types.
2024-09-23 14:46:29,642:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 14:46:29,654:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 14:46:29,654:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 14:46:29,684:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:29,685:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:29,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 14:46:29,721:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 14:46:29,737:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:29,737:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:29,737:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 14:46:29,752:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 14:46:29,776:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:29,777:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:29,798:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 14:46:29,812:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:29,813:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:29,814:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 14:46:29,839:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:29,839:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:29,888:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:29,892:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:29,896:INFO:Preparing preprocessing pipeline...
2024-09-23 14:46:29,900:INFO:Set up simple imputation.
2024-09-23 14:46:29,900:INFO:Set up feature normalization.
2024-09-23 14:46:29,903:INFO:Set up column name cleaning.
2024-09-23 14:46:29,937:INFO:Finished creating preprocessing pipeline.
2024-09-23 14:46:29,952:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hi...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 14:46:29,952:INFO:Creating final display dataframe.
2024-09-23 14:46:30,077:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 26)
4        Transformed data shape       (30380, 26)
5   Transformed train set shape       (21266, 26)
6    Transformed test set shape        (9114, 26)
7              Numeric features                25
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              8d2f
2024-09-23 14:46:30,126:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:30,126:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:30,179:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 14:46:30,180:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 14:46:30,182:INFO:setup() successfully completed in 0.66s...............
2024-09-23 14:46:36,364:INFO:Initializing compare_models()
2024-09-23 14:46:36,364:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 14:46:36,364:INFO:Checking exceptions
2024-09-23 14:46:36,371:INFO:Preparing display monitor
2024-09-23 14:46:36,388:INFO:Initializing Logistic Regression
2024-09-23 14:46:36,388:INFO:Total runtime is 0.0 minutes
2024-09-23 14:46:36,405:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:36,405:INFO:Initializing create_model()
2024-09-23 14:46:36,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:36,405:INFO:Checking exceptions
2024-09-23 14:46:36,405:INFO:Importing libraries
2024-09-23 14:46:36,405:INFO:Copying training dataset
2024-09-23 14:46:36,421:INFO:Defining folds
2024-09-23 14:46:36,421:INFO:Declaring metric variables
2024-09-23 14:46:36,421:INFO:Importing untrained model
2024-09-23 14:46:36,421:INFO:Logistic Regression Imported successfully
2024-09-23 14:46:36,438:INFO:Starting cross validation
2024-09-23 14:46:36,438:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:39,484:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,499:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,546:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,577:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,577:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,671:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,687:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,687:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,687:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,702:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,702:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,812:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,812:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,812:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:39,812:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:39,828:INFO:Calculating mean and std
2024-09-23 14:46:39,829:INFO:Creating metrics dataframe
2024-09-23 14:46:39,830:INFO:Uploading results into container
2024-09-23 14:46:39,830:INFO:Uploading model into container now
2024-09-23 14:46:39,832:INFO:_master_model_container: 1
2024-09-23 14:46:39,832:INFO:_display_container: 2
2024-09-23 14:46:39,832:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 14:46:39,832:INFO:create_model() successfully completed......................................
2024-09-23 14:46:39,888:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:39,888:INFO:Creating metrics dataframe
2024-09-23 14:46:39,904:INFO:Initializing K Neighbors Classifier
2024-09-23 14:46:39,904:INFO:Total runtime is 0.05859649578730265 minutes
2024-09-23 14:46:39,904:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:39,904:INFO:Initializing create_model()
2024-09-23 14:46:39,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:39,904:INFO:Checking exceptions
2024-09-23 14:46:39,904:INFO:Importing libraries
2024-09-23 14:46:39,909:INFO:Copying training dataset
2024-09-23 14:46:39,923:INFO:Defining folds
2024-09-23 14:46:39,923:INFO:Declaring metric variables
2024-09-23 14:46:39,923:INFO:Importing untrained model
2024-09-23 14:46:39,923:INFO:K Neighbors Classifier Imported successfully
2024-09-23 14:46:39,923:INFO:Starting cross validation
2024-09-23 14:46:39,923:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:42,004:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:42,066:INFO:Calculating mean and std
2024-09-23 14:46:42,082:INFO:Creating metrics dataframe
2024-09-23 14:46:42,082:INFO:Uploading results into container
2024-09-23 14:46:42,082:INFO:Uploading model into container now
2024-09-23 14:46:42,082:INFO:_master_model_container: 2
2024-09-23 14:46:42,082:INFO:_display_container: 2
2024-09-23 14:46:42,082:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 14:46:42,082:INFO:create_model() successfully completed......................................
2024-09-23 14:46:42,155:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:42,155:INFO:Creating metrics dataframe
2024-09-23 14:46:42,155:INFO:Initializing Naive Bayes
2024-09-23 14:46:42,155:INFO:Total runtime is 0.09611676931381224 minutes
2024-09-23 14:46:42,155:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:42,155:INFO:Initializing create_model()
2024-09-23 14:46:42,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:42,155:INFO:Checking exceptions
2024-09-23 14:46:42,155:INFO:Importing libraries
2024-09-23 14:46:42,155:INFO:Copying training dataset
2024-09-23 14:46:42,174:INFO:Defining folds
2024-09-23 14:46:42,174:INFO:Declaring metric variables
2024-09-23 14:46:42,188:INFO:Importing untrained model
2024-09-23 14:46:42,188:INFO:Naive Bayes Imported successfully
2024-09-23 14:46:42,188:INFO:Starting cross validation
2024-09-23 14:46:42,188:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:42,319:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:43,621:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:43,621:INFO:Calculating mean and std
2024-09-23 14:46:43,637:INFO:Creating metrics dataframe
2024-09-23 14:46:43,637:INFO:Uploading results into container
2024-09-23 14:46:43,637:INFO:Uploading model into container now
2024-09-23 14:46:43,637:INFO:_master_model_container: 3
2024-09-23 14:46:43,637:INFO:_display_container: 2
2024-09-23 14:46:43,637:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 14:46:43,637:INFO:create_model() successfully completed......................................
2024-09-23 14:46:43,704:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:43,704:INFO:Creating metrics dataframe
2024-09-23 14:46:43,708:INFO:Initializing Decision Tree Classifier
2024-09-23 14:46:43,708:INFO:Total runtime is 0.12200102011362711 minutes
2024-09-23 14:46:43,708:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:43,708:INFO:Initializing create_model()
2024-09-23 14:46:43,708:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:43,708:INFO:Checking exceptions
2024-09-23 14:46:43,708:INFO:Importing libraries
2024-09-23 14:46:43,708:INFO:Copying training dataset
2024-09-23 14:46:43,724:INFO:Defining folds
2024-09-23 14:46:43,724:INFO:Declaring metric variables
2024-09-23 14:46:43,724:INFO:Importing untrained model
2024-09-23 14:46:43,724:INFO:Decision Tree Classifier Imported successfully
2024-09-23 14:46:43,724:INFO:Starting cross validation
2024-09-23 14:46:43,724:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:44,075:INFO:Calculating mean and std
2024-09-23 14:46:44,075:INFO:Creating metrics dataframe
2024-09-23 14:46:44,075:INFO:Uploading results into container
2024-09-23 14:46:44,075:INFO:Uploading model into container now
2024-09-23 14:46:44,075:INFO:_master_model_container: 4
2024-09-23 14:46:44,075:INFO:_display_container: 2
2024-09-23 14:46:44,088:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 14:46:44,088:INFO:create_model() successfully completed......................................
2024-09-23 14:46:44,171:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:44,171:INFO:Creating metrics dataframe
2024-09-23 14:46:44,171:INFO:Initializing SVM - Linear Kernel
2024-09-23 14:46:44,171:INFO:Total runtime is 0.12971747716267903 minutes
2024-09-23 14:46:44,187:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:44,187:INFO:Initializing create_model()
2024-09-23 14:46:44,187:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:44,187:INFO:Checking exceptions
2024-09-23 14:46:44,187:INFO:Importing libraries
2024-09-23 14:46:44,189:INFO:Copying training dataset
2024-09-23 14:46:44,207:INFO:Defining folds
2024-09-23 14:46:44,207:INFO:Declaring metric variables
2024-09-23 14:46:44,207:INFO:Importing untrained model
2024-09-23 14:46:44,207:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 14:46:44,207:INFO:Starting cross validation
2024-09-23 14:46:44,207:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:44,457:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,502:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,502:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,570:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,570:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,570:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,581:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,581:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,581:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,596:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,596:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,596:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,659:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,659:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,659:INFO:Calculating mean and std
2024-09-23 14:46:44,675:INFO:Creating metrics dataframe
2024-09-23 14:46:44,675:INFO:Uploading results into container
2024-09-23 14:46:44,675:INFO:Uploading model into container now
2024-09-23 14:46:44,675:INFO:_master_model_container: 5
2024-09-23 14:46:44,675:INFO:_display_container: 2
2024-09-23 14:46:44,675:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 14:46:44,675:INFO:create_model() successfully completed......................................
2024-09-23 14:46:44,755:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:44,755:INFO:Creating metrics dataframe
2024-09-23 14:46:44,760:INFO:Initializing Ridge Classifier
2024-09-23 14:46:44,760:INFO:Total runtime is 0.1395250598589579 minutes
2024-09-23 14:46:44,762:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:44,762:INFO:Initializing create_model()
2024-09-23 14:46:44,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:44,763:INFO:Checking exceptions
2024-09-23 14:46:44,763:INFO:Importing libraries
2024-09-23 14:46:44,763:INFO:Copying training dataset
2024-09-23 14:46:44,773:INFO:Defining folds
2024-09-23 14:46:44,773:INFO:Declaring metric variables
2024-09-23 14:46:44,773:INFO:Importing untrained model
2024-09-23 14:46:44,773:INFO:Ridge Classifier Imported successfully
2024-09-23 14:46:44,773:INFO:Starting cross validation
2024-09-23 14:46:44,773:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:44,902:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,902:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,949:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,949:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,949:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:44,964:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:44,964:INFO:Calculating mean and std
2024-09-23 14:46:44,964:INFO:Creating metrics dataframe
2024-09-23 14:46:44,964:INFO:Uploading results into container
2024-09-23 14:46:44,964:INFO:Uploading model into container now
2024-09-23 14:46:44,964:INFO:_master_model_container: 6
2024-09-23 14:46:44,964:INFO:_display_container: 2
2024-09-23 14:46:44,964:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 14:46:44,964:INFO:create_model() successfully completed......................................
2024-09-23 14:46:45,036:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:45,036:INFO:Creating metrics dataframe
2024-09-23 14:46:45,046:INFO:Initializing Random Forest Classifier
2024-09-23 14:46:45,046:INFO:Total runtime is 0.14429476261138915 minutes
2024-09-23 14:46:45,048:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:45,048:INFO:Initializing create_model()
2024-09-23 14:46:45,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:45,049:INFO:Checking exceptions
2024-09-23 14:46:45,049:INFO:Importing libraries
2024-09-23 14:46:45,049:INFO:Copying training dataset
2024-09-23 14:46:45,056:INFO:Defining folds
2024-09-23 14:46:45,056:INFO:Declaring metric variables
2024-09-23 14:46:45,056:INFO:Importing untrained model
2024-09-23 14:46:45,056:INFO:Random Forest Classifier Imported successfully
2024-09-23 14:46:45,056:INFO:Starting cross validation
2024-09-23 14:46:45,056:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:47,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,261:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,261:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,261:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,276:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:46:47,276:INFO:Calculating mean and std
2024-09-23 14:46:47,292:INFO:Creating metrics dataframe
2024-09-23 14:46:47,292:INFO:Uploading results into container
2024-09-23 14:46:47,292:INFO:Uploading model into container now
2024-09-23 14:46:47,292:INFO:_master_model_container: 7
2024-09-23 14:46:47,292:INFO:_display_container: 2
2024-09-23 14:46:47,292:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 14:46:47,292:INFO:create_model() successfully completed......................................
2024-09-23 14:46:47,390:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:47,390:INFO:Creating metrics dataframe
2024-09-23 14:46:47,395:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 14:46:47,395:INFO:Total runtime is 0.18343750238418577 minutes
2024-09-23 14:46:47,397:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:47,397:INFO:Initializing create_model()
2024-09-23 14:46:47,397:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:47,397:INFO:Checking exceptions
2024-09-23 14:46:47,397:INFO:Importing libraries
2024-09-23 14:46:47,397:INFO:Copying training dataset
2024-09-23 14:46:47,408:INFO:Defining folds
2024-09-23 14:46:47,408:INFO:Declaring metric variables
2024-09-23 14:46:47,408:INFO:Importing untrained model
2024-09-23 14:46:47,408:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 14:46:47,408:INFO:Starting cross validation
2024-09-23 14:46:47,408:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:47,488:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,488:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,522:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,531:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,553:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,553:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,569:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,569:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,569:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,569:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,569:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,585:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,585:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,585:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 14:46:47,616:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:47,631:INFO:Calculating mean and std
2024-09-23 14:46:47,631:INFO:Creating metrics dataframe
2024-09-23 14:46:47,631:INFO:Uploading results into container
2024-09-23 14:46:47,631:INFO:Uploading model into container now
2024-09-23 14:46:47,631:INFO:_master_model_container: 8
2024-09-23 14:46:47,631:INFO:_display_container: 2
2024-09-23 14:46:47,631:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 14:46:47,631:INFO:create_model() successfully completed......................................
2024-09-23 14:46:47,704:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:47,704:INFO:Creating metrics dataframe
2024-09-23 14:46:47,704:INFO:Initializing Ada Boost Classifier
2024-09-23 14:46:47,704:INFO:Total runtime is 0.18858663638432818 minutes
2024-09-23 14:46:47,704:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:47,704:INFO:Initializing create_model()
2024-09-23 14:46:47,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:47,704:INFO:Checking exceptions
2024-09-23 14:46:47,704:INFO:Importing libraries
2024-09-23 14:46:47,704:INFO:Copying training dataset
2024-09-23 14:46:47,725:INFO:Defining folds
2024-09-23 14:46:47,725:INFO:Declaring metric variables
2024-09-23 14:46:47,725:INFO:Importing untrained model
2024-09-23 14:46:47,725:INFO:Ada Boost Classifier Imported successfully
2024-09-23 14:46:47,725:INFO:Starting cross validation
2024-09-23 14:46:47,725:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:46:47,788:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,851:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,851:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,866:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,866:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:47,866:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 14:46:48,870:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:48,917:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:48,964:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:48,980:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:48,980:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:49,010:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:49,031:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:49,085:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:49,094:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:49,154:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:46:49,154:INFO:Calculating mean and std
2024-09-23 14:46:49,154:INFO:Creating metrics dataframe
2024-09-23 14:46:49,154:INFO:Uploading results into container
2024-09-23 14:46:49,154:INFO:Uploading model into container now
2024-09-23 14:46:49,154:INFO:_master_model_container: 9
2024-09-23 14:46:49,154:INFO:_display_container: 2
2024-09-23 14:46:49,154:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 14:46:49,154:INFO:create_model() successfully completed......................................
2024-09-23 14:46:49,222:INFO:SubProcess create_model() end ==================================
2024-09-23 14:46:49,222:INFO:Creating metrics dataframe
2024-09-23 14:46:49,222:INFO:Initializing Gradient Boosting Classifier
2024-09-23 14:46:49,222:INFO:Total runtime is 0.2138897895812988 minutes
2024-09-23 14:46:49,222:INFO:SubProcess create_model() called ==================================
2024-09-23 14:46:49,222:INFO:Initializing create_model()
2024-09-23 14:46:49,222:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:46:49,222:INFO:Checking exceptions
2024-09-23 14:46:49,222:INFO:Importing libraries
2024-09-23 14:46:49,222:INFO:Copying training dataset
2024-09-23 14:46:49,253:INFO:Defining folds
2024-09-23 14:46:49,253:INFO:Declaring metric variables
2024-09-23 14:46:49,255:INFO:Importing untrained model
2024-09-23 14:46:49,256:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 14:46:49,256:INFO:Starting cross validation
2024-09-23 14:46:49,256:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:47:27,647:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:28,753:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:28,958:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:30,301:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:30,423:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:30,523:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:30,871:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,044:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,498:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,539:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,539:INFO:Calculating mean and std
2024-09-23 14:47:31,539:INFO:Creating metrics dataframe
2024-09-23 14:47:31,554:INFO:Uploading results into container
2024-09-23 14:47:31,555:INFO:Uploading model into container now
2024-09-23 14:47:31,555:INFO:_master_model_container: 10
2024-09-23 14:47:31,555:INFO:_display_container: 2
2024-09-23 14:47:31,555:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 14:47:31,555:INFO:create_model() successfully completed......................................
2024-09-23 14:47:31,616:INFO:SubProcess create_model() end ==================================
2024-09-23 14:47:31,616:INFO:Creating metrics dataframe
2024-09-23 14:47:31,620:INFO:Initializing Linear Discriminant Analysis
2024-09-23 14:47:31,620:INFO:Total runtime is 0.920529572168986 minutes
2024-09-23 14:47:31,622:INFO:SubProcess create_model() called ==================================
2024-09-23 14:47:31,622:INFO:Initializing create_model()
2024-09-23 14:47:31,622:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:47:31,622:INFO:Checking exceptions
2024-09-23 14:47:31,622:INFO:Importing libraries
2024-09-23 14:47:31,622:INFO:Copying training dataset
2024-09-23 14:47:31,634:INFO:Defining folds
2024-09-23 14:47:31,634:INFO:Declaring metric variables
2024-09-23 14:47:31,635:INFO:Importing untrained model
2024-09-23 14:47:31,638:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 14:47:31,640:INFO:Starting cross validation
2024-09-23 14:47:31,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:47:31,760:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,768:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:31,780:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,797:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,798:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,805:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,808:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,822:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,832:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,856:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,861:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 14:47:31,872:INFO:Calculating mean and std
2024-09-23 14:47:31,873:INFO:Creating metrics dataframe
2024-09-23 14:47:31,874:INFO:Uploading results into container
2024-09-23 14:47:31,875:INFO:Uploading model into container now
2024-09-23 14:47:31,875:INFO:_master_model_container: 11
2024-09-23 14:47:31,875:INFO:_display_container: 2
2024-09-23 14:47:31,875:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 14:47:31,875:INFO:create_model() successfully completed......................................
2024-09-23 14:47:31,949:INFO:SubProcess create_model() end ==================================
2024-09-23 14:47:31,949:INFO:Creating metrics dataframe
2024-09-23 14:47:31,955:INFO:Initializing Extra Trees Classifier
2024-09-23 14:47:31,955:INFO:Total runtime is 0.9261145035425822 minutes
2024-09-23 14:47:31,957:INFO:SubProcess create_model() called ==================================
2024-09-23 14:47:31,957:INFO:Initializing create_model()
2024-09-23 14:47:31,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:47:31,957:INFO:Checking exceptions
2024-09-23 14:47:31,957:INFO:Importing libraries
2024-09-23 14:47:31,957:INFO:Copying training dataset
2024-09-23 14:47:31,969:INFO:Defining folds
2024-09-23 14:47:31,969:INFO:Declaring metric variables
2024-09-23 14:47:31,971:INFO:Importing untrained model
2024-09-23 14:47:31,973:INFO:Extra Trees Classifier Imported successfully
2024-09-23 14:47:31,976:INFO:Starting cross validation
2024-09-23 14:47:31,976:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:47:34,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,436:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,436:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,436:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,467:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,483:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:34,483:INFO:Calculating mean and std
2024-09-23 14:47:34,499:INFO:Creating metrics dataframe
2024-09-23 14:47:34,499:INFO:Uploading results into container
2024-09-23 14:47:34,499:INFO:Uploading model into container now
2024-09-23 14:47:34,503:INFO:_master_model_container: 12
2024-09-23 14:47:34,503:INFO:_display_container: 2
2024-09-23 14:47:34,503:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 14:47:34,503:INFO:create_model() successfully completed......................................
2024-09-23 14:47:34,602:INFO:SubProcess create_model() end ==================================
2024-09-23 14:47:34,602:INFO:Creating metrics dataframe
2024-09-23 14:47:34,602:INFO:Initializing Extreme Gradient Boosting
2024-09-23 14:47:34,602:INFO:Total runtime is 0.9702236572901408 minutes
2024-09-23 14:47:34,617:INFO:SubProcess create_model() called ==================================
2024-09-23 14:47:34,617:INFO:Initializing create_model()
2024-09-23 14:47:34,618:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:47:34,618:INFO:Checking exceptions
2024-09-23 14:47:34,618:INFO:Importing libraries
2024-09-23 14:47:34,618:INFO:Copying training dataset
2024-09-23 14:47:34,632:INFO:Defining folds
2024-09-23 14:47:34,632:INFO:Declaring metric variables
2024-09-23 14:47:34,639:INFO:Importing untrained model
2024-09-23 14:47:34,640:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 14:47:34,640:INFO:Starting cross validation
2024-09-23 14:47:34,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:47:37,992:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,039:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,055:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,090:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,124:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,174:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,190:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,224:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:38,240:INFO:Calculating mean and std
2024-09-23 14:47:38,240:INFO:Creating metrics dataframe
2024-09-23 14:47:38,240:INFO:Uploading results into container
2024-09-23 14:47:38,240:INFO:Uploading model into container now
2024-09-23 14:47:38,240:INFO:_master_model_container: 13
2024-09-23 14:47:38,240:INFO:_display_container: 2
2024-09-23 14:47:38,240:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 14:47:38,240:INFO:create_model() successfully completed......................................
2024-09-23 14:47:38,309:INFO:SubProcess create_model() end ==================================
2024-09-23 14:47:38,309:INFO:Creating metrics dataframe
2024-09-23 14:47:38,324:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 14:47:38,324:INFO:Total runtime is 1.0322587728500368 minutes
2024-09-23 14:47:38,326:INFO:SubProcess create_model() called ==================================
2024-09-23 14:47:38,326:INFO:Initializing create_model()
2024-09-23 14:47:38,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:47:38,326:INFO:Checking exceptions
2024-09-23 14:47:38,326:INFO:Importing libraries
2024-09-23 14:47:38,326:INFO:Copying training dataset
2024-09-23 14:47:38,339:INFO:Defining folds
2024-09-23 14:47:38,339:INFO:Declaring metric variables
2024-09-23 14:47:38,340:INFO:Importing untrained model
2024-09-23 14:47:38,340:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 14:47:38,347:INFO:Starting cross validation
2024-09-23 14:47:38,349:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:47:53,577:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:56,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:56,799:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:56,830:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:56,842:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:57,092:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:57,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:47:57,151:INFO:Calculating mean and std
2024-09-23 14:47:57,152:INFO:Creating metrics dataframe
2024-09-23 14:47:57,154:INFO:Uploading results into container
2024-09-23 14:47:57,154:INFO:Uploading model into container now
2024-09-23 14:47:57,155:INFO:_master_model_container: 14
2024-09-23 14:47:57,155:INFO:_display_container: 2
2024-09-23 14:47:57,156:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 14:47:57,156:INFO:create_model() successfully completed......................................
2024-09-23 14:47:57,256:INFO:SubProcess create_model() end ==================================
2024-09-23 14:47:57,256:INFO:Creating metrics dataframe
2024-09-23 14:47:57,268:INFO:Initializing CatBoost Classifier
2024-09-23 14:47:57,268:INFO:Total runtime is 1.3479872544606528 minutes
2024-09-23 14:47:57,269:INFO:SubProcess create_model() called ==================================
2024-09-23 14:47:57,270:INFO:Initializing create_model()
2024-09-23 14:47:57,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:47:57,270:INFO:Checking exceptions
2024-09-23 14:47:57,270:INFO:Importing libraries
2024-09-23 14:47:57,270:INFO:Copying training dataset
2024-09-23 14:47:57,277:INFO:Defining folds
2024-09-23 14:47:57,277:INFO:Declaring metric variables
2024-09-23 14:47:57,289:INFO:Importing untrained model
2024-09-23 14:47:57,290:INFO:CatBoost Classifier Imported successfully
2024-09-23 14:47:57,294:INFO:Starting cross validation
2024-09-23 14:47:57,295:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:49:05,854:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:05,854:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:05,875:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:05,905:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,101:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,101:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,133:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,133:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,148:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,164:INFO:Calculating mean and std
2024-09-23 14:49:06,164:INFO:Creating metrics dataframe
2024-09-23 14:49:06,164:INFO:Uploading results into container
2024-09-23 14:49:06,164:INFO:Uploading model into container now
2024-09-23 14:49:06,164:INFO:_master_model_container: 15
2024-09-23 14:49:06,164:INFO:_display_container: 2
2024-09-23 14:49:06,164:INFO:<catboost.core.CatBoostClassifier object at 0x0000020063352F50>
2024-09-23 14:49:06,164:INFO:create_model() successfully completed......................................
2024-09-23 14:49:06,242:INFO:SubProcess create_model() end ==================================
2024-09-23 14:49:06,242:INFO:Creating metrics dataframe
2024-09-23 14:49:06,258:INFO:Initializing Dummy Classifier
2024-09-23 14:49:06,258:INFO:Total runtime is 2.497820989290873 minutes
2024-09-23 14:49:06,258:INFO:SubProcess create_model() called ==================================
2024-09-23 14:49:06,258:INFO:Initializing create_model()
2024-09-23 14:49:06,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067C83C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:49:06,258:INFO:Checking exceptions
2024-09-23 14:49:06,258:INFO:Importing libraries
2024-09-23 14:49:06,258:INFO:Copying training dataset
2024-09-23 14:49:06,274:INFO:Defining folds
2024-09-23 14:49:06,274:INFO:Declaring metric variables
2024-09-23 14:49:06,276:INFO:Importing untrained model
2024-09-23 14:49:06,277:INFO:Dummy Classifier Imported successfully
2024-09-23 14:49:06,280:INFO:Starting cross validation
2024-09-23 14:49:06,281:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:49:06,352:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,352:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,394:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,399:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,410:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,422:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,422:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,438:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,453:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:49:06,453:INFO:Calculating mean and std
2024-09-23 14:49:06,453:INFO:Creating metrics dataframe
2024-09-23 14:49:06,453:INFO:Uploading results into container
2024-09-23 14:49:06,453:INFO:Uploading model into container now
2024-09-23 14:49:06,453:INFO:_master_model_container: 16
2024-09-23 14:49:06,453:INFO:_display_container: 2
2024-09-23 14:49:06,453:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 14:49:06,453:INFO:create_model() successfully completed......................................
2024-09-23 14:49:06,541:INFO:SubProcess create_model() end ==================================
2024-09-23 14:49:06,541:INFO:Creating metrics dataframe
2024-09-23 14:49:06,541:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 14:49:06,554:INFO:Initializing create_model()
2024-09-23 14:49:06,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020063352F50>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:49:06,554:INFO:Checking exceptions
2024-09-23 14:49:06,555:INFO:Importing libraries
2024-09-23 14:49:06,555:INFO:Copying training dataset
2024-09-23 14:49:06,557:INFO:Defining folds
2024-09-23 14:49:06,557:INFO:Declaring metric variables
2024-09-23 14:49:06,557:INFO:Importing untrained model
2024-09-23 14:49:06,557:INFO:Declaring custom model
2024-09-23 14:49:06,557:INFO:CatBoost Classifier Imported successfully
2024-09-23 14:49:06,557:INFO:Cross validation set to False
2024-09-23 14:49:06,557:INFO:Fitting Model
2024-09-23 14:49:16,966:INFO:<catboost.core.CatBoostClassifier object at 0x000002006795FD90>
2024-09-23 14:49:16,966:INFO:create_model() successfully completed......................................
2024-09-23 14:49:17,057:INFO:_master_model_container: 16
2024-09-23 14:49:17,057:INFO:_display_container: 2
2024-09-23 14:49:17,057:INFO:<catboost.core.CatBoostClassifier object at 0x000002006795FD90>
2024-09-23 14:49:17,057:INFO:compare_models() successfully completed......................................
2024-09-23 14:50:38,605:INFO:Initializing create_model()
2024-09-23 14:50:38,606:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:50:38,606:INFO:Checking exceptions
2024-09-23 14:50:38,622:INFO:Importing libraries
2024-09-23 14:50:38,622:INFO:Copying training dataset
2024-09-23 14:50:38,638:INFO:Defining folds
2024-09-23 14:50:38,638:INFO:Declaring metric variables
2024-09-23 14:50:38,638:INFO:Importing untrained model
2024-09-23 14:50:38,654:INFO:CatBoost Classifier Imported successfully
2024-09-23 14:50:38,654:INFO:Starting cross validation
2024-09-23 14:50:38,654:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:51:46,532:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:46,757:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:46,897:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:47,101:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:47,163:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:47,751:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:48,645:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:48,835:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:48,835:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:51:48,851:INFO:Calculating mean and std
2024-09-23 14:51:48,851:INFO:Creating metrics dataframe
2024-09-23 14:51:48,851:INFO:Finalizing model
2024-09-23 14:51:58,740:INFO:Uploading results into container
2024-09-23 14:51:58,740:INFO:Uploading model into container now
2024-09-23 14:51:58,752:INFO:_master_model_container: 17
2024-09-23 14:51:58,753:INFO:_display_container: 3
2024-09-23 14:51:58,753:INFO:<catboost.core.CatBoostClassifier object at 0x0000020067DAFD50>
2024-09-23 14:51:58,753:INFO:create_model() successfully completed......................................
2024-09-23 14:53:30,862:INFO:Initializing tune_model()
2024-09-23 14:53:30,862:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020067DAFD50>, fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-23 14:53:30,862:INFO:Checking exceptions
2024-09-23 14:53:30,887:INFO:Copying training dataset
2024-09-23 14:53:30,893:INFO:Checking base model
2024-09-23 14:53:30,893:INFO:Base model : CatBoost Classifier
2024-09-23 14:53:30,904:INFO:Declaring metric variables
2024-09-23 14:53:30,904:INFO:Defining Hyperparameters
2024-09-23 14:53:30,991:INFO:Tuning with n_jobs=-1
2024-09-23 14:53:30,991:INFO:Initializing RandomizedSearchCV
2024-09-23 14:55:04,535:INFO:best_params: {'actual_estimator__random_strength': 0.7, 'actual_estimator__n_estimators': 250, 'actual_estimator__l2_leaf_reg': 50, 'actual_estimator__eta': 0.15, 'actual_estimator__depth': 3}
2024-09-23 14:55:04,535:INFO:Hyperparameter search completed
2024-09-23 14:55:04,535:INFO:SubProcess create_model() called ==================================
2024-09-23 14:55:04,551:INFO:Initializing create_model()
2024-09-23 14:55:04,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020067C61010>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006554E950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'random_strength': 0.7, 'n_estimators': 250, 'l2_leaf_reg': 50, 'eta': 0.15, 'depth': 3})
2024-09-23 14:55:04,551:INFO:Checking exceptions
2024-09-23 14:55:04,551:INFO:Importing libraries
2024-09-23 14:55:04,551:INFO:Copying training dataset
2024-09-23 14:55:04,571:INFO:Defining folds
2024-09-23 14:55:04,571:INFO:Declaring metric variables
2024-09-23 14:55:04,571:INFO:Importing untrained model
2024-09-23 14:55:04,571:INFO:Declaring custom model
2024-09-23 14:55:04,571:INFO:CatBoost Classifier Imported successfully
2024-09-23 14:55:04,571:INFO:Starting cross validation
2024-09-23 14:55:04,571:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:55:09,986:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,017:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,033:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,040:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,080:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,095:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,111:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,111:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,111:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,126:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:55:10,126:INFO:Calculating mean and std
2024-09-23 14:55:10,126:INFO:Creating metrics dataframe
2024-09-23 14:55:10,144:INFO:Finalizing model
2024-09-23 14:55:11,068:INFO:Uploading results into container
2024-09-23 14:55:11,068:INFO:Uploading model into container now
2024-09-23 14:55:11,069:INFO:_master_model_container: 18
2024-09-23 14:55:11,069:INFO:_display_container: 4
2024-09-23 14:55:11,069:INFO:<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>
2024-09-23 14:55:11,069:INFO:create_model() successfully completed......................................
2024-09-23 14:55:11,138:INFO:SubProcess create_model() end ==================================
2024-09-23 14:55:11,138:INFO:choose_better activated
2024-09-23 14:55:11,138:INFO:SubProcess create_model() called ==================================
2024-09-23 14:55:11,138:INFO:Initializing create_model()
2024-09-23 14:55:11,138:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020067DAFD50>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 14:55:11,138:INFO:Checking exceptions
2024-09-23 14:55:11,138:INFO:Importing libraries
2024-09-23 14:55:11,138:INFO:Copying training dataset
2024-09-23 14:55:11,176:INFO:Defining folds
2024-09-23 14:55:11,176:INFO:Declaring metric variables
2024-09-23 14:55:11,176:INFO:Importing untrained model
2024-09-23 14:55:11,176:INFO:Declaring custom model
2024-09-23 14:55:11,176:INFO:CatBoost Classifier Imported successfully
2024-09-23 14:55:11,177:INFO:Starting cross validation
2024-09-23 14:55:11,177:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 14:56:21,153:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,215:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,443:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,557:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,560:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,560:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,564:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,571:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,572:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:56:21,572:INFO:Calculating mean and std
2024-09-23 14:56:21,572:INFO:Creating metrics dataframe
2024-09-23 14:56:21,587:INFO:Finalizing model
2024-09-23 14:56:31,079:INFO:Uploading results into container
2024-09-23 14:56:31,079:INFO:Uploading model into container now
2024-09-23 14:56:31,079:INFO:_master_model_container: 19
2024-09-23 14:56:31,079:INFO:_display_container: 5
2024-09-23 14:56:31,079:INFO:<catboost.core.CatBoostClassifier object at 0x0000020067BC3590>
2024-09-23 14:56:31,079:INFO:create_model() successfully completed......................................
2024-09-23 14:56:31,141:INFO:SubProcess create_model() end ==================================
2024-09-23 14:56:31,141:INFO:<catboost.core.CatBoostClassifier object at 0x0000020067BC3590> result for Accuracy is 0.5519
2024-09-23 14:56:31,141:INFO:<catboost.core.CatBoostClassifier object at 0x000002006795DAD0> result for Accuracy is 0.5556
2024-09-23 14:56:31,141:INFO:<catboost.core.CatBoostClassifier object at 0x000002006795DAD0> is best model
2024-09-23 14:56:31,141:INFO:choose_better completed
2024-09-23 14:56:31,149:INFO:_master_model_container: 19
2024-09-23 14:56:31,149:INFO:_display_container: 4
2024-09-23 14:56:31,149:INFO:<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>
2024-09-23 14:56:31,149:INFO:tune_model() successfully completed......................................
2024-09-23 14:58:15,611:INFO:Initializing evaluate_model()
2024-09-23 14:58:15,612:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 14:58:15,632:INFO:Initializing plot_model()
2024-09-23 14:58:15,632:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:58:15,632:INFO:Checking exceptions
2024-09-23 14:58:15,633:INFO:Preloading libraries
2024-09-23 14:58:15,637:INFO:Copying training dataset
2024-09-23 14:58:15,637:INFO:Plot type: pipeline
2024-09-23 14:58:15,771:INFO:Visual Rendered Successfully
2024-09-23 14:58:15,833:INFO:plot_model() successfully completed......................................
2024-09-23 14:58:18,623:INFO:Initializing plot_model()
2024-09-23 14:58:18,624:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:58:18,624:INFO:Checking exceptions
2024-09-23 14:58:18,640:INFO:Preloading libraries
2024-09-23 14:58:18,640:INFO:Copying training dataset
2024-09-23 14:58:18,640:INFO:Plot type: parameter
2024-09-23 14:58:18,642:INFO:Visual Rendered Successfully
2024-09-23 14:58:18,738:INFO:plot_model() successfully completed......................................
2024-09-23 14:59:17,825:INFO:Initializing plot_model()
2024-09-23 14:59:17,825:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:59:17,825:INFO:Checking exceptions
2024-09-23 14:59:17,839:INFO:Preloading libraries
2024-09-23 14:59:17,840:INFO:Copying training dataset
2024-09-23 14:59:17,840:INFO:Plot type: confusion_matrix
2024-09-23 14:59:17,990:INFO:Fitting Model
2024-09-23 14:59:18,005:INFO:Scoring test/hold-out set
2024-09-23 14:59:18,167:INFO:Visual Rendered Successfully
2024-09-23 14:59:18,234:INFO:plot_model() successfully completed......................................
2024-09-23 14:59:35,721:INFO:Initializing plot_model()
2024-09-23 14:59:35,721:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:59:35,722:INFO:Checking exceptions
2024-09-23 14:59:35,740:INFO:Preloading libraries
2024-09-23 14:59:35,741:INFO:Copying training dataset
2024-09-23 14:59:35,741:INFO:Plot type: pr
2024-09-23 14:59:35,896:INFO:Fitting Model
2024-09-23 14:59:42,810:INFO:Scoring test/hold-out set
2024-09-23 14:59:42,983:INFO:Visual Rendered Successfully
2024-09-23 14:59:43,045:INFO:plot_model() successfully completed......................................
2024-09-23 14:59:43,059:INFO:Initializing plot_model()
2024-09-23 14:59:43,059:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=class_report, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 14:59:43,060:INFO:Checking exceptions
2024-09-23 14:59:43,060:INFO:Preloading libraries
2024-09-23 14:59:43,060:INFO:Copying training dataset
2024-09-23 14:59:43,060:INFO:Plot type: class_report
2024-09-23 14:59:43,194:INFO:Fitting Model
2024-09-23 14:59:43,194:INFO:Scoring test/hold-out set
2024-09-23 14:59:43,205:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 14:59:43,304:INFO:Visual Rendered Successfully
2024-09-23 14:59:43,383:INFO:plot_model() successfully completed......................................
2024-09-23 15:00:23,523:INFO:Initializing plot_model()
2024-09-23 15:00:23,523:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:00:23,523:INFO:Checking exceptions
2024-09-23 15:00:28,956:INFO:Initializing plot_model()
2024-09-23 15:00:28,956:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:00:28,956:INFO:Checking exceptions
2024-09-23 15:00:28,975:INFO:Preloading libraries
2024-09-23 15:00:28,975:INFO:Copying training dataset
2024-09-23 15:00:28,975:INFO:Plot type: learning
2024-09-23 15:00:29,100:INFO:Fitting Model
2024-09-23 15:00:56,505:INFO:Initializing plot_model()
2024-09-23 15:00:56,505:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:00:56,505:INFO:Checking exceptions
2024-09-23 15:00:56,506:INFO:Preloading libraries
2024-09-23 15:00:56,506:INFO:Copying training dataset
2024-09-23 15:00:56,506:INFO:Plot type: feature
2024-09-23 15:00:56,506:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 15:00:56,673:INFO:Visual Rendered Successfully
2024-09-23 15:00:56,758:INFO:plot_model() successfully completed......................................
2024-09-23 15:01:11,793:INFO:Initializing plot_model()
2024-09-23 15:01:11,793:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:01:11,793:INFO:Checking exceptions
2024-09-23 15:01:11,809:INFO:Preloading libraries
2024-09-23 15:01:11,809:INFO:Copying training dataset
2024-09-23 15:01:11,809:INFO:Plot type: feature_all
2024-09-23 15:01:11,870:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 15:01:12,020:INFO:Visual Rendered Successfully
2024-09-23 15:01:12,094:INFO:plot_model() successfully completed......................................
2024-09-23 15:01:17,345:INFO:Initializing plot_model()
2024-09-23 15:01:17,345:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:01:17,345:INFO:Checking exceptions
2024-09-23 15:01:17,358:INFO:Preloading libraries
2024-09-23 15:01:17,371:INFO:Copying training dataset
2024-09-23 15:01:17,371:INFO:Plot type: feature
2024-09-23 15:01:17,371:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 15:01:17,482:INFO:Visual Rendered Successfully
2024-09-23 15:01:17,564:INFO:plot_model() successfully completed......................................
2024-09-23 15:01:19,641:INFO:Initializing plot_model()
2024-09-23 15:01:19,641:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:01:19,641:INFO:Checking exceptions
2024-09-23 15:01:19,658:INFO:Preloading libraries
2024-09-23 15:01:19,658:INFO:Copying training dataset
2024-09-23 15:01:19,658:INFO:Plot type: feature_all
2024-09-23 15:01:19,715:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 15:01:19,869:INFO:Visual Rendered Successfully
2024-09-23 15:01:19,937:INFO:plot_model() successfully completed......................................
2024-09-23 15:01:24,455:INFO:Initializing plot_model()
2024-09-23 15:01:24,455:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020065C16290>, estimator=<catboost.core.CatBoostClassifier object at 0x000002006795DAD0>, plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:01:24,456:INFO:Checking exceptions
2024-09-23 15:01:24,456:INFO:Preloading libraries
2024-09-23 15:01:24,471:INFO:Copying training dataset
2024-09-23 15:01:24,472:INFO:Plot type: feature
2024-09-23 15:01:24,472:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 15:01:24,596:INFO:Visual Rendered Successfully
2024-09-23 15:01:24,684:INFO:plot_model() successfully completed......................................
2024-09-23 15:02:46,269:INFO:Initializing plot_model()
2024-09-23 15:02:46,269:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181ADCCC310>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 15:02:46,270:INFO:Checking exceptions
2024-09-23 15:02:46,275:INFO:Preloading libraries
2024-09-23 15:02:46,309:INFO:Copying training dataset
2024-09-23 15:02:46,309:INFO:Plot type: feature_all
2024-09-23 15:02:46,350:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 15:02:46,586:INFO:Visual Rendered Successfully
2024-09-23 15:02:46,681:INFO:plot_model() successfully completed......................................
2024-09-23 15:06:58,002:INFO:PyCaret ClassificationExperiment
2024-09-23 15:06:58,002:INFO:Logging name: clf-default-name
2024-09-23 15:06:58,002:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:06:58,003:INFO:version 3.3.1
2024-09-23 15:06:58,003:INFO:Initializing setup()
2024-09-23 15:06:58,003:INFO:self.USI: 089f
2024-09-23 15:06:58,004:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:06:58,004:INFO:Checking environment
2024-09-23 15:06:58,004:INFO:python_version: 3.11.9
2024-09-23 15:06:58,004:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:06:58,004:INFO:machine: AMD64
2024-09-23 15:06:58,004:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:06:58,004:INFO:Memory: svmem(total=16853458944, available=7832203264, percent=53.5, used=9021255680, free=7832203264)
2024-09-23 15:06:58,005:INFO:Physical Core: 16
2024-09-23 15:06:58,005:INFO:Logical Core: 24
2024-09-23 15:06:58,005:INFO:Checking libraries
2024-09-23 15:06:58,005:INFO:System:
2024-09-23 15:06:58,005:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:06:58,005:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:06:58,005:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:06:58,005:INFO:PyCaret required dependencies:
2024-09-23 15:06:58,005:INFO:                 pip: 24.2
2024-09-23 15:06:58,005:INFO:          setuptools: 75.1.0
2024-09-23 15:06:58,005:INFO:             pycaret: 3.3.1
2024-09-23 15:06:58,005:INFO:             IPython: 8.27.0
2024-09-23 15:06:58,005:INFO:          ipywidgets: 8.1.2
2024-09-23 15:06:58,005:INFO:                tqdm: 4.66.5
2024-09-23 15:06:58,005:INFO:               numpy: 1.26.4
2024-09-23 15:06:58,005:INFO:              pandas: 2.1.4
2024-09-23 15:06:58,005:INFO:              jinja2: 3.1.4
2024-09-23 15:06:58,005:INFO:               scipy: 1.11.4
2024-09-23 15:06:58,005:INFO:              joblib: 1.2.0
2024-09-23 15:06:58,005:INFO:             sklearn: 1.4.2
2024-09-23 15:06:58,005:INFO:                pyod: 2.0.2
2024-09-23 15:06:58,005:INFO:            imblearn: 0.12.3
2024-09-23 15:06:58,005:INFO:   category_encoders: 2.6.3
2024-09-23 15:06:58,005:INFO:            lightgbm: 4.5.0
2024-09-23 15:06:58,005:INFO:               numba: 0.60.0
2024-09-23 15:06:58,005:INFO:            requests: 2.32.3
2024-09-23 15:06:58,005:INFO:          matplotlib: 3.9.2
2024-09-23 15:06:58,005:INFO:          scikitplot: 0.3.7
2024-09-23 15:06:58,005:INFO:         yellowbrick: 1.5
2024-09-23 15:06:58,005:INFO:              plotly: 5.24.1
2024-09-23 15:06:58,005:INFO:    plotly-resampler: Not installed
2024-09-23 15:06:58,005:INFO:             kaleido: 0.2.1
2024-09-23 15:06:58,005:INFO:           schemdraw: 0.15
2024-09-23 15:06:58,005:INFO:         statsmodels: 0.14.2
2024-09-23 15:06:58,005:INFO:              sktime: 0.26.0
2024-09-23 15:06:58,005:INFO:               tbats: 1.1.3
2024-09-23 15:06:58,005:INFO:            pmdarima: 2.0.4
2024-09-23 15:06:58,005:INFO:              psutil: 5.9.0
2024-09-23 15:06:58,005:INFO:          markupsafe: 2.1.3
2024-09-23 15:06:58,005:INFO:             pickle5: Not installed
2024-09-23 15:06:58,005:INFO:         cloudpickle: 3.0.0
2024-09-23 15:06:58,005:INFO:         deprecation: 2.1.0
2024-09-23 15:06:58,005:INFO:              xxhash: 2.0.2
2024-09-23 15:06:58,005:INFO:           wurlitzer: 3.1.1
2024-09-23 15:06:58,005:INFO:PyCaret optional dependencies:
2024-09-23 15:06:58,005:INFO:                shap: Not installed
2024-09-23 15:06:58,005:INFO:           interpret: Not installed
2024-09-23 15:06:58,005:INFO:                umap: Not installed
2024-09-23 15:06:58,005:INFO:     ydata_profiling: Not installed
2024-09-23 15:06:58,005:INFO:  explainerdashboard: Not installed
2024-09-23 15:06:58,005:INFO:             autoviz: Not installed
2024-09-23 15:06:58,005:INFO:           fairlearn: Not installed
2024-09-23 15:06:58,005:INFO:          deepchecks: Not installed
2024-09-23 15:06:58,005:INFO:             xgboost: 2.1.1
2024-09-23 15:06:58,005:INFO:            catboost: 1.2.7
2024-09-23 15:06:58,005:INFO:              kmodes: Not installed
2024-09-23 15:06:58,005:INFO:             mlxtend: Not installed
2024-09-23 15:06:58,005:INFO:       statsforecast: Not installed
2024-09-23 15:06:58,005:INFO:        tune_sklearn: Not installed
2024-09-23 15:06:58,005:INFO:                 ray: Not installed
2024-09-23 15:06:58,005:INFO:            hyperopt: Not installed
2024-09-23 15:06:58,005:INFO:              optuna: Not installed
2024-09-23 15:06:58,005:INFO:               skopt: Not installed
2024-09-23 15:06:58,005:INFO:              mlflow: Not installed
2024-09-23 15:06:58,005:INFO:              gradio: Not installed
2024-09-23 15:06:58,005:INFO:             fastapi: Not installed
2024-09-23 15:06:58,005:INFO:             uvicorn: Not installed
2024-09-23 15:06:58,005:INFO:              m2cgen: Not installed
2024-09-23 15:06:58,005:INFO:           evidently: Not installed
2024-09-23 15:06:58,005:INFO:               fugue: Not installed
2024-09-23 15:06:58,005:INFO:           streamlit: Not installed
2024-09-23 15:06:58,005:INFO:             prophet: Not installed
2024-09-23 15:06:58,005:INFO:None
2024-09-23 15:06:58,005:INFO:Set up data.
2024-09-23 15:06:58,021:INFO:Set up folding strategy.
2024-09-23 15:06:58,021:INFO:Set up train/test split.
2024-09-23 15:06:58,037:INFO:Set up index.
2024-09-23 15:06:58,037:INFO:Assigning column types.
2024-09-23 15:06:58,059:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:06:58,090:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:06:58,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:06:58,104:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,105:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,126:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:06:58,127:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:06:58,140:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,141:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,141:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:06:58,154:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:06:58,169:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,169:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,185:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:06:58,220:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,221:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,221:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:06:58,254:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,254:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,297:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,298:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,299:INFO:Preparing preprocessing pipeline...
2024-09-23 15:06:58,301:INFO:Set up simple imputation.
2024-09-23 15:06:58,301:INFO:Set up feature normalization.
2024-09-23 15:06:58,302:INFO:Set up column name cleaning.
2024-09-23 15:06:58,351:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:06:58,355:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?'...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:06:58,355:INFO:Creating final display dataframe.
2024-09-23 15:06:58,468:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 25)
4        Transformed data shape       (30380, 25)
5   Transformed train set shape       (21266, 25)
6    Transformed test set shape        (9114, 25)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              089f
2024-09-23 15:06:58,520:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,520:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,564:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:06:58,565:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:06:58,566:INFO:setup() successfully completed in 0.57s...............
2024-09-23 15:07:01,826:INFO:Initializing compare_models()
2024-09-23 15:07:01,837:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:07:01,837:INFO:Checking exceptions
2024-09-23 15:07:01,856:INFO:Preparing display monitor
2024-09-23 15:07:01,874:INFO:Initializing Logistic Regression
2024-09-23 15:07:01,874:INFO:Total runtime is 0.0 minutes
2024-09-23 15:07:01,874:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:01,874:INFO:Initializing create_model()
2024-09-23 15:07:01,874:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:01,874:INFO:Checking exceptions
2024-09-23 15:07:01,874:INFO:Importing libraries
2024-09-23 15:07:01,874:INFO:Copying training dataset
2024-09-23 15:07:01,913:INFO:Defining folds
2024-09-23 15:07:01,914:INFO:Declaring metric variables
2024-09-23 15:07:01,918:INFO:Importing untrained model
2024-09-23 15:07:01,923:INFO:Logistic Regression Imported successfully
2024-09-23 15:07:01,931:INFO:Starting cross validation
2024-09-23 15:07:01,932:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:04,966:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:04,966:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:04,982:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:04,982:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,045:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,045:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,060:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,060:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,076:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,076:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,091:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,091:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,123:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,123:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,138:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,138:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,154:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:05,170:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:05,170:INFO:Calculating mean and std
2024-09-23 15:07:05,170:INFO:Creating metrics dataframe
2024-09-23 15:07:05,170:INFO:Uploading results into container
2024-09-23 15:07:05,170:INFO:Uploading model into container now
2024-09-23 15:07:05,170:INFO:_master_model_container: 1
2024-09-23 15:07:05,170:INFO:_display_container: 2
2024-09-23 15:07:05,170:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:07:05,170:INFO:create_model() successfully completed......................................
2024-09-23 15:07:05,268:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:05,268:INFO:Creating metrics dataframe
2024-09-23 15:07:05,268:INFO:Initializing K Neighbors Classifier
2024-09-23 15:07:05,268:INFO:Total runtime is 0.056564851601918535 minutes
2024-09-23 15:07:05,268:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:05,268:INFO:Initializing create_model()
2024-09-23 15:07:05,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:05,268:INFO:Checking exceptions
2024-09-23 15:07:05,268:INFO:Importing libraries
2024-09-23 15:07:05,268:INFO:Copying training dataset
2024-09-23 15:07:05,290:INFO:Defining folds
2024-09-23 15:07:05,290:INFO:Declaring metric variables
2024-09-23 15:07:05,290:INFO:Importing untrained model
2024-09-23 15:07:05,290:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:07:05,290:INFO:Starting cross validation
2024-09-23 15:07:05,303:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:07,388:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:07,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:07,466:INFO:Calculating mean and std
2024-09-23 15:07:07,466:INFO:Creating metrics dataframe
2024-09-23 15:07:07,466:INFO:Uploading results into container
2024-09-23 15:07:07,466:INFO:Uploading model into container now
2024-09-23 15:07:07,466:INFO:_master_model_container: 2
2024-09-23 15:07:07,466:INFO:_display_container: 2
2024-09-23 15:07:07,466:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:07:07,466:INFO:create_model() successfully completed......................................
2024-09-23 15:07:07,583:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:07,583:INFO:Creating metrics dataframe
2024-09-23 15:07:07,599:INFO:Initializing Naive Bayes
2024-09-23 15:07:07,599:INFO:Total runtime is 0.09542259375254313 minutes
2024-09-23 15:07:07,599:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:07,607:INFO:Initializing create_model()
2024-09-23 15:07:07,607:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:07,607:INFO:Checking exceptions
2024-09-23 15:07:07,607:INFO:Importing libraries
2024-09-23 15:07:07,607:INFO:Copying training dataset
2024-09-23 15:07:07,612:INFO:Defining folds
2024-09-23 15:07:07,612:INFO:Declaring metric variables
2024-09-23 15:07:07,622:INFO:Importing untrained model
2024-09-23 15:07:07,624:INFO:Naive Bayes Imported successfully
2024-09-23 15:07:07,624:INFO:Starting cross validation
2024-09-23 15:07:07,624:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:09,022:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:09,044:INFO:Calculating mean and std
2024-09-23 15:07:09,044:INFO:Creating metrics dataframe
2024-09-23 15:07:09,046:INFO:Uploading results into container
2024-09-23 15:07:09,046:INFO:Uploading model into container now
2024-09-23 15:07:09,046:INFO:_master_model_container: 3
2024-09-23 15:07:09,046:INFO:_display_container: 2
2024-09-23 15:07:09,047:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:07:09,047:INFO:create_model() successfully completed......................................
2024-09-23 15:07:09,137:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:09,137:INFO:Creating metrics dataframe
2024-09-23 15:07:09,141:INFO:Initializing Decision Tree Classifier
2024-09-23 15:07:09,141:INFO:Total runtime is 0.12111946741739908 minutes
2024-09-23 15:07:09,143:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:09,143:INFO:Initializing create_model()
2024-09-23 15:07:09,143:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:09,143:INFO:Checking exceptions
2024-09-23 15:07:09,143:INFO:Importing libraries
2024-09-23 15:07:09,143:INFO:Copying training dataset
2024-09-23 15:07:09,170:INFO:Defining folds
2024-09-23 15:07:09,170:INFO:Declaring metric variables
2024-09-23 15:07:09,173:INFO:Importing untrained model
2024-09-23 15:07:09,174:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:07:09,174:INFO:Starting cross validation
2024-09-23 15:07:09,174:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:09,506:INFO:Calculating mean and std
2024-09-23 15:07:09,506:INFO:Creating metrics dataframe
2024-09-23 15:07:09,508:INFO:Uploading results into container
2024-09-23 15:07:09,508:INFO:Uploading model into container now
2024-09-23 15:07:09,508:INFO:_master_model_container: 4
2024-09-23 15:07:09,508:INFO:_display_container: 2
2024-09-23 15:07:09,508:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:07:09,509:INFO:create_model() successfully completed......................................
2024-09-23 15:07:09,587:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:09,587:INFO:Creating metrics dataframe
2024-09-23 15:07:09,591:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:07:09,592:INFO:Total runtime is 0.12863100767135618 minutes
2024-09-23 15:07:09,594:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:09,594:INFO:Initializing create_model()
2024-09-23 15:07:09,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:09,594:INFO:Checking exceptions
2024-09-23 15:07:09,594:INFO:Importing libraries
2024-09-23 15:07:09,594:INFO:Copying training dataset
2024-09-23 15:07:09,605:INFO:Defining folds
2024-09-23 15:07:09,605:INFO:Declaring metric variables
2024-09-23 15:07:09,607:INFO:Importing untrained model
2024-09-23 15:07:09,608:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:07:09,611:INFO:Starting cross validation
2024-09-23 15:07:09,611:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:09,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,840:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:09,900:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,904:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:09,914:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,958:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,965:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,967:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,970:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:09,973:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,008:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,008:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,012:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,017:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,023:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,031:INFO:Calculating mean and std
2024-09-23 15:07:10,031:INFO:Creating metrics dataframe
2024-09-23 15:07:10,033:INFO:Uploading results into container
2024-09-23 15:07:10,034:INFO:Uploading model into container now
2024-09-23 15:07:10,034:INFO:_master_model_container: 5
2024-09-23 15:07:10,034:INFO:_display_container: 2
2024-09-23 15:07:10,034:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:07:10,034:INFO:create_model() successfully completed......................................
2024-09-23 15:07:10,113:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:10,114:INFO:Creating metrics dataframe
2024-09-23 15:07:10,117:INFO:Initializing Ridge Classifier
2024-09-23 15:07:10,117:INFO:Total runtime is 0.1373832066853841 minutes
2024-09-23 15:07:10,119:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:10,119:INFO:Initializing create_model()
2024-09-23 15:07:10,119:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:10,119:INFO:Checking exceptions
2024-09-23 15:07:10,119:INFO:Importing libraries
2024-09-23 15:07:10,119:INFO:Copying training dataset
2024-09-23 15:07:10,125:INFO:Defining folds
2024-09-23 15:07:10,125:INFO:Declaring metric variables
2024-09-23 15:07:10,125:INFO:Importing untrained model
2024-09-23 15:07:10,125:INFO:Ridge Classifier Imported successfully
2024-09-23 15:07:10,138:INFO:Starting cross validation
2024-09-23 15:07:10,139:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:10,253:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,259:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,266:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,266:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,271:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,273:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,276:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,283:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,284:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,290:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,292:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,293:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,296:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,310:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,316:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,324:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,326:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:10,329:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,331:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:10,338:INFO:Calculating mean and std
2024-09-23 15:07:10,339:INFO:Creating metrics dataframe
2024-09-23 15:07:10,340:INFO:Uploading results into container
2024-09-23 15:07:10,340:INFO:Uploading model into container now
2024-09-23 15:07:10,340:INFO:_master_model_container: 6
2024-09-23 15:07:10,341:INFO:_display_container: 2
2024-09-23 15:07:10,341:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:07:10,342:INFO:create_model() successfully completed......................................
2024-09-23 15:07:10,404:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:10,404:INFO:Creating metrics dataframe
2024-09-23 15:07:10,425:INFO:Initializing Random Forest Classifier
2024-09-23 15:07:10,425:INFO:Total runtime is 0.14251927534739173 minutes
2024-09-23 15:07:10,425:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:10,425:INFO:Initializing create_model()
2024-09-23 15:07:10,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:10,425:INFO:Checking exceptions
2024-09-23 15:07:10,425:INFO:Importing libraries
2024-09-23 15:07:10,425:INFO:Copying training dataset
2024-09-23 15:07:10,439:INFO:Defining folds
2024-09-23 15:07:10,439:INFO:Declaring metric variables
2024-09-23 15:07:10,439:INFO:Importing untrained model
2024-09-23 15:07:10,439:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:07:10,439:INFO:Starting cross validation
2024-09-23 15:07:10,439:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:12,266:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,314:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,330:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,330:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,330:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,345:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:12,345:INFO:Calculating mean and std
2024-09-23 15:07:12,345:INFO:Creating metrics dataframe
2024-09-23 15:07:12,345:INFO:Uploading results into container
2024-09-23 15:07:12,345:INFO:Uploading model into container now
2024-09-23 15:07:12,361:INFO:_master_model_container: 7
2024-09-23 15:07:12,361:INFO:_display_container: 2
2024-09-23 15:07:12,361:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:07:12,361:INFO:create_model() successfully completed......................................
2024-09-23 15:07:12,487:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:12,487:INFO:Creating metrics dataframe
2024-09-23 15:07:12,487:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:07:12,487:INFO:Total runtime is 0.17688934008280432 minutes
2024-09-23 15:07:12,502:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:12,502:INFO:Initializing create_model()
2024-09-23 15:07:12,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:12,502:INFO:Checking exceptions
2024-09-23 15:07:12,502:INFO:Importing libraries
2024-09-23 15:07:12,502:INFO:Copying training dataset
2024-09-23 15:07:12,511:INFO:Defining folds
2024-09-23 15:07:12,511:INFO:Declaring metric variables
2024-09-23 15:07:12,511:INFO:Importing untrained model
2024-09-23 15:07:12,520:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:07:12,523:INFO:Starting cross validation
2024-09-23 15:07:12,525:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:12,604:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,619:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,619:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,635:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,635:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,635:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,635:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,635:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,651:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,666:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,666:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,666:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,682:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,682:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,682:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,682:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:07:12,698:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,698:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,698:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,698:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:12,713:INFO:Calculating mean and std
2024-09-23 15:07:12,713:INFO:Creating metrics dataframe
2024-09-23 15:07:12,713:INFO:Uploading results into container
2024-09-23 15:07:12,713:INFO:Uploading model into container now
2024-09-23 15:07:12,713:INFO:_master_model_container: 8
2024-09-23 15:07:12,713:INFO:_display_container: 2
2024-09-23 15:07:12,713:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:07:12,713:INFO:create_model() successfully completed......................................
2024-09-23 15:07:12,803:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:12,803:INFO:Creating metrics dataframe
2024-09-23 15:07:12,803:INFO:Initializing Ada Boost Classifier
2024-09-23 15:07:12,803:INFO:Total runtime is 0.18214751084645586 minutes
2024-09-23 15:07:12,813:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:12,813:INFO:Initializing create_model()
2024-09-23 15:07:12,813:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:12,813:INFO:Checking exceptions
2024-09-23 15:07:12,813:INFO:Importing libraries
2024-09-23 15:07:12,813:INFO:Copying training dataset
2024-09-23 15:07:12,823:INFO:Defining folds
2024-09-23 15:07:12,823:INFO:Declaring metric variables
2024-09-23 15:07:12,823:INFO:Importing untrained model
2024-09-23 15:07:12,823:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:07:12,823:INFO:Starting cross validation
2024-09-23 15:07:12,823:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:12,886:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,887:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,903:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,934:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,934:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,965:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,971:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:12,986:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:07:13,768:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:13,784:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:13,800:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:13,902:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:13,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:13,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:14,012:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:14,027:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:14,043:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:14,043:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:14,059:INFO:Calculating mean and std
2024-09-23 15:07:14,059:INFO:Creating metrics dataframe
2024-09-23 15:07:14,059:INFO:Uploading results into container
2024-09-23 15:07:14,059:INFO:Uploading model into container now
2024-09-23 15:07:14,059:INFO:_master_model_container: 9
2024-09-23 15:07:14,059:INFO:_display_container: 2
2024-09-23 15:07:14,059:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:07:14,059:INFO:create_model() successfully completed......................................
2024-09-23 15:07:14,142:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:14,143:INFO:Creating metrics dataframe
2024-09-23 15:07:14,147:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:07:14,147:INFO:Total runtime is 0.20456020832061764 minutes
2024-09-23 15:07:14,149:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:14,150:INFO:Initializing create_model()
2024-09-23 15:07:14,150:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:14,151:INFO:Checking exceptions
2024-09-23 15:07:14,151:INFO:Importing libraries
2024-09-23 15:07:14,151:INFO:Copying training dataset
2024-09-23 15:07:14,156:INFO:Defining folds
2024-09-23 15:07:14,156:INFO:Declaring metric variables
2024-09-23 15:07:14,156:INFO:Importing untrained model
2024-09-23 15:07:14,156:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:07:14,156:INFO:Starting cross validation
2024-09-23 15:07:14,156:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:46,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:47,105:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:47,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:47,480:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:47,699:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:48,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:48,215:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:48,449:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:48,449:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:48,605:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:48,824:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:48,824:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:48,824:INFO:Calculating mean and std
2024-09-23 15:07:48,824:INFO:Creating metrics dataframe
2024-09-23 15:07:48,824:INFO:Uploading results into container
2024-09-23 15:07:48,824:INFO:Uploading model into container now
2024-09-23 15:07:48,824:INFO:_master_model_container: 10
2024-09-23 15:07:48,824:INFO:_display_container: 2
2024-09-23 15:07:48,824:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:07:48,824:INFO:create_model() successfully completed......................................
2024-09-23 15:07:48,918:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:48,918:INFO:Creating metrics dataframe
2024-09-23 15:07:48,926:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:07:48,926:INFO:Total runtime is 0.7841971675554911 minutes
2024-09-23 15:07:48,927:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:48,928:INFO:Initializing create_model()
2024-09-23 15:07:48,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:48,928:INFO:Checking exceptions
2024-09-23 15:07:48,928:INFO:Importing libraries
2024-09-23 15:07:48,928:INFO:Copying training dataset
2024-09-23 15:07:48,939:INFO:Defining folds
2024-09-23 15:07:48,939:INFO:Declaring metric variables
2024-09-23 15:07:48,941:INFO:Importing untrained model
2024-09-23 15:07:48,943:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:07:48,946:INFO:Starting cross validation
2024-09-23 15:07:48,947:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:49,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,051:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:49,051:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,051:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:49,082:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,082:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,082:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:49,097:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:07:49,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:49,129:INFO:Calculating mean and std
2024-09-23 15:07:49,129:INFO:Creating metrics dataframe
2024-09-23 15:07:49,129:INFO:Uploading results into container
2024-09-23 15:07:49,129:INFO:Uploading model into container now
2024-09-23 15:07:49,129:INFO:_master_model_container: 11
2024-09-23 15:07:49,129:INFO:_display_container: 2
2024-09-23 15:07:49,129:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:07:49,129:INFO:create_model() successfully completed......................................
2024-09-23 15:07:49,200:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:49,200:INFO:Creating metrics dataframe
2024-09-23 15:07:49,214:INFO:Initializing Extra Trees Classifier
2024-09-23 15:07:49,214:INFO:Total runtime is 0.7890123208363851 minutes
2024-09-23 15:07:49,216:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:49,217:INFO:Initializing create_model()
2024-09-23 15:07:49,217:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:49,217:INFO:Checking exceptions
2024-09-23 15:07:49,217:INFO:Importing libraries
2024-09-23 15:07:49,218:INFO:Copying training dataset
2024-09-23 15:07:49,235:INFO:Defining folds
2024-09-23 15:07:49,236:INFO:Declaring metric variables
2024-09-23 15:07:49,238:INFO:Importing untrained model
2024-09-23 15:07:49,241:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:07:49,244:INFO:Starting cross validation
2024-09-23 15:07:49,245:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:51,434:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,434:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,434:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,450:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,466:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,466:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,466:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,481:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:51,497:INFO:Calculating mean and std
2024-09-23 15:07:51,497:INFO:Creating metrics dataframe
2024-09-23 15:07:51,497:INFO:Uploading results into container
2024-09-23 15:07:51,497:INFO:Uploading model into container now
2024-09-23 15:07:51,497:INFO:_master_model_container: 12
2024-09-23 15:07:51,497:INFO:_display_container: 2
2024-09-23 15:07:51,497:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:07:51,497:INFO:create_model() successfully completed......................................
2024-09-23 15:07:51,591:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:51,591:INFO:Creating metrics dataframe
2024-09-23 15:07:51,604:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:07:51,604:INFO:Total runtime is 0.8288385272026062 minutes
2024-09-23 15:07:51,605:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:51,606:INFO:Initializing create_model()
2024-09-23 15:07:51,606:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:51,606:INFO:Checking exceptions
2024-09-23 15:07:51,606:INFO:Importing libraries
2024-09-23 15:07:51,607:INFO:Copying training dataset
2024-09-23 15:07:51,619:INFO:Defining folds
2024-09-23 15:07:51,619:INFO:Declaring metric variables
2024-09-23 15:07:51,620:INFO:Importing untrained model
2024-09-23 15:07:51,622:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:07:51,625:INFO:Starting cross validation
2024-09-23 15:07:51,625:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:07:54,335:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,475:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,709:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,788:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,803:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,850:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,902:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:07:54,933:INFO:Calculating mean and std
2024-09-23 15:07:54,933:INFO:Creating metrics dataframe
2024-09-23 15:07:54,933:INFO:Uploading results into container
2024-09-23 15:07:54,933:INFO:Uploading model into container now
2024-09-23 15:07:54,933:INFO:_master_model_container: 13
2024-09-23 15:07:54,933:INFO:_display_container: 2
2024-09-23 15:07:54,933:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:07:54,933:INFO:create_model() successfully completed......................................
2024-09-23 15:07:55,018:INFO:SubProcess create_model() end ==================================
2024-09-23 15:07:55,018:INFO:Creating metrics dataframe
2024-09-23 15:07:55,018:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:07:55,018:INFO:Total runtime is 0.8857384284337362 minutes
2024-09-23 15:07:55,018:INFO:SubProcess create_model() called ==================================
2024-09-23 15:07:55,018:INFO:Initializing create_model()
2024-09-23 15:07:55,018:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:07:55,018:INFO:Checking exceptions
2024-09-23 15:07:55,018:INFO:Importing libraries
2024-09-23 15:07:55,018:INFO:Copying training dataset
2024-09-23 15:07:55,038:INFO:Defining folds
2024-09-23 15:07:55,038:INFO:Declaring metric variables
2024-09-23 15:07:55,040:INFO:Importing untrained model
2024-09-23 15:07:55,042:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:07:55,045:INFO:Starting cross validation
2024-09-23 15:07:55,046:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:08:14,688:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:08:14,996:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:08:15,027:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:08:15,074:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:08:15,169:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:08:15,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:08:15,419:INFO:Calculating mean and std
2024-09-23 15:08:15,419:INFO:Creating metrics dataframe
2024-09-23 15:08:15,419:INFO:Uploading results into container
2024-09-23 15:08:15,419:INFO:Uploading model into container now
2024-09-23 15:08:15,434:INFO:_master_model_container: 14
2024-09-23 15:08:15,434:INFO:_display_container: 2
2024-09-23 15:08:15,434:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:08:15,434:INFO:create_model() successfully completed......................................
2024-09-23 15:08:15,586:INFO:SubProcess create_model() end ==================================
2024-09-23 15:08:15,587:INFO:Creating metrics dataframe
2024-09-23 15:08:15,589:INFO:Initializing CatBoost Classifier
2024-09-23 15:08:15,589:INFO:Total runtime is 1.2285935600598652 minutes
2024-09-23 15:08:15,589:INFO:SubProcess create_model() called ==================================
2024-09-23 15:08:15,589:INFO:Initializing create_model()
2024-09-23 15:08:15,589:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:08:15,589:INFO:Checking exceptions
2024-09-23 15:08:15,589:INFO:Importing libraries
2024-09-23 15:08:15,589:INFO:Copying training dataset
2024-09-23 15:08:15,609:INFO:Defining folds
2024-09-23 15:08:15,609:INFO:Declaring metric variables
2024-09-23 15:08:15,609:INFO:Importing untrained model
2024-09-23 15:08:15,609:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:08:15,609:INFO:Starting cross validation
2024-09-23 15:08:15,609:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:09:23,041:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,106:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,145:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,289:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,289:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,314:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,325:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,354:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,363:INFO:Calculating mean and std
2024-09-23 15:09:23,364:INFO:Creating metrics dataframe
2024-09-23 15:09:23,365:INFO:Uploading results into container
2024-09-23 15:09:23,366:INFO:Uploading model into container now
2024-09-23 15:09:23,366:INFO:_master_model_container: 15
2024-09-23 15:09:23,366:INFO:_display_container: 2
2024-09-23 15:09:23,366:INFO:<catboost.core.CatBoostClassifier object at 0x00000200679B7910>
2024-09-23 15:09:23,367:INFO:create_model() successfully completed......................................
2024-09-23 15:09:23,460:INFO:SubProcess create_model() end ==================================
2024-09-23 15:09:23,461:INFO:Creating metrics dataframe
2024-09-23 15:09:23,467:INFO:Initializing Dummy Classifier
2024-09-23 15:09:23,467:INFO:Total runtime is 2.3598962386449176 minutes
2024-09-23 15:09:23,468:INFO:SubProcess create_model() called ==================================
2024-09-23 15:09:23,469:INFO:Initializing create_model()
2024-09-23 15:09:23,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA27610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:09:23,469:INFO:Checking exceptions
2024-09-23 15:09:23,469:INFO:Importing libraries
2024-09-23 15:09:23,469:INFO:Copying training dataset
2024-09-23 15:09:23,481:INFO:Defining folds
2024-09-23 15:09:23,481:INFO:Declaring metric variables
2024-09-23 15:09:23,483:INFO:Importing untrained model
2024-09-23 15:09:23,484:INFO:Dummy Classifier Imported successfully
2024-09-23 15:09:23,488:INFO:Starting cross validation
2024-09-23 15:09:23,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:09:23,570:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,572:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,603:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,625:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,627:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,637:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,637:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,640:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,670:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:09:23,670:INFO:Calculating mean and std
2024-09-23 15:09:23,670:INFO:Creating metrics dataframe
2024-09-23 15:09:23,670:INFO:Uploading results into container
2024-09-23 15:09:23,670:INFO:Uploading model into container now
2024-09-23 15:09:23,670:INFO:_master_model_container: 16
2024-09-23 15:09:23,670:INFO:_display_container: 2
2024-09-23 15:09:23,670:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:09:23,670:INFO:create_model() successfully completed......................................
2024-09-23 15:09:23,753:INFO:SubProcess create_model() end ==================================
2024-09-23 15:09:23,754:INFO:Creating metrics dataframe
2024-09-23 15:09:23,759:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:09:23,763:INFO:Initializing create_model()
2024-09-23 15:09:23,763:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679B50D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:09:23,765:INFO:Checking exceptions
2024-09-23 15:09:23,765:INFO:Importing libraries
2024-09-23 15:09:23,766:INFO:Copying training dataset
2024-09-23 15:09:23,778:INFO:Defining folds
2024-09-23 15:09:23,778:INFO:Declaring metric variables
2024-09-23 15:09:23,778:INFO:Importing untrained model
2024-09-23 15:09:23,778:INFO:Declaring custom model
2024-09-23 15:09:23,779:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:09:23,779:INFO:Cross validation set to False
2024-09-23 15:09:23,779:INFO:Fitting Model
2024-09-23 15:09:49,121:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:09:49,121:INFO:create_model() successfully completed......................................
2024-09-23 15:09:49,224:INFO:_master_model_container: 16
2024-09-23 15:09:49,224:INFO:_display_container: 2
2024-09-23 15:09:49,224:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:09:49,224:INFO:compare_models() successfully completed......................................
2024-09-23 15:10:21,071:INFO:PyCaret ClassificationExperiment
2024-09-23 15:10:21,071:INFO:Logging name: clf-default-name
2024-09-23 15:10:21,071:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:10:21,071:INFO:version 3.3.1
2024-09-23 15:10:21,071:INFO:Initializing setup()
2024-09-23 15:10:21,071:INFO:self.USI: cd42
2024-09-23 15:10:21,071:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:10:21,071:INFO:Checking environment
2024-09-23 15:10:21,071:INFO:python_version: 3.11.9
2024-09-23 15:10:21,071:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:10:21,071:INFO:machine: AMD64
2024-09-23 15:10:21,071:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:10:21,071:INFO:Memory: svmem(total=16853458944, available=4396961792, percent=73.9, used=12456497152, free=4396961792)
2024-09-23 15:10:21,071:INFO:Physical Core: 16
2024-09-23 15:10:21,071:INFO:Logical Core: 24
2024-09-23 15:10:21,071:INFO:Checking libraries
2024-09-23 15:10:21,071:INFO:System:
2024-09-23 15:10:21,071:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:10:21,071:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:10:21,071:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:10:21,071:INFO:PyCaret required dependencies:
2024-09-23 15:10:21,071:INFO:                 pip: 24.2
2024-09-23 15:10:21,071:INFO:          setuptools: 75.1.0
2024-09-23 15:10:21,071:INFO:             pycaret: 3.3.1
2024-09-23 15:10:21,071:INFO:             IPython: 8.27.0
2024-09-23 15:10:21,071:INFO:          ipywidgets: 8.1.2
2024-09-23 15:10:21,071:INFO:                tqdm: 4.66.5
2024-09-23 15:10:21,071:INFO:               numpy: 1.26.4
2024-09-23 15:10:21,071:INFO:              pandas: 2.1.4
2024-09-23 15:10:21,071:INFO:              jinja2: 3.1.4
2024-09-23 15:10:21,071:INFO:               scipy: 1.11.4
2024-09-23 15:10:21,071:INFO:              joblib: 1.2.0
2024-09-23 15:10:21,071:INFO:             sklearn: 1.4.2
2024-09-23 15:10:21,071:INFO:                pyod: 2.0.2
2024-09-23 15:10:21,071:INFO:            imblearn: 0.12.3
2024-09-23 15:10:21,071:INFO:   category_encoders: 2.6.3
2024-09-23 15:10:21,071:INFO:            lightgbm: 4.5.0
2024-09-23 15:10:21,071:INFO:               numba: 0.60.0
2024-09-23 15:10:21,071:INFO:            requests: 2.32.3
2024-09-23 15:10:21,071:INFO:          matplotlib: 3.9.2
2024-09-23 15:10:21,071:INFO:          scikitplot: 0.3.7
2024-09-23 15:10:21,071:INFO:         yellowbrick: 1.5
2024-09-23 15:10:21,071:INFO:              plotly: 5.24.1
2024-09-23 15:10:21,071:INFO:    plotly-resampler: Not installed
2024-09-23 15:10:21,071:INFO:             kaleido: 0.2.1
2024-09-23 15:10:21,071:INFO:           schemdraw: 0.15
2024-09-23 15:10:21,071:INFO:         statsmodels: 0.14.2
2024-09-23 15:10:21,071:INFO:              sktime: 0.26.0
2024-09-23 15:10:21,071:INFO:               tbats: 1.1.3
2024-09-23 15:10:21,071:INFO:            pmdarima: 2.0.4
2024-09-23 15:10:21,071:INFO:              psutil: 5.9.0
2024-09-23 15:10:21,071:INFO:          markupsafe: 2.1.3
2024-09-23 15:10:21,071:INFO:             pickle5: Not installed
2024-09-23 15:10:21,071:INFO:         cloudpickle: 3.0.0
2024-09-23 15:10:21,071:INFO:         deprecation: 2.1.0
2024-09-23 15:10:21,071:INFO:              xxhash: 2.0.2
2024-09-23 15:10:21,071:INFO:           wurlitzer: 3.1.1
2024-09-23 15:10:21,071:INFO:PyCaret optional dependencies:
2024-09-23 15:10:21,071:INFO:                shap: Not installed
2024-09-23 15:10:21,071:INFO:           interpret: Not installed
2024-09-23 15:10:21,071:INFO:                umap: Not installed
2024-09-23 15:10:21,071:INFO:     ydata_profiling: Not installed
2024-09-23 15:10:21,071:INFO:  explainerdashboard: Not installed
2024-09-23 15:10:21,071:INFO:             autoviz: Not installed
2024-09-23 15:10:21,071:INFO:           fairlearn: Not installed
2024-09-23 15:10:21,071:INFO:          deepchecks: Not installed
2024-09-23 15:10:21,071:INFO:             xgboost: 2.1.1
2024-09-23 15:10:21,071:INFO:            catboost: 1.2.7
2024-09-23 15:10:21,071:INFO:              kmodes: Not installed
2024-09-23 15:10:21,071:INFO:             mlxtend: Not installed
2024-09-23 15:10:21,071:INFO:       statsforecast: Not installed
2024-09-23 15:10:21,071:INFO:        tune_sklearn: Not installed
2024-09-23 15:10:21,071:INFO:                 ray: Not installed
2024-09-23 15:10:21,071:INFO:            hyperopt: Not installed
2024-09-23 15:10:21,071:INFO:              optuna: Not installed
2024-09-23 15:10:21,071:INFO:               skopt: Not installed
2024-09-23 15:10:21,071:INFO:              mlflow: Not installed
2024-09-23 15:10:21,071:INFO:              gradio: Not installed
2024-09-23 15:10:21,071:INFO:             fastapi: Not installed
2024-09-23 15:10:21,071:INFO:             uvicorn: Not installed
2024-09-23 15:10:21,071:INFO:              m2cgen: Not installed
2024-09-23 15:10:21,071:INFO:           evidently: Not installed
2024-09-23 15:10:21,071:INFO:               fugue: Not installed
2024-09-23 15:10:21,071:INFO:           streamlit: Not installed
2024-09-23 15:10:21,071:INFO:             prophet: Not installed
2024-09-23 15:10:21,071:INFO:None
2024-09-23 15:10:21,071:INFO:Set up data.
2024-09-23 15:10:21,097:INFO:Set up folding strategy.
2024-09-23 15:10:21,097:INFO:Set up train/test split.
2024-09-23 15:10:21,106:INFO:Set up index.
2024-09-23 15:10:21,106:INFO:Assigning column types.
2024-09-23 15:10:21,121:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:10:21,139:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:10:21,139:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:10:21,155:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,157:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,179:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:10:21,180:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:10:21,193:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,194:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,195:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:10:21,217:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:10:21,230:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,231:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,254:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:10:21,254:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,254:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,254:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:10:21,301:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,304:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,337:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,337:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,337:INFO:Preparing preprocessing pipeline...
2024-09-23 15:10:21,337:INFO:Set up simple imputation.
2024-09-23 15:10:21,337:INFO:Set up feature normalization.
2024-09-23 15:10:21,337:INFO:Set up column name cleaning.
2024-09-23 15:10:21,400:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:10:21,400:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?'...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:10:21,400:INFO:Creating final display dataframe.
2024-09-23 15:10:21,543:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 21)
4        Transformed data shape       (30380, 21)
5   Transformed train set shape       (21266, 21)
6    Transformed test set shape        (9114, 21)
7              Numeric features                20
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              cd42
2024-09-23 15:10:21,601:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,606:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,642:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:10:21,643:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:10:21,644:INFO:setup() successfully completed in 0.57s...............
2024-09-23 15:10:25,406:INFO:Initializing compare_models()
2024-09-23 15:10:25,406:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:10:25,406:INFO:Checking exceptions
2024-09-23 15:10:25,419:INFO:Preparing display monitor
2024-09-23 15:10:25,437:INFO:Initializing Logistic Regression
2024-09-23 15:10:25,437:INFO:Total runtime is 0.0 minutes
2024-09-23 15:10:25,437:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:25,437:INFO:Initializing create_model()
2024-09-23 15:10:25,437:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:25,437:INFO:Checking exceptions
2024-09-23 15:10:25,437:INFO:Importing libraries
2024-09-23 15:10:25,437:INFO:Copying training dataset
2024-09-23 15:10:25,470:INFO:Defining folds
2024-09-23 15:10:25,470:INFO:Declaring metric variables
2024-09-23 15:10:25,470:INFO:Importing untrained model
2024-09-23 15:10:25,470:INFO:Logistic Regression Imported successfully
2024-09-23 15:10:25,470:INFO:Starting cross validation
2024-09-23 15:10:25,470:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:26,036:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,037:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,040:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,041:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,043:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,045:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,181:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,206:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,220:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,239:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,240:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,256:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,256:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,356:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:26,356:INFO:Calculating mean and std
2024-09-23 15:10:26,356:INFO:Creating metrics dataframe
2024-09-23 15:10:26,356:INFO:Uploading results into container
2024-09-23 15:10:26,356:INFO:Uploading model into container now
2024-09-23 15:10:26,356:INFO:_master_model_container: 1
2024-09-23 15:10:26,356:INFO:_display_container: 2
2024-09-23 15:10:26,356:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:10:26,356:INFO:create_model() successfully completed......................................
2024-09-23 15:10:26,444:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:26,444:INFO:Creating metrics dataframe
2024-09-23 15:10:26,447:INFO:Initializing K Neighbors Classifier
2024-09-23 15:10:26,447:INFO:Total runtime is 0.016836416721343995 minutes
2024-09-23 15:10:26,450:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:26,450:INFO:Initializing create_model()
2024-09-23 15:10:26,450:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:26,450:INFO:Checking exceptions
2024-09-23 15:10:26,450:INFO:Importing libraries
2024-09-23 15:10:26,450:INFO:Copying training dataset
2024-09-23 15:10:26,457:INFO:Defining folds
2024-09-23 15:10:26,457:INFO:Declaring metric variables
2024-09-23 15:10:26,457:INFO:Importing untrained model
2024-09-23 15:10:26,457:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:10:26,457:INFO:Starting cross validation
2024-09-23 15:10:26,457:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:26,896:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:26,987:INFO:Calculating mean and std
2024-09-23 15:10:26,987:INFO:Creating metrics dataframe
2024-09-23 15:10:26,987:INFO:Uploading results into container
2024-09-23 15:10:26,987:INFO:Uploading model into container now
2024-09-23 15:10:26,987:INFO:_master_model_container: 2
2024-09-23 15:10:26,987:INFO:_display_container: 2
2024-09-23 15:10:26,987:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:10:26,987:INFO:create_model() successfully completed......................................
2024-09-23 15:10:27,069:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:27,069:INFO:Creating metrics dataframe
2024-09-23 15:10:27,069:INFO:Initializing Naive Bayes
2024-09-23 15:10:27,069:INFO:Total runtime is 0.027198068300882977 minutes
2024-09-23 15:10:27,081:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:27,082:INFO:Initializing create_model()
2024-09-23 15:10:27,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:27,082:INFO:Checking exceptions
2024-09-23 15:10:27,082:INFO:Importing libraries
2024-09-23 15:10:27,082:INFO:Copying training dataset
2024-09-23 15:10:27,089:INFO:Defining folds
2024-09-23 15:10:27,089:INFO:Declaring metric variables
2024-09-23 15:10:27,089:INFO:Importing untrained model
2024-09-23 15:10:27,089:INFO:Naive Bayes Imported successfully
2024-09-23 15:10:27,089:INFO:Starting cross validation
2024-09-23 15:10:27,089:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:27,217:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:27,256:INFO:Calculating mean and std
2024-09-23 15:10:27,257:INFO:Creating metrics dataframe
2024-09-23 15:10:27,259:INFO:Uploading results into container
2024-09-23 15:10:27,259:INFO:Uploading model into container now
2024-09-23 15:10:27,260:INFO:_master_model_container: 3
2024-09-23 15:10:27,260:INFO:_display_container: 2
2024-09-23 15:10:27,260:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:10:27,260:INFO:create_model() successfully completed......................................
2024-09-23 15:10:27,368:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:27,368:INFO:Creating metrics dataframe
2024-09-23 15:10:27,370:INFO:Initializing Decision Tree Classifier
2024-09-23 15:10:27,370:INFO:Total runtime is 0.03221488396326701 minutes
2024-09-23 15:10:27,370:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:27,370:INFO:Initializing create_model()
2024-09-23 15:10:27,370:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:27,370:INFO:Checking exceptions
2024-09-23 15:10:27,370:INFO:Importing libraries
2024-09-23 15:10:27,370:INFO:Copying training dataset
2024-09-23 15:10:27,370:INFO:Defining folds
2024-09-23 15:10:27,370:INFO:Declaring metric variables
2024-09-23 15:10:27,386:INFO:Importing untrained model
2024-09-23 15:10:27,388:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:10:27,389:INFO:Starting cross validation
2024-09-23 15:10:27,389:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:27,689:INFO:Calculating mean and std
2024-09-23 15:10:27,689:INFO:Creating metrics dataframe
2024-09-23 15:10:27,689:INFO:Uploading results into container
2024-09-23 15:10:27,689:INFO:Uploading model into container now
2024-09-23 15:10:27,689:INFO:_master_model_container: 4
2024-09-23 15:10:27,689:INFO:_display_container: 2
2024-09-23 15:10:27,689:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:10:27,689:INFO:create_model() successfully completed......................................
2024-09-23 15:10:27,787:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:27,787:INFO:Creating metrics dataframe
2024-09-23 15:10:27,787:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:10:27,787:INFO:Total runtime is 0.03915916283925374 minutes
2024-09-23 15:10:27,787:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:27,787:INFO:Initializing create_model()
2024-09-23 15:10:27,787:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:27,787:INFO:Checking exceptions
2024-09-23 15:10:27,787:INFO:Importing libraries
2024-09-23 15:10:27,787:INFO:Copying training dataset
2024-09-23 15:10:27,806:INFO:Defining folds
2024-09-23 15:10:27,806:INFO:Declaring metric variables
2024-09-23 15:10:27,807:INFO:Importing untrained model
2024-09-23 15:10:27,807:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:10:27,807:INFO:Starting cross validation
2024-09-23 15:10:27,807:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:27,996:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,011:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,011:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,011:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,058:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,074:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,074:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,089:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,105:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,121:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,168:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,168:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,168:INFO:Calculating mean and std
2024-09-23 15:10:28,168:INFO:Creating metrics dataframe
2024-09-23 15:10:28,183:INFO:Uploading results into container
2024-09-23 15:10:28,183:INFO:Uploading model into container now
2024-09-23 15:10:28,183:INFO:_master_model_container: 5
2024-09-23 15:10:28,183:INFO:_display_container: 2
2024-09-23 15:10:28,185:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:10:28,185:INFO:create_model() successfully completed......................................
2024-09-23 15:10:28,278:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:28,279:INFO:Creating metrics dataframe
2024-09-23 15:10:28,283:INFO:Initializing Ridge Classifier
2024-09-23 15:10:28,283:INFO:Total runtime is 0.04743732611338297 minutes
2024-09-23 15:10:28,286:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:28,287:INFO:Initializing create_model()
2024-09-23 15:10:28,287:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:28,287:INFO:Checking exceptions
2024-09-23 15:10:28,287:INFO:Importing libraries
2024-09-23 15:10:28,287:INFO:Copying training dataset
2024-09-23 15:10:28,292:INFO:Defining folds
2024-09-23 15:10:28,292:INFO:Declaring metric variables
2024-09-23 15:10:28,292:INFO:Importing untrained model
2024-09-23 15:10:28,292:INFO:Ridge Classifier Imported successfully
2024-09-23 15:10:28,304:INFO:Starting cross validation
2024-09-23 15:10:28,304:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:28,370:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,386:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,386:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,386:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,386:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,408:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,413:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,414:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,417:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,418:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,420:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,441:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:28,445:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:28,452:INFO:Calculating mean and std
2024-09-23 15:10:28,453:INFO:Creating metrics dataframe
2024-09-23 15:10:28,453:INFO:Uploading results into container
2024-09-23 15:10:28,454:INFO:Uploading model into container now
2024-09-23 15:10:28,454:INFO:_master_model_container: 6
2024-09-23 15:10:28,454:INFO:_display_container: 2
2024-09-23 15:10:28,454:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:10:28,455:INFO:create_model() successfully completed......................................
2024-09-23 15:10:28,533:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:28,533:INFO:Creating metrics dataframe
2024-09-23 15:10:28,539:INFO:Initializing Random Forest Classifier
2024-09-23 15:10:28,539:INFO:Total runtime is 0.05169312953948974 minutes
2024-09-23 15:10:28,540:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:28,540:INFO:Initializing create_model()
2024-09-23 15:10:28,540:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:28,540:INFO:Checking exceptions
2024-09-23 15:10:28,540:INFO:Importing libraries
2024-09-23 15:10:28,540:INFO:Copying training dataset
2024-09-23 15:10:28,556:INFO:Defining folds
2024-09-23 15:10:28,556:INFO:Declaring metric variables
2024-09-23 15:10:28,556:INFO:Importing untrained model
2024-09-23 15:10:28,560:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:10:28,564:INFO:Starting cross validation
2024-09-23 15:10:28,564:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:30,301:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,317:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,317:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,317:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,317:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,333:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,349:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,349:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,349:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,364:INFO:Calculating mean and std
2024-09-23 15:10:30,364:INFO:Creating metrics dataframe
2024-09-23 15:10:30,364:INFO:Uploading results into container
2024-09-23 15:10:30,364:INFO:Uploading model into container now
2024-09-23 15:10:30,364:INFO:_master_model_container: 7
2024-09-23 15:10:30,364:INFO:_display_container: 2
2024-09-23 15:10:30,364:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:10:30,364:INFO:create_model() successfully completed......................................
2024-09-23 15:10:30,518:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:30,518:INFO:Creating metrics dataframe
2024-09-23 15:10:30,520:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:10:30,520:INFO:Total runtime is 0.08471508820851643 minutes
2024-09-23 15:10:30,520:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:30,520:INFO:Initializing create_model()
2024-09-23 15:10:30,520:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:30,520:INFO:Checking exceptions
2024-09-23 15:10:30,520:INFO:Importing libraries
2024-09-23 15:10:30,520:INFO:Copying training dataset
2024-09-23 15:10:30,537:INFO:Defining folds
2024-09-23 15:10:30,538:INFO:Declaring metric variables
2024-09-23 15:10:30,539:INFO:Importing untrained model
2024-09-23 15:10:30,539:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:10:30,545:INFO:Starting cross validation
2024-09-23 15:10:30,545:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:30,634:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,684:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,684:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,684:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:30,684:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,715:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,715:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:30,731:INFO:Calculating mean and std
2024-09-23 15:10:30,731:INFO:Creating metrics dataframe
2024-09-23 15:10:30,731:INFO:Uploading results into container
2024-09-23 15:10:30,731:INFO:Uploading model into container now
2024-09-23 15:10:30,731:INFO:_master_model_container: 8
2024-09-23 15:10:30,731:INFO:_display_container: 2
2024-09-23 15:10:30,731:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:10:30,731:INFO:create_model() successfully completed......................................
2024-09-23 15:10:30,804:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:30,804:INFO:Creating metrics dataframe
2024-09-23 15:10:30,820:INFO:Initializing Ada Boost Classifier
2024-09-23 15:10:30,820:INFO:Total runtime is 0.08971126079559326 minutes
2024-09-23 15:10:30,820:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:30,820:INFO:Initializing create_model()
2024-09-23 15:10:30,820:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:30,820:INFO:Checking exceptions
2024-09-23 15:10:30,820:INFO:Importing libraries
2024-09-23 15:10:30,820:INFO:Copying training dataset
2024-09-23 15:10:30,837:INFO:Defining folds
2024-09-23 15:10:30,837:INFO:Declaring metric variables
2024-09-23 15:10:30,837:INFO:Importing untrained model
2024-09-23 15:10:30,837:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:10:30,846:INFO:Starting cross validation
2024-09-23 15:10:30,846:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:30,909:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,914:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,915:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,920:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,959:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,966:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,967:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,970:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:30,970:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:31,017:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:10:31,751:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,798:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,814:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,845:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,860:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,923:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,938:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,970:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,985:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:31,985:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:32,001:INFO:Calculating mean and std
2024-09-23 15:10:32,001:INFO:Creating metrics dataframe
2024-09-23 15:10:32,001:INFO:Uploading results into container
2024-09-23 15:10:32,001:INFO:Uploading model into container now
2024-09-23 15:10:32,001:INFO:_master_model_container: 9
2024-09-23 15:10:32,001:INFO:_display_container: 2
2024-09-23 15:10:32,001:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:10:32,001:INFO:create_model() successfully completed......................................
2024-09-23 15:10:32,102:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:32,102:INFO:Creating metrics dataframe
2024-09-23 15:10:32,109:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:10:32,109:INFO:Total runtime is 0.11119050184885661 minutes
2024-09-23 15:10:32,109:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:32,109:INFO:Initializing create_model()
2024-09-23 15:10:32,109:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:32,109:INFO:Checking exceptions
2024-09-23 15:10:32,109:INFO:Importing libraries
2024-09-23 15:10:32,109:INFO:Copying training dataset
2024-09-23 15:10:32,122:INFO:Defining folds
2024-09-23 15:10:32,122:INFO:Declaring metric variables
2024-09-23 15:10:32,122:INFO:Importing untrained model
2024-09-23 15:10:32,122:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:10:32,122:INFO:Starting cross validation
2024-09-23 15:10:32,122:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:56,907:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:57,547:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:57,657:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:57,735:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:57,829:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:57,954:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:58,297:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:58,469:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:58,985:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,251:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,266:INFO:Calculating mean and std
2024-09-23 15:10:59,266:INFO:Creating metrics dataframe
2024-09-23 15:10:59,266:INFO:Uploading results into container
2024-09-23 15:10:59,266:INFO:Uploading model into container now
2024-09-23 15:10:59,266:INFO:_master_model_container: 10
2024-09-23 15:10:59,266:INFO:_display_container: 2
2024-09-23 15:10:59,266:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:10:59,266:INFO:create_model() successfully completed......................................
2024-09-23 15:10:59,344:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:59,344:INFO:Creating metrics dataframe
2024-09-23 15:10:59,360:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:10:59,360:INFO:Total runtime is 0.5653801719347636 minutes
2024-09-23 15:10:59,360:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:59,360:INFO:Initializing create_model()
2024-09-23 15:10:59,360:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:59,360:INFO:Checking exceptions
2024-09-23 15:10:59,360:INFO:Importing libraries
2024-09-23 15:10:59,360:INFO:Copying training dataset
2024-09-23 15:10:59,372:INFO:Defining folds
2024-09-23 15:10:59,372:INFO:Declaring metric variables
2024-09-23 15:10:59,374:INFO:Importing untrained model
2024-09-23 15:10:59,376:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:10:59,379:INFO:Starting cross validation
2024-09-23 15:10:59,380:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:10:59,487:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,487:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,487:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,487:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:59,487:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,503:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,503:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,503:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,518:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,518:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,518:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:10:59,550:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:10:59,565:INFO:Calculating mean and std
2024-09-23 15:10:59,565:INFO:Creating metrics dataframe
2024-09-23 15:10:59,565:INFO:Uploading results into container
2024-09-23 15:10:59,565:INFO:Uploading model into container now
2024-09-23 15:10:59,565:INFO:_master_model_container: 11
2024-09-23 15:10:59,565:INFO:_display_container: 2
2024-09-23 15:10:59,565:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:10:59,565:INFO:create_model() successfully completed......................................
2024-09-23 15:10:59,628:INFO:SubProcess create_model() end ==================================
2024-09-23 15:10:59,628:INFO:Creating metrics dataframe
2024-09-23 15:10:59,628:INFO:Initializing Extra Trees Classifier
2024-09-23 15:10:59,643:INFO:Total runtime is 0.5701006650924683 minutes
2024-09-23 15:10:59,643:INFO:SubProcess create_model() called ==================================
2024-09-23 15:10:59,643:INFO:Initializing create_model()
2024-09-23 15:10:59,643:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:10:59,643:INFO:Checking exceptions
2024-09-23 15:10:59,643:INFO:Importing libraries
2024-09-23 15:10:59,643:INFO:Copying training dataset
2024-09-23 15:10:59,671:INFO:Defining folds
2024-09-23 15:10:59,671:INFO:Declaring metric variables
2024-09-23 15:10:59,672:INFO:Importing untrained model
2024-09-23 15:10:59,675:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:10:59,679:INFO:Starting cross validation
2024-09-23 15:10:59,679:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:11:01,674:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,689:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,689:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,689:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,705:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,705:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,705:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,720:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,752:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,767:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:01,767:INFO:Calculating mean and std
2024-09-23 15:11:01,767:INFO:Creating metrics dataframe
2024-09-23 15:11:01,783:INFO:Uploading results into container
2024-09-23 15:11:01,783:INFO:Uploading model into container now
2024-09-23 15:11:01,783:INFO:_master_model_container: 12
2024-09-23 15:11:01,783:INFO:_display_container: 2
2024-09-23 15:11:01,783:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:11:01,783:INFO:create_model() successfully completed......................................
2024-09-23 15:11:01,861:INFO:SubProcess create_model() end ==================================
2024-09-23 15:11:01,861:INFO:Creating metrics dataframe
2024-09-23 15:11:01,861:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:11:01,861:INFO:Total runtime is 0.6070653001467387 minutes
2024-09-23 15:11:01,861:INFO:SubProcess create_model() called ==================================
2024-09-23 15:11:01,861:INFO:Initializing create_model()
2024-09-23 15:11:01,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:11:01,861:INFO:Checking exceptions
2024-09-23 15:11:01,861:INFO:Importing libraries
2024-09-23 15:11:01,861:INFO:Copying training dataset
2024-09-23 15:11:01,895:INFO:Defining folds
2024-09-23 15:11:01,895:INFO:Declaring metric variables
2024-09-23 15:11:01,897:INFO:Importing untrained model
2024-09-23 15:11:01,898:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:11:01,901:INFO:Starting cross validation
2024-09-23 15:11:01,902:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:11:04,540:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:04,554:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:04,647:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:04,701:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:04,701:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:04,716:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:04,825:INFO:Calculating mean and std
2024-09-23 15:11:04,825:INFO:Creating metrics dataframe
2024-09-23 15:11:04,825:INFO:Uploading results into container
2024-09-23 15:11:04,825:INFO:Uploading model into container now
2024-09-23 15:11:04,825:INFO:_master_model_container: 13
2024-09-23 15:11:04,825:INFO:_display_container: 2
2024-09-23 15:11:04,825:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:11:04,825:INFO:create_model() successfully completed......................................
2024-09-23 15:11:04,919:INFO:SubProcess create_model() end ==================================
2024-09-23 15:11:04,919:INFO:Creating metrics dataframe
2024-09-23 15:11:04,935:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:11:04,935:INFO:Total runtime is 0.6582933028539022 minutes
2024-09-23 15:11:04,935:INFO:SubProcess create_model() called ==================================
2024-09-23 15:11:04,935:INFO:Initializing create_model()
2024-09-23 15:11:04,935:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:11:04,935:INFO:Checking exceptions
2024-09-23 15:11:04,935:INFO:Importing libraries
2024-09-23 15:11:04,946:INFO:Copying training dataset
2024-09-23 15:11:04,955:INFO:Defining folds
2024-09-23 15:11:04,955:INFO:Declaring metric variables
2024-09-23 15:11:04,958:INFO:Importing untrained model
2024-09-23 15:11:04,959:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:11:04,963:INFO:Starting cross validation
2024-09-23 15:11:04,964:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:11:21,938:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:22,266:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:23,891:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:24,094:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:24,094:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:24,125:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:24,234:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:24,391:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:11:24,391:INFO:Calculating mean and std
2024-09-23 15:11:24,391:INFO:Creating metrics dataframe
2024-09-23 15:11:24,391:INFO:Uploading results into container
2024-09-23 15:11:24,391:INFO:Uploading model into container now
2024-09-23 15:11:24,391:INFO:_master_model_container: 14
2024-09-23 15:11:24,391:INFO:_display_container: 2
2024-09-23 15:11:24,406:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:11:24,406:INFO:create_model() successfully completed......................................
2024-09-23 15:11:24,537:INFO:SubProcess create_model() end ==================================
2024-09-23 15:11:24,537:INFO:Creating metrics dataframe
2024-09-23 15:11:24,552:INFO:Initializing CatBoost Classifier
2024-09-23 15:11:24,552:INFO:Total runtime is 0.9852537234624227 minutes
2024-09-23 15:11:24,552:INFO:SubProcess create_model() called ==================================
2024-09-23 15:11:24,552:INFO:Initializing create_model()
2024-09-23 15:11:24,552:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:11:24,552:INFO:Checking exceptions
2024-09-23 15:11:24,552:INFO:Importing libraries
2024-09-23 15:11:24,552:INFO:Copying training dataset
2024-09-23 15:11:24,573:INFO:Defining folds
2024-09-23 15:11:24,573:INFO:Declaring metric variables
2024-09-23 15:11:24,573:INFO:Importing untrained model
2024-09-23 15:11:24,573:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:11:24,580:INFO:Starting cross validation
2024-09-23 15:11:24,580:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:12:13,267:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:13,628:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:13,971:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:14,221:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:14,719:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,020:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,327:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,533:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,548:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,548:INFO:Calculating mean and std
2024-09-23 15:12:15,548:INFO:Creating metrics dataframe
2024-09-23 15:12:15,548:INFO:Uploading results into container
2024-09-23 15:12:15,548:INFO:Uploading model into container now
2024-09-23 15:12:15,548:INFO:_master_model_container: 15
2024-09-23 15:12:15,548:INFO:_display_container: 2
2024-09-23 15:12:15,548:INFO:<catboost.core.CatBoostClassifier object at 0x000002006BD37850>
2024-09-23 15:12:15,548:INFO:create_model() successfully completed......................................
2024-09-23 15:12:15,651:INFO:SubProcess create_model() end ==================================
2024-09-23 15:12:15,651:INFO:Creating metrics dataframe
2024-09-23 15:12:15,667:INFO:Initializing Dummy Classifier
2024-09-23 15:12:15,667:INFO:Total runtime is 1.8371612429618835 minutes
2024-09-23 15:12:15,674:INFO:SubProcess create_model() called ==================================
2024-09-23 15:12:15,674:INFO:Initializing create_model()
2024-09-23 15:12:15,674:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E8FF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:12:15,674:INFO:Checking exceptions
2024-09-23 15:12:15,674:INFO:Importing libraries
2024-09-23 15:12:15,674:INFO:Copying training dataset
2024-09-23 15:12:15,674:INFO:Defining folds
2024-09-23 15:12:15,674:INFO:Declaring metric variables
2024-09-23 15:12:15,687:INFO:Importing untrained model
2024-09-23 15:12:15,690:INFO:Dummy Classifier Imported successfully
2024-09-23 15:12:15,690:INFO:Starting cross validation
2024-09-23 15:12:15,690:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:12:15,753:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,769:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,769:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,798:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,834:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:12:15,834:INFO:Calculating mean and std
2024-09-23 15:12:15,834:INFO:Creating metrics dataframe
2024-09-23 15:12:15,834:INFO:Uploading results into container
2024-09-23 15:12:15,834:INFO:Uploading model into container now
2024-09-23 15:12:15,834:INFO:_master_model_container: 16
2024-09-23 15:12:15,834:INFO:_display_container: 2
2024-09-23 15:12:15,834:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:12:15,834:INFO:create_model() successfully completed......................................
2024-09-23 15:12:15,967:INFO:SubProcess create_model() end ==================================
2024-09-23 15:12:15,967:INFO:Creating metrics dataframe
2024-09-23 15:12:15,967:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:12:15,982:INFO:Initializing create_model()
2024-09-23 15:12:15,982:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000200679CB910>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:12:15,982:INFO:Checking exceptions
2024-09-23 15:12:15,982:INFO:Importing libraries
2024-09-23 15:12:15,982:INFO:Copying training dataset
2024-09-23 15:12:15,988:INFO:Defining folds
2024-09-23 15:12:15,988:INFO:Declaring metric variables
2024-09-23 15:12:15,988:INFO:Importing untrained model
2024-09-23 15:12:15,988:INFO:Declaring custom model
2024-09-23 15:12:15,988:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:12:15,988:INFO:Cross validation set to False
2024-09-23 15:12:15,988:INFO:Fitting Model
2024-09-23 15:12:37,070:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:12:37,070:INFO:create_model() successfully completed......................................
2024-09-23 15:12:37,172:INFO:_master_model_container: 16
2024-09-23 15:12:37,172:INFO:_display_container: 2
2024-09-23 15:12:37,172:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:12:37,172:INFO:compare_models() successfully completed......................................
2024-09-23 15:13:33,733:INFO:PyCaret ClassificationExperiment
2024-09-23 15:13:33,733:INFO:Logging name: clf-default-name
2024-09-23 15:13:33,733:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:13:33,733:INFO:version 3.3.1
2024-09-23 15:13:33,733:INFO:Initializing setup()
2024-09-23 15:13:33,733:INFO:self.USI: 0460
2024-09-23 15:13:33,734:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:13:33,734:INFO:Checking environment
2024-09-23 15:13:33,734:INFO:python_version: 3.11.9
2024-09-23 15:13:33,734:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:13:33,734:INFO:machine: AMD64
2024-09-23 15:13:33,734:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:13:33,734:INFO:Memory: svmem(total=16853458944, available=4154990592, percent=75.3, used=12698468352, free=4154990592)
2024-09-23 15:13:33,735:INFO:Physical Core: 16
2024-09-23 15:13:33,735:INFO:Logical Core: 24
2024-09-23 15:13:33,735:INFO:Checking libraries
2024-09-23 15:13:33,735:INFO:System:
2024-09-23 15:13:33,735:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:13:33,735:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:13:33,735:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:13:33,735:INFO:PyCaret required dependencies:
2024-09-23 15:13:33,736:INFO:                 pip: 24.2
2024-09-23 15:13:33,736:INFO:          setuptools: 75.1.0
2024-09-23 15:13:33,736:INFO:             pycaret: 3.3.1
2024-09-23 15:13:33,736:INFO:             IPython: 8.27.0
2024-09-23 15:13:33,736:INFO:          ipywidgets: 8.1.2
2024-09-23 15:13:33,736:INFO:                tqdm: 4.66.5
2024-09-23 15:13:33,737:INFO:               numpy: 1.26.4
2024-09-23 15:13:33,737:INFO:              pandas: 2.1.4
2024-09-23 15:13:33,737:INFO:              jinja2: 3.1.4
2024-09-23 15:13:33,737:INFO:               scipy: 1.11.4
2024-09-23 15:13:33,737:INFO:              joblib: 1.2.0
2024-09-23 15:13:33,737:INFO:             sklearn: 1.4.2
2024-09-23 15:13:33,737:INFO:                pyod: 2.0.2
2024-09-23 15:13:33,737:INFO:            imblearn: 0.12.3
2024-09-23 15:13:33,737:INFO:   category_encoders: 2.6.3
2024-09-23 15:13:33,737:INFO:            lightgbm: 4.5.0
2024-09-23 15:13:33,737:INFO:               numba: 0.60.0
2024-09-23 15:13:33,738:INFO:            requests: 2.32.3
2024-09-23 15:13:33,738:INFO:          matplotlib: 3.9.2
2024-09-23 15:13:33,738:INFO:          scikitplot: 0.3.7
2024-09-23 15:13:33,738:INFO:         yellowbrick: 1.5
2024-09-23 15:13:33,738:INFO:              plotly: 5.24.1
2024-09-23 15:13:33,738:INFO:    plotly-resampler: Not installed
2024-09-23 15:13:33,738:INFO:             kaleido: 0.2.1
2024-09-23 15:13:33,738:INFO:           schemdraw: 0.15
2024-09-23 15:13:33,738:INFO:         statsmodels: 0.14.2
2024-09-23 15:13:33,738:INFO:              sktime: 0.26.0
2024-09-23 15:13:33,739:INFO:               tbats: 1.1.3
2024-09-23 15:13:33,739:INFO:            pmdarima: 2.0.4
2024-09-23 15:13:33,739:INFO:              psutil: 5.9.0
2024-09-23 15:13:33,739:INFO:          markupsafe: 2.1.3
2024-09-23 15:13:33,739:INFO:             pickle5: Not installed
2024-09-23 15:13:33,739:INFO:         cloudpickle: 3.0.0
2024-09-23 15:13:33,739:INFO:         deprecation: 2.1.0
2024-09-23 15:13:33,739:INFO:              xxhash: 2.0.2
2024-09-23 15:13:33,739:INFO:           wurlitzer: 3.1.1
2024-09-23 15:13:33,739:INFO:PyCaret optional dependencies:
2024-09-23 15:13:33,739:INFO:                shap: Not installed
2024-09-23 15:13:33,739:INFO:           interpret: Not installed
2024-09-23 15:13:33,739:INFO:                umap: Not installed
2024-09-23 15:13:33,739:INFO:     ydata_profiling: Not installed
2024-09-23 15:13:33,739:INFO:  explainerdashboard: Not installed
2024-09-23 15:13:33,739:INFO:             autoviz: Not installed
2024-09-23 15:13:33,739:INFO:           fairlearn: Not installed
2024-09-23 15:13:33,739:INFO:          deepchecks: Not installed
2024-09-23 15:13:33,739:INFO:             xgboost: 2.1.1
2024-09-23 15:13:33,739:INFO:            catboost: 1.2.7
2024-09-23 15:13:33,739:INFO:              kmodes: Not installed
2024-09-23 15:13:33,739:INFO:             mlxtend: Not installed
2024-09-23 15:13:33,739:INFO:       statsforecast: Not installed
2024-09-23 15:13:33,739:INFO:        tune_sklearn: Not installed
2024-09-23 15:13:33,739:INFO:                 ray: Not installed
2024-09-23 15:13:33,739:INFO:            hyperopt: Not installed
2024-09-23 15:13:33,739:INFO:              optuna: Not installed
2024-09-23 15:13:33,739:INFO:               skopt: Not installed
2024-09-23 15:13:33,739:INFO:              mlflow: Not installed
2024-09-23 15:13:33,739:INFO:              gradio: Not installed
2024-09-23 15:13:33,739:INFO:             fastapi: Not installed
2024-09-23 15:13:33,739:INFO:             uvicorn: Not installed
2024-09-23 15:13:33,739:INFO:              m2cgen: Not installed
2024-09-23 15:13:33,739:INFO:           evidently: Not installed
2024-09-23 15:13:33,739:INFO:               fugue: Not installed
2024-09-23 15:13:33,739:INFO:           streamlit: Not installed
2024-09-23 15:13:33,739:INFO:             prophet: Not installed
2024-09-23 15:13:33,739:INFO:None
2024-09-23 15:13:33,739:INFO:Set up data.
2024-09-23 15:13:33,754:INFO:Set up folding strategy.
2024-09-23 15:13:33,754:INFO:Set up train/test split.
2024-09-23 15:13:33,770:INFO:Set up index.
2024-09-23 15:13:33,770:INFO:Assigning column types.
2024-09-23 15:13:33,786:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:13:33,804:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:13:33,804:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:13:33,825:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:33,826:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:33,852:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:13:33,852:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:13:33,853:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:33,869:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:33,869:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:13:33,885:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:13:33,900:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:33,900:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:33,937:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:13:33,937:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:33,937:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:33,937:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:13:33,990:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:33,992:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:34,020:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:34,020:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:34,020:INFO:Preparing preprocessing pipeline...
2024-09-23 15:13:34,020:INFO:Set up simple imputation.
2024-09-23 15:13:34,020:INFO:Set up feature normalization.
2024-09-23 15:13:34,020:INFO:Set up column name cleaning.
2024-09-23 15:13:34,067:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:13:34,067:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?'...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:13:34,067:INFO:Creating final display dataframe.
2024-09-23 15:13:34,176:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 20)
4        Transformed data shape       (30380, 20)
5   Transformed train set shape       (21266, 20)
6    Transformed test set shape        (9114, 20)
7              Numeric features                19
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              0460
2024-09-23 15:13:34,224:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:34,224:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:34,262:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:13:34,263:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:13:34,264:INFO:setup() successfully completed in 0.54s...............
2024-09-23 15:13:37,563:INFO:Initializing compare_models()
2024-09-23 15:13:37,563:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:13:37,563:INFO:Checking exceptions
2024-09-23 15:13:37,573:INFO:Preparing display monitor
2024-09-23 15:13:37,587:INFO:Initializing Logistic Regression
2024-09-23 15:13:37,587:INFO:Total runtime is 0.0 minutes
2024-09-23 15:13:37,587:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:37,587:INFO:Initializing create_model()
2024-09-23 15:13:37,587:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:37,587:INFO:Checking exceptions
2024-09-23 15:13:37,587:INFO:Importing libraries
2024-09-23 15:13:37,587:INFO:Copying training dataset
2024-09-23 15:13:37,604:INFO:Defining folds
2024-09-23 15:13:37,604:INFO:Declaring metric variables
2024-09-23 15:13:37,620:INFO:Importing untrained model
2024-09-23 15:13:37,620:INFO:Logistic Regression Imported successfully
2024-09-23 15:13:37,620:INFO:Starting cross validation
2024-09-23 15:13:37,620:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:38,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,166:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,166:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,198:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,198:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,229:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,229:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,229:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,342:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,347:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,373:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,390:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:38,390:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:38,390:INFO:Calculating mean and std
2024-09-23 15:13:38,390:INFO:Creating metrics dataframe
2024-09-23 15:13:38,390:INFO:Uploading results into container
2024-09-23 15:13:38,390:INFO:Uploading model into container now
2024-09-23 15:13:38,390:INFO:_master_model_container: 1
2024-09-23 15:13:38,390:INFO:_display_container: 2
2024-09-23 15:13:38,390:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:13:38,390:INFO:create_model() successfully completed......................................
2024-09-23 15:13:38,490:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:38,490:INFO:Creating metrics dataframe
2024-09-23 15:13:38,490:INFO:Initializing K Neighbors Classifier
2024-09-23 15:13:38,490:INFO:Total runtime is 0.01504278580347697 minutes
2024-09-23 15:13:38,490:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:38,490:INFO:Initializing create_model()
2024-09-23 15:13:38,490:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:38,490:INFO:Checking exceptions
2024-09-23 15:13:38,490:INFO:Importing libraries
2024-09-23 15:13:38,490:INFO:Copying training dataset
2024-09-23 15:13:38,507:INFO:Defining folds
2024-09-23 15:13:38,507:INFO:Declaring metric variables
2024-09-23 15:13:38,507:INFO:Importing untrained model
2024-09-23 15:13:38,507:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:13:38,507:INFO:Starting cross validation
2024-09-23 15:13:38,507:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:38,872:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:39,085:INFO:Calculating mean and std
2024-09-23 15:13:39,087:INFO:Creating metrics dataframe
2024-09-23 15:13:39,087:INFO:Uploading results into container
2024-09-23 15:13:39,087:INFO:Uploading model into container now
2024-09-23 15:13:39,087:INFO:_master_model_container: 2
2024-09-23 15:13:39,087:INFO:_display_container: 2
2024-09-23 15:13:39,087:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:13:39,087:INFO:create_model() successfully completed......................................
2024-09-23 15:13:39,154:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:39,154:INFO:Creating metrics dataframe
2024-09-23 15:13:39,170:INFO:Initializing Naive Bayes
2024-09-23 15:13:39,170:INFO:Total runtime is 0.026375508308410643 minutes
2024-09-23 15:13:39,170:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:39,170:INFO:Initializing create_model()
2024-09-23 15:13:39,170:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:39,170:INFO:Checking exceptions
2024-09-23 15:13:39,170:INFO:Importing libraries
2024-09-23 15:13:39,170:INFO:Copying training dataset
2024-09-23 15:13:39,182:INFO:Defining folds
2024-09-23 15:13:39,182:INFO:Declaring metric variables
2024-09-23 15:13:39,182:INFO:Importing untrained model
2024-09-23 15:13:39,188:INFO:Naive Bayes Imported successfully
2024-09-23 15:13:39,189:INFO:Starting cross validation
2024-09-23 15:13:39,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:39,323:INFO:Calculating mean and std
2024-09-23 15:13:39,323:INFO:Creating metrics dataframe
2024-09-23 15:13:39,323:INFO:Uploading results into container
2024-09-23 15:13:39,323:INFO:Uploading model into container now
2024-09-23 15:13:39,323:INFO:_master_model_container: 3
2024-09-23 15:13:39,323:INFO:_display_container: 2
2024-09-23 15:13:39,323:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:13:39,337:INFO:create_model() successfully completed......................................
2024-09-23 15:13:39,439:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:39,439:INFO:Creating metrics dataframe
2024-09-23 15:13:39,439:INFO:Initializing Decision Tree Classifier
2024-09-23 15:13:39,439:INFO:Total runtime is 0.03086857795715332 minutes
2024-09-23 15:13:39,439:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:39,439:INFO:Initializing create_model()
2024-09-23 15:13:39,439:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:39,439:INFO:Checking exceptions
2024-09-23 15:13:39,439:INFO:Importing libraries
2024-09-23 15:13:39,439:INFO:Copying training dataset
2024-09-23 15:13:39,456:INFO:Defining folds
2024-09-23 15:13:39,456:INFO:Declaring metric variables
2024-09-23 15:13:39,456:INFO:Importing untrained model
2024-09-23 15:13:39,456:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:13:39,456:INFO:Starting cross validation
2024-09-23 15:13:39,456:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:39,764:INFO:Calculating mean and std
2024-09-23 15:13:39,764:INFO:Creating metrics dataframe
2024-09-23 15:13:39,764:INFO:Uploading results into container
2024-09-23 15:13:39,764:INFO:Uploading model into container now
2024-09-23 15:13:39,764:INFO:_master_model_container: 4
2024-09-23 15:13:39,764:INFO:_display_container: 2
2024-09-23 15:13:39,764:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:13:39,764:INFO:create_model() successfully completed......................................
2024-09-23 15:13:39,855:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:39,855:INFO:Creating metrics dataframe
2024-09-23 15:13:39,859:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:13:39,859:INFO:Total runtime is 0.03786625464757284 minutes
2024-09-23 15:13:39,859:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:39,859:INFO:Initializing create_model()
2024-09-23 15:13:39,859:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:39,859:INFO:Checking exceptions
2024-09-23 15:13:39,859:INFO:Importing libraries
2024-09-23 15:13:39,859:INFO:Copying training dataset
2024-09-23 15:13:39,872:INFO:Defining folds
2024-09-23 15:13:39,872:INFO:Declaring metric variables
2024-09-23 15:13:39,872:INFO:Importing untrained model
2024-09-23 15:13:39,872:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:13:39,879:INFO:Starting cross validation
2024-09-23 15:13:39,879:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:40,070:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,070:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,070:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,100:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,120:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,136:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,136:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,214:INFO:Calculating mean and std
2024-09-23 15:13:40,214:INFO:Creating metrics dataframe
2024-09-23 15:13:40,214:INFO:Uploading results into container
2024-09-23 15:13:40,214:INFO:Uploading model into container now
2024-09-23 15:13:40,214:INFO:_master_model_container: 5
2024-09-23 15:13:40,214:INFO:_display_container: 2
2024-09-23 15:13:40,214:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:13:40,214:INFO:create_model() successfully completed......................................
2024-09-23 15:13:40,322:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:40,322:INFO:Creating metrics dataframe
2024-09-23 15:13:40,326:INFO:Initializing Ridge Classifier
2024-09-23 15:13:40,326:INFO:Total runtime is 0.04565539757410685 minutes
2024-09-23 15:13:40,329:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:40,329:INFO:Initializing create_model()
2024-09-23 15:13:40,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:40,329:INFO:Checking exceptions
2024-09-23 15:13:40,329:INFO:Importing libraries
2024-09-23 15:13:40,329:INFO:Copying training dataset
2024-09-23 15:13:40,338:INFO:Defining folds
2024-09-23 15:13:40,338:INFO:Declaring metric variables
2024-09-23 15:13:40,339:INFO:Importing untrained model
2024-09-23 15:13:40,339:INFO:Ridge Classifier Imported successfully
2024-09-23 15:13:40,343:INFO:Starting cross validation
2024-09-23 15:13:40,343:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:40,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,461:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,464:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,467:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,470:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,470:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,470:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,470:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,470:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,486:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:40,486:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:40,486:INFO:Calculating mean and std
2024-09-23 15:13:40,486:INFO:Creating metrics dataframe
2024-09-23 15:13:40,501:INFO:Uploading results into container
2024-09-23 15:13:40,501:INFO:Uploading model into container now
2024-09-23 15:13:40,501:INFO:_master_model_container: 6
2024-09-23 15:13:40,501:INFO:_display_container: 2
2024-09-23 15:13:40,501:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:13:40,501:INFO:create_model() successfully completed......................................
2024-09-23 15:13:40,579:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:40,579:INFO:Creating metrics dataframe
2024-09-23 15:13:40,591:INFO:Initializing Random Forest Classifier
2024-09-23 15:13:40,591:INFO:Total runtime is 0.05006163120269776 minutes
2024-09-23 15:13:40,591:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:40,591:INFO:Initializing create_model()
2024-09-23 15:13:40,591:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:40,591:INFO:Checking exceptions
2024-09-23 15:13:40,591:INFO:Importing libraries
2024-09-23 15:13:40,591:INFO:Copying training dataset
2024-09-23 15:13:40,603:INFO:Defining folds
2024-09-23 15:13:40,603:INFO:Declaring metric variables
2024-09-23 15:13:40,605:INFO:Importing untrained model
2024-09-23 15:13:40,605:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:13:40,605:INFO:Starting cross validation
2024-09-23 15:13:40,605:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:42,200:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,200:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,200:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,232:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,247:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,357:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,357:INFO:Calculating mean and std
2024-09-23 15:13:42,357:INFO:Creating metrics dataframe
2024-09-23 15:13:42,357:INFO:Uploading results into container
2024-09-23 15:13:42,357:INFO:Uploading model into container now
2024-09-23 15:13:42,357:INFO:_master_model_container: 7
2024-09-23 15:13:42,357:INFO:_display_container: 2
2024-09-23 15:13:42,357:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:13:42,357:INFO:create_model() successfully completed......................................
2024-09-23 15:13:42,435:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:42,435:INFO:Creating metrics dataframe
2024-09-23 15:13:42,451:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:13:42,451:INFO:Total runtime is 0.08106591701507568 minutes
2024-09-23 15:13:42,451:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:42,451:INFO:Initializing create_model()
2024-09-23 15:13:42,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:42,451:INFO:Checking exceptions
2024-09-23 15:13:42,451:INFO:Importing libraries
2024-09-23 15:13:42,451:INFO:Copying training dataset
2024-09-23 15:13:42,461:INFO:Defining folds
2024-09-23 15:13:42,461:INFO:Declaring metric variables
2024-09-23 15:13:42,461:INFO:Importing untrained model
2024-09-23 15:13:42,461:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:13:42,472:INFO:Starting cross validation
2024-09-23 15:13:42,472:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:42,552:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,552:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,589:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,592:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,593:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,596:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,598:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:13:42,604:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,604:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,604:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,619:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:42,619:INFO:Calculating mean and std
2024-09-23 15:13:42,619:INFO:Creating metrics dataframe
2024-09-23 15:13:42,619:INFO:Uploading results into container
2024-09-23 15:13:42,619:INFO:Uploading model into container now
2024-09-23 15:13:42,619:INFO:_master_model_container: 8
2024-09-23 15:13:42,619:INFO:_display_container: 2
2024-09-23 15:13:42,619:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:13:42,619:INFO:create_model() successfully completed......................................
2024-09-23 15:13:42,713:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:42,713:INFO:Creating metrics dataframe
2024-09-23 15:13:42,729:INFO:Initializing Ada Boost Classifier
2024-09-23 15:13:42,729:INFO:Total runtime is 0.08569310506184895 minutes
2024-09-23 15:13:42,729:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:42,729:INFO:Initializing create_model()
2024-09-23 15:13:42,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:42,729:INFO:Checking exceptions
2024-09-23 15:13:42,729:INFO:Importing libraries
2024-09-23 15:13:42,729:INFO:Copying training dataset
2024-09-23 15:13:42,741:INFO:Defining folds
2024-09-23 15:13:42,741:INFO:Declaring metric variables
2024-09-23 15:13:42,741:INFO:Importing untrained model
2024-09-23 15:13:42,741:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:13:42,741:INFO:Starting cross validation
2024-09-23 15:13:42,749:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:13:42,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,804:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,836:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,848:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,851:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,852:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:42,868:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:13:43,544:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,544:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,560:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,560:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,620:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,683:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,698:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,714:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,745:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,793:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:13:43,808:INFO:Calculating mean and std
2024-09-23 15:13:43,808:INFO:Creating metrics dataframe
2024-09-23 15:13:43,808:INFO:Uploading results into container
2024-09-23 15:13:43,808:INFO:Uploading model into container now
2024-09-23 15:13:43,808:INFO:_master_model_container: 9
2024-09-23 15:13:43,808:INFO:_display_container: 2
2024-09-23 15:13:43,808:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:13:43,808:INFO:create_model() successfully completed......................................
2024-09-23 15:13:43,886:INFO:SubProcess create_model() end ==================================
2024-09-23 15:13:43,886:INFO:Creating metrics dataframe
2024-09-23 15:13:43,900:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:13:43,901:INFO:Total runtime is 0.1052247126897176 minutes
2024-09-23 15:13:43,901:INFO:SubProcess create_model() called ==================================
2024-09-23 15:13:43,901:INFO:Initializing create_model()
2024-09-23 15:13:43,901:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:13:43,901:INFO:Checking exceptions
2024-09-23 15:13:43,901:INFO:Importing libraries
2024-09-23 15:13:43,901:INFO:Copying training dataset
2024-09-23 15:13:43,909:INFO:Defining folds
2024-09-23 15:13:43,909:INFO:Declaring metric variables
2024-09-23 15:13:43,909:INFO:Importing untrained model
2024-09-23 15:13:43,909:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:13:43,920:INFO:Starting cross validation
2024-09-23 15:13:43,921:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:14:04,714:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:05,589:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:05,605:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:05,839:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:05,949:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:06,386:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:06,496:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:06,527:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:06,839:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:06,949:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:06,964:INFO:Calculating mean and std
2024-09-23 15:14:06,964:INFO:Creating metrics dataframe
2024-09-23 15:14:06,964:INFO:Uploading results into container
2024-09-23 15:14:06,964:INFO:Uploading model into container now
2024-09-23 15:14:06,964:INFO:_master_model_container: 10
2024-09-23 15:14:06,964:INFO:_display_container: 2
2024-09-23 15:14:06,964:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:14:06,964:INFO:create_model() successfully completed......................................
2024-09-23 15:14:07,042:INFO:SubProcess create_model() end ==================================
2024-09-23 15:14:07,042:INFO:Creating metrics dataframe
2024-09-23 15:14:07,042:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:14:07,042:INFO:Total runtime is 0.49092281659444176 minutes
2024-09-23 15:14:07,042:INFO:SubProcess create_model() called ==================================
2024-09-23 15:14:07,042:INFO:Initializing create_model()
2024-09-23 15:14:07,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:14:07,042:INFO:Checking exceptions
2024-09-23 15:14:07,042:INFO:Importing libraries
2024-09-23 15:14:07,042:INFO:Copying training dataset
2024-09-23 15:14:07,058:INFO:Defining folds
2024-09-23 15:14:07,058:INFO:Declaring metric variables
2024-09-23 15:14:07,058:INFO:Importing untrained model
2024-09-23 15:14:07,058:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:14:07,058:INFO:Starting cross validation
2024-09-23 15:14:07,058:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:14:07,168:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,168:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:07,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:07,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,183:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:07,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:07,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:14:07,230:INFO:Calculating mean and std
2024-09-23 15:14:07,230:INFO:Creating metrics dataframe
2024-09-23 15:14:07,230:INFO:Uploading results into container
2024-09-23 15:14:07,230:INFO:Uploading model into container now
2024-09-23 15:14:07,230:INFO:_master_model_container: 11
2024-09-23 15:14:07,230:INFO:_display_container: 2
2024-09-23 15:14:07,230:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:14:07,230:INFO:create_model() successfully completed......................................
2024-09-23 15:14:07,308:INFO:SubProcess create_model() end ==================================
2024-09-23 15:14:07,308:INFO:Creating metrics dataframe
2024-09-23 15:14:07,308:INFO:Initializing Extra Trees Classifier
2024-09-23 15:14:07,308:INFO:Total runtime is 0.4953508019447327 minutes
2024-09-23 15:14:07,308:INFO:SubProcess create_model() called ==================================
2024-09-23 15:14:07,308:INFO:Initializing create_model()
2024-09-23 15:14:07,308:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:14:07,308:INFO:Checking exceptions
2024-09-23 15:14:07,308:INFO:Importing libraries
2024-09-23 15:14:07,308:INFO:Copying training dataset
2024-09-23 15:14:07,324:INFO:Defining folds
2024-09-23 15:14:07,324:INFO:Declaring metric variables
2024-09-23 15:14:07,324:INFO:Importing untrained model
2024-09-23 15:14:07,324:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:14:07,339:INFO:Starting cross validation
2024-09-23 15:14:07,339:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:14:09,377:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,393:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,456:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,475:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,534:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,550:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,644:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:09,644:INFO:Calculating mean and std
2024-09-23 15:14:09,644:INFO:Creating metrics dataframe
2024-09-23 15:14:09,644:INFO:Uploading results into container
2024-09-23 15:14:09,644:INFO:Uploading model into container now
2024-09-23 15:14:09,644:INFO:_master_model_container: 12
2024-09-23 15:14:09,644:INFO:_display_container: 2
2024-09-23 15:14:09,644:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:14:09,644:INFO:create_model() successfully completed......................................
2024-09-23 15:14:09,769:INFO:SubProcess create_model() end ==================================
2024-09-23 15:14:09,769:INFO:Creating metrics dataframe
2024-09-23 15:14:09,785:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:14:09,785:INFO:Total runtime is 0.5366278926531474 minutes
2024-09-23 15:14:09,785:INFO:SubProcess create_model() called ==================================
2024-09-23 15:14:09,785:INFO:Initializing create_model()
2024-09-23 15:14:09,785:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:14:09,785:INFO:Checking exceptions
2024-09-23 15:14:09,785:INFO:Importing libraries
2024-09-23 15:14:09,785:INFO:Copying training dataset
2024-09-23 15:14:09,806:INFO:Defining folds
2024-09-23 15:14:09,806:INFO:Declaring metric variables
2024-09-23 15:14:09,810:INFO:Importing untrained model
2024-09-23 15:14:09,810:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:14:09,816:INFO:Starting cross validation
2024-09-23 15:14:09,816:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:14:12,617:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,617:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,695:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,757:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,789:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,882:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:12,898:INFO:Calculating mean and std
2024-09-23 15:14:12,898:INFO:Creating metrics dataframe
2024-09-23 15:14:12,898:INFO:Uploading results into container
2024-09-23 15:14:12,898:INFO:Uploading model into container now
2024-09-23 15:14:12,898:INFO:_master_model_container: 13
2024-09-23 15:14:12,898:INFO:_display_container: 2
2024-09-23 15:14:12,898:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:14:12,898:INFO:create_model() successfully completed......................................
2024-09-23 15:14:12,992:INFO:SubProcess create_model() end ==================================
2024-09-23 15:14:12,992:INFO:Creating metrics dataframe
2024-09-23 15:14:13,007:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:14:13,007:INFO:Total runtime is 0.5903375069300334 minutes
2024-09-23 15:14:13,007:INFO:SubProcess create_model() called ==================================
2024-09-23 15:14:13,007:INFO:Initializing create_model()
2024-09-23 15:14:13,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:14:13,007:INFO:Checking exceptions
2024-09-23 15:14:13,007:INFO:Importing libraries
2024-09-23 15:14:13,007:INFO:Copying training dataset
2024-09-23 15:14:13,023:INFO:Defining folds
2024-09-23 15:14:13,023:INFO:Declaring metric variables
2024-09-23 15:14:13,023:INFO:Importing untrained model
2024-09-23 15:14:13,023:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:14:13,023:INFO:Starting cross validation
2024-09-23 15:14:13,023:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:14:30,238:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:32,072:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:32,182:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:32,307:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:32,322:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:32,557:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:14:32,620:INFO:Calculating mean and std
2024-09-23 15:14:32,635:INFO:Creating metrics dataframe
2024-09-23 15:14:32,635:INFO:Uploading results into container
2024-09-23 15:14:32,635:INFO:Uploading model into container now
2024-09-23 15:14:32,635:INFO:_master_model_container: 14
2024-09-23 15:14:32,635:INFO:_display_container: 2
2024-09-23 15:14:32,635:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:14:32,635:INFO:create_model() successfully completed......................................
2024-09-23 15:14:32,799:INFO:SubProcess create_model() end ==================================
2024-09-23 15:14:32,799:INFO:Creating metrics dataframe
2024-09-23 15:14:32,799:INFO:Initializing CatBoost Classifier
2024-09-23 15:14:32,799:INFO:Total runtime is 0.9201999624570212 minutes
2024-09-23 15:14:32,799:INFO:SubProcess create_model() called ==================================
2024-09-23 15:14:32,799:INFO:Initializing create_model()
2024-09-23 15:14:32,799:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:14:32,799:INFO:Checking exceptions
2024-09-23 15:14:32,799:INFO:Importing libraries
2024-09-23 15:14:32,799:INFO:Copying training dataset
2024-09-23 15:14:32,823:INFO:Defining folds
2024-09-23 15:14:32,823:INFO:Declaring metric variables
2024-09-23 15:14:32,825:INFO:Importing untrained model
2024-09-23 15:14:32,826:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:14:32,828:INFO:Starting cross validation
2024-09-23 15:14:32,828:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:15:21,747:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:21,982:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:23,424:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:23,486:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,020:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,050:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,093:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,475:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,475:INFO:Calculating mean and std
2024-09-23 15:15:24,475:INFO:Creating metrics dataframe
2024-09-23 15:15:24,475:INFO:Uploading results into container
2024-09-23 15:15:24,475:INFO:Uploading model into container now
2024-09-23 15:15:24,475:INFO:_master_model_container: 15
2024-09-23 15:15:24,475:INFO:_display_container: 2
2024-09-23 15:15:24,475:INFO:<catboost.core.CatBoostClassifier object at 0x000002006BD85190>
2024-09-23 15:15:24,475:INFO:create_model() successfully completed......................................
2024-09-23 15:15:24,569:INFO:SubProcess create_model() end ==================================
2024-09-23 15:15:24,569:INFO:Creating metrics dataframe
2024-09-23 15:15:24,569:INFO:Initializing Dummy Classifier
2024-09-23 15:15:24,569:INFO:Total runtime is 1.7830290158589683 minutes
2024-09-23 15:15:24,584:INFO:SubProcess create_model() called ==================================
2024-09-23 15:15:24,584:INFO:Initializing create_model()
2024-09-23 15:15:24,584:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CE34610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:15:24,584:INFO:Checking exceptions
2024-09-23 15:15:24,584:INFO:Importing libraries
2024-09-23 15:15:24,584:INFO:Copying training dataset
2024-09-23 15:15:24,590:INFO:Defining folds
2024-09-23 15:15:24,590:INFO:Declaring metric variables
2024-09-23 15:15:24,590:INFO:Importing untrained model
2024-09-23 15:15:24,590:INFO:Dummy Classifier Imported successfully
2024-09-23 15:15:24,604:INFO:Starting cross validation
2024-09-23 15:15:24,604:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:15:24,671:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,681:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,687:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,702:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,702:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,702:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,702:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,722:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,726:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,728:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:15:24,734:INFO:Calculating mean and std
2024-09-23 15:15:24,734:INFO:Creating metrics dataframe
2024-09-23 15:15:24,735:INFO:Uploading results into container
2024-09-23 15:15:24,736:INFO:Uploading model into container now
2024-09-23 15:15:24,737:INFO:_master_model_container: 16
2024-09-23 15:15:24,737:INFO:_display_container: 2
2024-09-23 15:15:24,737:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:15:24,737:INFO:create_model() successfully completed......................................
2024-09-23 15:15:24,815:INFO:SubProcess create_model() end ==================================
2024-09-23 15:15:24,815:INFO:Creating metrics dataframe
2024-09-23 15:15:24,830:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:15:24,839:INFO:Initializing create_model()
2024-09-23 15:15:24,839:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006DA66590>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:15:24,839:INFO:Checking exceptions
2024-09-23 15:15:24,840:INFO:Importing libraries
2024-09-23 15:15:24,840:INFO:Copying training dataset
2024-09-23 15:15:24,844:INFO:Defining folds
2024-09-23 15:15:24,844:INFO:Declaring metric variables
2024-09-23 15:15:24,844:INFO:Importing untrained model
2024-09-23 15:15:24,844:INFO:Declaring custom model
2024-09-23 15:15:24,853:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:15:24,853:INFO:Cross validation set to False
2024-09-23 15:15:24,853:INFO:Fitting Model
2024-09-23 15:15:44,854:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:15:44,854:INFO:create_model() successfully completed......................................
2024-09-23 15:15:44,973:INFO:_master_model_container: 16
2024-09-23 15:15:44,973:INFO:_display_container: 2
2024-09-23 15:15:44,973:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:15:44,973:INFO:compare_models() successfully completed......................................
2024-09-23 15:16:08,703:INFO:PyCaret ClassificationExperiment
2024-09-23 15:16:08,703:INFO:Logging name: clf-default-name
2024-09-23 15:16:08,703:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:16:08,703:INFO:version 3.3.1
2024-09-23 15:16:08,703:INFO:Initializing setup()
2024-09-23 15:16:08,703:INFO:self.USI: bc77
2024-09-23 15:16:08,703:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:16:08,703:INFO:Checking environment
2024-09-23 15:16:08,703:INFO:python_version: 3.11.9
2024-09-23 15:16:08,703:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:16:08,703:INFO:machine: AMD64
2024-09-23 15:16:08,703:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:16:08,704:INFO:Memory: svmem(total=16853458944, available=4200071168, percent=75.1, used=12653387776, free=4200071168)
2024-09-23 15:16:08,704:INFO:Physical Core: 16
2024-09-23 15:16:08,704:INFO:Logical Core: 24
2024-09-23 15:16:08,704:INFO:Checking libraries
2024-09-23 15:16:08,704:INFO:System:
2024-09-23 15:16:08,704:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:16:08,704:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:16:08,704:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:16:08,704:INFO:PyCaret required dependencies:
2024-09-23 15:16:08,704:INFO:                 pip: 24.2
2024-09-23 15:16:08,704:INFO:          setuptools: 75.1.0
2024-09-23 15:16:08,704:INFO:             pycaret: 3.3.1
2024-09-23 15:16:08,704:INFO:             IPython: 8.27.0
2024-09-23 15:16:08,704:INFO:          ipywidgets: 8.1.2
2024-09-23 15:16:08,704:INFO:                tqdm: 4.66.5
2024-09-23 15:16:08,704:INFO:               numpy: 1.26.4
2024-09-23 15:16:08,704:INFO:              pandas: 2.1.4
2024-09-23 15:16:08,704:INFO:              jinja2: 3.1.4
2024-09-23 15:16:08,704:INFO:               scipy: 1.11.4
2024-09-23 15:16:08,704:INFO:              joblib: 1.2.0
2024-09-23 15:16:08,704:INFO:             sklearn: 1.4.2
2024-09-23 15:16:08,705:INFO:                pyod: 2.0.2
2024-09-23 15:16:08,705:INFO:            imblearn: 0.12.3
2024-09-23 15:16:08,705:INFO:   category_encoders: 2.6.3
2024-09-23 15:16:08,705:INFO:            lightgbm: 4.5.0
2024-09-23 15:16:08,705:INFO:               numba: 0.60.0
2024-09-23 15:16:08,705:INFO:            requests: 2.32.3
2024-09-23 15:16:08,705:INFO:          matplotlib: 3.9.2
2024-09-23 15:16:08,705:INFO:          scikitplot: 0.3.7
2024-09-23 15:16:08,705:INFO:         yellowbrick: 1.5
2024-09-23 15:16:08,705:INFO:              plotly: 5.24.1
2024-09-23 15:16:08,705:INFO:    plotly-resampler: Not installed
2024-09-23 15:16:08,705:INFO:             kaleido: 0.2.1
2024-09-23 15:16:08,705:INFO:           schemdraw: 0.15
2024-09-23 15:16:08,705:INFO:         statsmodels: 0.14.2
2024-09-23 15:16:08,705:INFO:              sktime: 0.26.0
2024-09-23 15:16:08,705:INFO:               tbats: 1.1.3
2024-09-23 15:16:08,705:INFO:            pmdarima: 2.0.4
2024-09-23 15:16:08,705:INFO:              psutil: 5.9.0
2024-09-23 15:16:08,705:INFO:          markupsafe: 2.1.3
2024-09-23 15:16:08,705:INFO:             pickle5: Not installed
2024-09-23 15:16:08,705:INFO:         cloudpickle: 3.0.0
2024-09-23 15:16:08,705:INFO:         deprecation: 2.1.0
2024-09-23 15:16:08,706:INFO:              xxhash: 2.0.2
2024-09-23 15:16:08,706:INFO:           wurlitzer: 3.1.1
2024-09-23 15:16:08,706:INFO:PyCaret optional dependencies:
2024-09-23 15:16:08,706:INFO:                shap: Not installed
2024-09-23 15:16:08,706:INFO:           interpret: Not installed
2024-09-23 15:16:08,706:INFO:                umap: Not installed
2024-09-23 15:16:08,706:INFO:     ydata_profiling: Not installed
2024-09-23 15:16:08,706:INFO:  explainerdashboard: Not installed
2024-09-23 15:16:08,706:INFO:             autoviz: Not installed
2024-09-23 15:16:08,706:INFO:           fairlearn: Not installed
2024-09-23 15:16:08,706:INFO:          deepchecks: Not installed
2024-09-23 15:16:08,706:INFO:             xgboost: 2.1.1
2024-09-23 15:16:08,706:INFO:            catboost: 1.2.7
2024-09-23 15:16:08,706:INFO:              kmodes: Not installed
2024-09-23 15:16:08,706:INFO:             mlxtend: Not installed
2024-09-23 15:16:08,706:INFO:       statsforecast: Not installed
2024-09-23 15:16:08,706:INFO:        tune_sklearn: Not installed
2024-09-23 15:16:08,706:INFO:                 ray: Not installed
2024-09-23 15:16:08,706:INFO:            hyperopt: Not installed
2024-09-23 15:16:08,706:INFO:              optuna: Not installed
2024-09-23 15:16:08,706:INFO:               skopt: Not installed
2024-09-23 15:16:08,706:INFO:              mlflow: Not installed
2024-09-23 15:16:08,706:INFO:              gradio: Not installed
2024-09-23 15:16:08,706:INFO:             fastapi: Not installed
2024-09-23 15:16:08,706:INFO:             uvicorn: Not installed
2024-09-23 15:16:08,706:INFO:              m2cgen: Not installed
2024-09-23 15:16:08,707:INFO:           evidently: Not installed
2024-09-23 15:16:08,707:INFO:               fugue: Not installed
2024-09-23 15:16:08,707:INFO:           streamlit: Not installed
2024-09-23 15:16:08,707:INFO:             prophet: Not installed
2024-09-23 15:16:08,707:INFO:None
2024-09-23 15:16:08,707:INFO:Set up data.
2024-09-23 15:16:08,710:INFO:Set up folding strategy.
2024-09-23 15:16:08,710:INFO:Set up train/test split.
2024-09-23 15:16:08,723:INFO:Set up index.
2024-09-23 15:16:08,737:INFO:Assigning column types.
2024-09-23 15:16:08,747:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:16:08,784:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:16:08,784:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:16:08,797:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:08,798:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:08,818:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:16:08,818:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:16:08,834:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:08,835:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:08,835:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:16:08,852:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:16:08,868:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:08,868:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:08,918:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:16:08,934:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:08,935:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:08,935:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:16:08,954:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:08,954:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:09,001:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:09,001:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:09,001:INFO:Preparing preprocessing pipeline...
2024-09-23 15:16:09,001:INFO:Set up simple imputation.
2024-09-23 15:16:09,001:INFO:Set up feature normalization.
2024-09-23 15:16:09,001:INFO:Set up column name cleaning.
2024-09-23 15:16:09,032:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:16:09,032:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?', 'Ibu Hidup?',
                                             'Jumlah (P...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:16:09,032:INFO:Creating final display dataframe.
2024-09-23 15:16:09,137:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 19)
4        Transformed data shape       (30380, 19)
5   Transformed train set shape       (21266, 19)
6    Transformed test set shape        (9114, 19)
7              Numeric features                18
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              bc77
2024-09-23 15:16:09,189:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:09,191:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:09,232:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:16:09,233:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:16:09,234:INFO:setup() successfully completed in 0.55s...............
2024-09-23 15:16:16,461:INFO:Initializing compare_models()
2024-09-23 15:16:16,462:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:16:16,462:INFO:Checking exceptions
2024-09-23 15:16:16,474:INFO:Preparing display monitor
2024-09-23 15:16:16,487:INFO:Initializing Logistic Regression
2024-09-23 15:16:16,487:INFO:Total runtime is 0.0 minutes
2024-09-23 15:16:16,503:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:16,503:INFO:Initializing create_model()
2024-09-23 15:16:16,503:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:16,503:INFO:Checking exceptions
2024-09-23 15:16:16,503:INFO:Importing libraries
2024-09-23 15:16:16,503:INFO:Copying training dataset
2024-09-23 15:16:16,503:INFO:Defining folds
2024-09-23 15:16:16,503:INFO:Declaring metric variables
2024-09-23 15:16:16,503:INFO:Importing untrained model
2024-09-23 15:16:16,503:INFO:Logistic Regression Imported successfully
2024-09-23 15:16:16,520:INFO:Starting cross validation
2024-09-23 15:16:16,520:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:16,990:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:16,990:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,004:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,020:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,114:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,114:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,161:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,161:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,192:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,192:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,270:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:17,286:INFO:Calculating mean and std
2024-09-23 15:16:17,286:INFO:Creating metrics dataframe
2024-09-23 15:16:17,286:INFO:Uploading results into container
2024-09-23 15:16:17,286:INFO:Uploading model into container now
2024-09-23 15:16:17,286:INFO:_master_model_container: 1
2024-09-23 15:16:17,286:INFO:_display_container: 2
2024-09-23 15:16:17,286:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:16:17,286:INFO:create_model() successfully completed......................................
2024-09-23 15:16:17,364:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:17,364:INFO:Creating metrics dataframe
2024-09-23 15:16:17,364:INFO:Initializing K Neighbors Classifier
2024-09-23 15:16:17,364:INFO:Total runtime is 0.014611319700876872 minutes
2024-09-23 15:16:17,364:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:17,364:INFO:Initializing create_model()
2024-09-23 15:16:17,364:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:17,364:INFO:Checking exceptions
2024-09-23 15:16:17,364:INFO:Importing libraries
2024-09-23 15:16:17,364:INFO:Copying training dataset
2024-09-23 15:16:17,384:INFO:Defining folds
2024-09-23 15:16:17,384:INFO:Declaring metric variables
2024-09-23 15:16:17,387:INFO:Importing untrained model
2024-09-23 15:16:17,389:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:16:17,392:INFO:Starting cross validation
2024-09-23 15:16:17,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:17,733:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,863:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:17,939:INFO:Calculating mean and std
2024-09-23 15:16:17,940:INFO:Creating metrics dataframe
2024-09-23 15:16:17,941:INFO:Uploading results into container
2024-09-23 15:16:17,941:INFO:Uploading model into container now
2024-09-23 15:16:17,941:INFO:_master_model_container: 2
2024-09-23 15:16:17,941:INFO:_display_container: 2
2024-09-23 15:16:17,942:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:16:17,942:INFO:create_model() successfully completed......................................
2024-09-23 15:16:18,005:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:18,005:INFO:Creating metrics dataframe
2024-09-23 15:16:18,005:INFO:Initializing Naive Bayes
2024-09-23 15:16:18,005:INFO:Total runtime is 0.025307651360829672 minutes
2024-09-23 15:16:18,005:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:18,005:INFO:Initializing create_model()
2024-09-23 15:16:18,005:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:18,005:INFO:Checking exceptions
2024-09-23 15:16:18,005:INFO:Importing libraries
2024-09-23 15:16:18,005:INFO:Copying training dataset
2024-09-23 15:16:18,024:INFO:Defining folds
2024-09-23 15:16:18,024:INFO:Declaring metric variables
2024-09-23 15:16:18,024:INFO:Importing untrained model
2024-09-23 15:16:18,024:INFO:Naive Bayes Imported successfully
2024-09-23 15:16:18,024:INFO:Starting cross validation
2024-09-23 15:16:18,037:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:18,156:INFO:Calculating mean and std
2024-09-23 15:16:18,156:INFO:Creating metrics dataframe
2024-09-23 15:16:18,156:INFO:Uploading results into container
2024-09-23 15:16:18,156:INFO:Uploading model into container now
2024-09-23 15:16:18,156:INFO:_master_model_container: 3
2024-09-23 15:16:18,156:INFO:_display_container: 2
2024-09-23 15:16:18,170:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:16:18,170:INFO:create_model() successfully completed......................................
2024-09-23 15:16:18,262:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:18,262:INFO:Creating metrics dataframe
2024-09-23 15:16:18,267:INFO:Initializing Decision Tree Classifier
2024-09-23 15:16:18,267:INFO:Total runtime is 0.029663832982381184 minutes
2024-09-23 15:16:18,270:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:18,270:INFO:Initializing create_model()
2024-09-23 15:16:18,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:18,270:INFO:Checking exceptions
2024-09-23 15:16:18,270:INFO:Importing libraries
2024-09-23 15:16:18,270:INFO:Copying training dataset
2024-09-23 15:16:18,271:INFO:Defining folds
2024-09-23 15:16:18,271:INFO:Declaring metric variables
2024-09-23 15:16:18,284:INFO:Importing untrained model
2024-09-23 15:16:18,287:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:16:18,290:INFO:Starting cross validation
2024-09-23 15:16:18,291:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:18,541:INFO:Calculating mean and std
2024-09-23 15:16:18,553:INFO:Creating metrics dataframe
2024-09-23 15:16:18,554:INFO:Uploading results into container
2024-09-23 15:16:18,554:INFO:Uploading model into container now
2024-09-23 15:16:18,555:INFO:_master_model_container: 4
2024-09-23 15:16:18,555:INFO:_display_container: 2
2024-09-23 15:16:18,556:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:16:18,556:INFO:create_model() successfully completed......................................
2024-09-23 15:16:18,640:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:18,640:INFO:Creating metrics dataframe
2024-09-23 15:16:18,640:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:16:18,640:INFO:Total runtime is 0.03588645458221436 minutes
2024-09-23 15:16:18,654:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:18,654:INFO:Initializing create_model()
2024-09-23 15:16:18,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:18,654:INFO:Checking exceptions
2024-09-23 15:16:18,654:INFO:Importing libraries
2024-09-23 15:16:18,654:INFO:Copying training dataset
2024-09-23 15:16:18,659:INFO:Defining folds
2024-09-23 15:16:18,659:INFO:Declaring metric variables
2024-09-23 15:16:18,659:INFO:Importing untrained model
2024-09-23 15:16:18,659:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:16:18,674:INFO:Starting cross validation
2024-09-23 15:16:18,674:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:18,875:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,881:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,887:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:18,896:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,899:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,900:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:18,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,922:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:18,968:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,968:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,968:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:18,999:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,999:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:18,999:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:18,999:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:18,999:INFO:Calculating mean and std
2024-09-23 15:16:18,999:INFO:Creating metrics dataframe
2024-09-23 15:16:18,999:INFO:Uploading results into container
2024-09-23 15:16:18,999:INFO:Uploading model into container now
2024-09-23 15:16:18,999:INFO:_master_model_container: 5
2024-09-23 15:16:18,999:INFO:_display_container: 2
2024-09-23 15:16:18,999:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:16:18,999:INFO:create_model() successfully completed......................................
2024-09-23 15:16:19,087:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:19,087:INFO:Creating metrics dataframe
2024-09-23 15:16:19,102:INFO:Initializing Ridge Classifier
2024-09-23 15:16:19,102:INFO:Total runtime is 0.04359112977981568 minutes
2024-09-23 15:16:19,102:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:19,102:INFO:Initializing create_model()
2024-09-23 15:16:19,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:19,102:INFO:Checking exceptions
2024-09-23 15:16:19,102:INFO:Importing libraries
2024-09-23 15:16:19,102:INFO:Copying training dataset
2024-09-23 15:16:19,114:INFO:Defining folds
2024-09-23 15:16:19,114:INFO:Declaring metric variables
2024-09-23 15:16:19,114:INFO:Importing untrained model
2024-09-23 15:16:19,114:INFO:Ridge Classifier Imported successfully
2024-09-23 15:16:19,122:INFO:Starting cross validation
2024-09-23 15:16:19,122:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:19,192:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,196:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,198:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,199:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,203:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,204:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,204:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,211:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,216:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,221:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,221:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,224:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,225:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,226:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,228:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,240:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:19,244:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:19,251:INFO:Calculating mean and std
2024-09-23 15:16:19,251:INFO:Creating metrics dataframe
2024-09-23 15:16:19,252:INFO:Uploading results into container
2024-09-23 15:16:19,253:INFO:Uploading model into container now
2024-09-23 15:16:19,254:INFO:_master_model_container: 6
2024-09-23 15:16:19,254:INFO:_display_container: 2
2024-09-23 15:16:19,254:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:16:19,254:INFO:create_model() successfully completed......................................
2024-09-23 15:16:19,336:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:19,336:INFO:Creating metrics dataframe
2024-09-23 15:16:19,340:INFO:Initializing Random Forest Classifier
2024-09-23 15:16:19,340:INFO:Total runtime is 0.04755035638809205 minutes
2024-09-23 15:16:19,340:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:19,340:INFO:Initializing create_model()
2024-09-23 15:16:19,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:19,340:INFO:Checking exceptions
2024-09-23 15:16:19,340:INFO:Importing libraries
2024-09-23 15:16:19,340:INFO:Copying training dataset
2024-09-23 15:16:19,340:INFO:Defining folds
2024-09-23 15:16:19,340:INFO:Declaring metric variables
2024-09-23 15:16:19,340:INFO:Importing untrained model
2024-09-23 15:16:19,353:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:16:19,355:INFO:Starting cross validation
2024-09-23 15:16:19,355:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:21,010:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,013:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,034:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,043:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,044:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,059:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,059:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,059:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,072:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,075:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,090:INFO:Calculating mean and std
2024-09-23 15:16:21,090:INFO:Creating metrics dataframe
2024-09-23 15:16:21,091:INFO:Uploading results into container
2024-09-23 15:16:21,091:INFO:Uploading model into container now
2024-09-23 15:16:21,091:INFO:_master_model_container: 7
2024-09-23 15:16:21,091:INFO:_display_container: 2
2024-09-23 15:16:21,091:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:16:21,091:INFO:create_model() successfully completed......................................
2024-09-23 15:16:21,197:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:21,197:INFO:Creating metrics dataframe
2024-09-23 15:16:21,202:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:16:21,202:INFO:Total runtime is 0.07858473062515259 minutes
2024-09-23 15:16:21,203:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:21,204:INFO:Initializing create_model()
2024-09-23 15:16:21,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:21,204:INFO:Checking exceptions
2024-09-23 15:16:21,204:INFO:Importing libraries
2024-09-23 15:16:21,204:INFO:Copying training dataset
2024-09-23 15:16:21,211:INFO:Defining folds
2024-09-23 15:16:21,211:INFO:Declaring metric variables
2024-09-23 15:16:21,211:INFO:Importing untrained model
2024-09-23 15:16:21,211:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:16:21,221:INFO:Starting cross validation
2024-09-23 15:16:21,221:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:21,288:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,288:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,293:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,294:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,308:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,315:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,316:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,318:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:21,320:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:21,351:INFO:Calculating mean and std
2024-09-23 15:16:21,351:INFO:Creating metrics dataframe
2024-09-23 15:16:21,351:INFO:Uploading results into container
2024-09-23 15:16:21,351:INFO:Uploading model into container now
2024-09-23 15:16:21,351:INFO:_master_model_container: 8
2024-09-23 15:16:21,351:INFO:_display_container: 2
2024-09-23 15:16:21,351:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:16:21,351:INFO:create_model() successfully completed......................................
2024-09-23 15:16:21,429:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:21,429:INFO:Creating metrics dataframe
2024-09-23 15:16:21,443:INFO:Initializing Ada Boost Classifier
2024-09-23 15:16:21,443:INFO:Total runtime is 0.08260515530904135 minutes
2024-09-23 15:16:21,443:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:21,443:INFO:Initializing create_model()
2024-09-23 15:16:21,443:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:21,443:INFO:Checking exceptions
2024-09-23 15:16:21,443:INFO:Importing libraries
2024-09-23 15:16:21,443:INFO:Copying training dataset
2024-09-23 15:16:21,456:INFO:Defining folds
2024-09-23 15:16:21,456:INFO:Declaring metric variables
2024-09-23 15:16:21,456:INFO:Importing untrained model
2024-09-23 15:16:21,456:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:16:21,456:INFO:Starting cross validation
2024-09-23 15:16:21,456:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:21,501:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,505:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,518:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,520:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,521:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,539:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,541:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,541:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,541:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:21,541:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:16:22,155:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,190:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,206:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,223:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,223:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,240:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,256:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,319:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,319:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,319:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:22,335:INFO:Calculating mean and std
2024-09-23 15:16:22,335:INFO:Creating metrics dataframe
2024-09-23 15:16:22,335:INFO:Uploading results into container
2024-09-23 15:16:22,335:INFO:Uploading model into container now
2024-09-23 15:16:22,335:INFO:_master_model_container: 9
2024-09-23 15:16:22,335:INFO:_display_container: 2
2024-09-23 15:16:22,335:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:16:22,335:INFO:create_model() successfully completed......................................
2024-09-23 15:16:22,411:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:22,411:INFO:Creating metrics dataframe
2024-09-23 15:16:22,418:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:16:22,418:INFO:Total runtime is 0.09884473085403443 minutes
2024-09-23 15:16:22,420:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:22,421:INFO:Initializing create_model()
2024-09-23 15:16:22,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:22,421:INFO:Checking exceptions
2024-09-23 15:16:22,421:INFO:Importing libraries
2024-09-23 15:16:22,421:INFO:Copying training dataset
2024-09-23 15:16:22,436:INFO:Defining folds
2024-09-23 15:16:22,436:INFO:Declaring metric variables
2024-09-23 15:16:22,437:INFO:Importing untrained model
2024-09-23 15:16:22,438:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:16:22,440:INFO:Starting cross validation
2024-09-23 15:16:22,440:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:42,167:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:42,906:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:43,074:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:43,076:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:43,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:43,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:43,546:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:43,552:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:43,720:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:43,956:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:44,210:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:44,210:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:44,452:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,141:INFO:Calculating mean and std
2024-09-23 15:16:45,142:INFO:Creating metrics dataframe
2024-09-23 15:16:45,143:INFO:Uploading results into container
2024-09-23 15:16:45,144:INFO:Uploading model into container now
2024-09-23 15:16:45,144:INFO:_master_model_container: 10
2024-09-23 15:16:45,144:INFO:_display_container: 2
2024-09-23 15:16:45,145:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:16:45,145:INFO:create_model() successfully completed......................................
2024-09-23 15:16:45,220:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:45,220:INFO:Creating metrics dataframe
2024-09-23 15:16:45,237:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:16:45,237:INFO:Total runtime is 0.47917251586914067 minutes
2024-09-23 15:16:45,239:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:45,239:INFO:Initializing create_model()
2024-09-23 15:16:45,239:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:45,239:INFO:Checking exceptions
2024-09-23 15:16:45,239:INFO:Importing libraries
2024-09-23 15:16:45,239:INFO:Copying training dataset
2024-09-23 15:16:45,252:INFO:Defining folds
2024-09-23 15:16:45,252:INFO:Declaring metric variables
2024-09-23 15:16:45,254:INFO:Importing untrained model
2024-09-23 15:16:45,256:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:16:45,260:INFO:Starting cross validation
2024-09-23 15:16:45,260:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:45,321:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:45,352:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,356:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,356:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,359:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:45,371:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,382:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:16:45,388:INFO:Calculating mean and std
2024-09-23 15:16:45,388:INFO:Creating metrics dataframe
2024-09-23 15:16:45,388:INFO:Uploading results into container
2024-09-23 15:16:45,388:INFO:Uploading model into container now
2024-09-23 15:16:45,388:INFO:_master_model_container: 11
2024-09-23 15:16:45,388:INFO:_display_container: 2
2024-09-23 15:16:45,388:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:16:45,388:INFO:create_model() successfully completed......................................
2024-09-23 15:16:45,482:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:45,482:INFO:Creating metrics dataframe
2024-09-23 15:16:45,493:INFO:Initializing Extra Trees Classifier
2024-09-23 15:16:45,493:INFO:Total runtime is 0.4834364414215088 minutes
2024-09-23 15:16:45,493:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:45,493:INFO:Initializing create_model()
2024-09-23 15:16:45,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:45,493:INFO:Checking exceptions
2024-09-23 15:16:45,493:INFO:Importing libraries
2024-09-23 15:16:45,493:INFO:Copying training dataset
2024-09-23 15:16:45,507:INFO:Defining folds
2024-09-23 15:16:45,507:INFO:Declaring metric variables
2024-09-23 15:16:45,507:INFO:Importing untrained model
2024-09-23 15:16:45,507:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:16:45,507:INFO:Starting cross validation
2024-09-23 15:16:45,507:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:47,349:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,412:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,412:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,444:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,444:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,444:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,459:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,475:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:47,475:INFO:Calculating mean and std
2024-09-23 15:16:47,475:INFO:Creating metrics dataframe
2024-09-23 15:16:47,491:INFO:Uploading results into container
2024-09-23 15:16:47,492:INFO:Uploading model into container now
2024-09-23 15:16:47,493:INFO:_master_model_container: 12
2024-09-23 15:16:47,493:INFO:_display_container: 2
2024-09-23 15:16:47,493:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:16:47,493:INFO:create_model() successfully completed......................................
2024-09-23 15:16:47,612:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:47,613:INFO:Creating metrics dataframe
2024-09-23 15:16:47,619:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:16:47,619:INFO:Total runtime is 0.5188620090484619 minutes
2024-09-23 15:16:47,620:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:47,620:INFO:Initializing create_model()
2024-09-23 15:16:47,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:47,620:INFO:Checking exceptions
2024-09-23 15:16:47,620:INFO:Importing libraries
2024-09-23 15:16:47,620:INFO:Copying training dataset
2024-09-23 15:16:47,620:INFO:Defining folds
2024-09-23 15:16:47,620:INFO:Declaring metric variables
2024-09-23 15:16:47,620:INFO:Importing untrained model
2024-09-23 15:16:47,620:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:16:47,637:INFO:Starting cross validation
2024-09-23 15:16:47,637:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:16:49,629:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:49,848:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:49,895:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:49,895:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:50,020:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:50,051:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:50,098:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:50,317:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:16:50,332:INFO:Calculating mean and std
2024-09-23 15:16:50,332:INFO:Creating metrics dataframe
2024-09-23 15:16:50,332:INFO:Uploading results into container
2024-09-23 15:16:50,332:INFO:Uploading model into container now
2024-09-23 15:16:50,332:INFO:_master_model_container: 13
2024-09-23 15:16:50,332:INFO:_display_container: 2
2024-09-23 15:16:50,332:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:16:50,332:INFO:create_model() successfully completed......................................
2024-09-23 15:16:50,419:INFO:SubProcess create_model() end ==================================
2024-09-23 15:16:50,419:INFO:Creating metrics dataframe
2024-09-23 15:16:50,419:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:16:50,419:INFO:Total runtime is 0.5655342300732931 minutes
2024-09-23 15:16:50,434:INFO:SubProcess create_model() called ==================================
2024-09-23 15:16:50,434:INFO:Initializing create_model()
2024-09-23 15:16:50,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:16:50,434:INFO:Checking exceptions
2024-09-23 15:16:50,434:INFO:Importing libraries
2024-09-23 15:16:50,434:INFO:Copying training dataset
2024-09-23 15:16:50,442:INFO:Defining folds
2024-09-23 15:16:50,442:INFO:Declaring metric variables
2024-09-23 15:16:50,442:INFO:Importing untrained model
2024-09-23 15:16:50,442:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:16:50,456:INFO:Starting cross validation
2024-09-23 15:16:50,457:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:17:07,144:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:07,162:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:07,669:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:07,704:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:07,909:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:07,945:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:08,693:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:08,724:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:08,724:INFO:Calculating mean and std
2024-09-23 15:17:08,724:INFO:Creating metrics dataframe
2024-09-23 15:17:08,724:INFO:Uploading results into container
2024-09-23 15:17:08,724:INFO:Uploading model into container now
2024-09-23 15:17:08,724:INFO:_master_model_container: 14
2024-09-23 15:17:08,724:INFO:_display_container: 2
2024-09-23 15:17:08,724:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:17:08,724:INFO:create_model() successfully completed......................................
2024-09-23 15:17:08,870:INFO:SubProcess create_model() end ==================================
2024-09-23 15:17:08,870:INFO:Creating metrics dataframe
2024-09-23 15:17:08,870:INFO:Initializing CatBoost Classifier
2024-09-23 15:17:08,870:INFO:Total runtime is 0.8730518897374471 minutes
2024-09-23 15:17:08,870:INFO:SubProcess create_model() called ==================================
2024-09-23 15:17:08,870:INFO:Initializing create_model()
2024-09-23 15:17:08,870:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:17:08,870:INFO:Checking exceptions
2024-09-23 15:17:08,870:INFO:Importing libraries
2024-09-23 15:17:08,870:INFO:Copying training dataset
2024-09-23 15:17:08,891:INFO:Defining folds
2024-09-23 15:17:08,891:INFO:Declaring metric variables
2024-09-23 15:17:08,891:INFO:Importing untrained model
2024-09-23 15:17:08,891:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:17:08,891:INFO:Starting cross validation
2024-09-23 15:17:08,891:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:17:56,650:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:56,744:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,088:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,214:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,229:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,901:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:58,917:INFO:Calculating mean and std
2024-09-23 15:17:58,917:INFO:Creating metrics dataframe
2024-09-23 15:17:58,917:INFO:Uploading results into container
2024-09-23 15:17:58,917:INFO:Uploading model into container now
2024-09-23 15:17:58,917:INFO:_master_model_container: 15
2024-09-23 15:17:58,917:INFO:_display_container: 2
2024-09-23 15:17:58,917:INFO:<catboost.core.CatBoostClassifier object at 0x000002006BD7CB90>
2024-09-23 15:17:58,917:INFO:create_model() successfully completed......................................
2024-09-23 15:17:58,995:INFO:SubProcess create_model() end ==================================
2024-09-23 15:17:58,995:INFO:Creating metrics dataframe
2024-09-23 15:17:59,011:INFO:Initializing Dummy Classifier
2024-09-23 15:17:59,011:INFO:Total runtime is 1.708728007475535 minutes
2024-09-23 15:17:59,011:INFO:SubProcess create_model() called ==================================
2024-09-23 15:17:59,011:INFO:Initializing create_model()
2024-09-23 15:17:59,011:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006CEF3E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:17:59,011:INFO:Checking exceptions
2024-09-23 15:17:59,011:INFO:Importing libraries
2024-09-23 15:17:59,011:INFO:Copying training dataset
2024-09-23 15:17:59,028:INFO:Defining folds
2024-09-23 15:17:59,028:INFO:Declaring metric variables
2024-09-23 15:17:59,030:INFO:Importing untrained model
2024-09-23 15:17:59,032:INFO:Dummy Classifier Imported successfully
2024-09-23 15:17:59,035:INFO:Starting cross validation
2024-09-23 15:17:59,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:17:59,087:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,087:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,102:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,102:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,102:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,102:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,118:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,118:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,118:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,134:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:17:59,140:INFO:Calculating mean and std
2024-09-23 15:17:59,141:INFO:Creating metrics dataframe
2024-09-23 15:17:59,142:INFO:Uploading results into container
2024-09-23 15:17:59,143:INFO:Uploading model into container now
2024-09-23 15:17:59,144:INFO:_master_model_container: 16
2024-09-23 15:17:59,144:INFO:_display_container: 2
2024-09-23 15:17:59,144:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:17:59,144:INFO:create_model() successfully completed......................................
2024-09-23 15:17:59,305:INFO:SubProcess create_model() end ==================================
2024-09-23 15:17:59,305:INFO:Creating metrics dataframe
2024-09-23 15:17:59,316:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:17:59,323:INFO:Initializing create_model()
2024-09-23 15:17:59,323:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020067A2AD10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:17:59,323:INFO:Checking exceptions
2024-09-23 15:17:59,325:INFO:Importing libraries
2024-09-23 15:17:59,325:INFO:Copying training dataset
2024-09-23 15:17:59,342:INFO:Defining folds
2024-09-23 15:17:59,342:INFO:Declaring metric variables
2024-09-23 15:17:59,343:INFO:Importing untrained model
2024-09-23 15:17:59,343:INFO:Declaring custom model
2024-09-23 15:17:59,343:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:17:59,344:INFO:Cross validation set to False
2024-09-23 15:17:59,344:INFO:Fitting Model
2024-09-23 15:18:17,520:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:18:17,520:INFO:create_model() successfully completed......................................
2024-09-23 15:18:17,620:INFO:_master_model_container: 16
2024-09-23 15:18:17,620:INFO:_display_container: 2
2024-09-23 15:18:17,620:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:18:17,620:INFO:compare_models() successfully completed......................................
2024-09-23 15:24:40,442:WARNING:C:\Users\ICT\AppData\Local\Temp\ipykernel_20412\3227501785.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_FE2["Golongan UKT"] = df_FE["Golongan UKT"]

2024-09-23 15:24:44,857:WARNING:C:\Users\ICT\AppData\Local\Temp\ipykernel_20412\2394529213.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_FE2["Golongan UKT"] = df_FE["Golongan UKT"]

2024-09-23 15:24:57,339:WARNING:C:\Users\ICT\AppData\Local\Temp\ipykernel_20412\3419833154.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df_FE2["Golongan UKT"] = df_FE["Golongan UKT"]

2024-09-23 15:26:54,738:INFO:PyCaret ClassificationExperiment
2024-09-23 15:26:54,738:INFO:Logging name: clf-default-name
2024-09-23 15:26:54,739:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:26:54,739:INFO:version 3.3.1
2024-09-23 15:26:54,739:INFO:Initializing setup()
2024-09-23 15:26:54,739:INFO:self.USI: ca48
2024-09-23 15:26:54,739:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:26:54,740:INFO:Checking environment
2024-09-23 15:26:54,740:INFO:python_version: 3.11.9
2024-09-23 15:26:54,740:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:26:54,740:INFO:machine: AMD64
2024-09-23 15:26:54,740:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:26:54,740:INFO:Memory: svmem(total=16853458944, available=7816896512, percent=53.6, used=9036562432, free=7816896512)
2024-09-23 15:26:54,740:INFO:Physical Core: 16
2024-09-23 15:26:54,741:INFO:Logical Core: 24
2024-09-23 15:26:54,741:INFO:Checking libraries
2024-09-23 15:26:54,741:INFO:System:
2024-09-23 15:26:54,741:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:26:54,741:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:26:54,741:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:26:54,741:INFO:PyCaret required dependencies:
2024-09-23 15:26:54,741:INFO:                 pip: 24.2
2024-09-23 15:26:54,741:INFO:          setuptools: 75.1.0
2024-09-23 15:26:54,741:INFO:             pycaret: 3.3.1
2024-09-23 15:26:54,742:INFO:             IPython: 8.27.0
2024-09-23 15:26:54,742:INFO:          ipywidgets: 8.1.2
2024-09-23 15:26:54,742:INFO:                tqdm: 4.66.5
2024-09-23 15:26:54,742:INFO:               numpy: 1.26.4
2024-09-23 15:26:54,742:INFO:              pandas: 2.1.4
2024-09-23 15:26:54,742:INFO:              jinja2: 3.1.4
2024-09-23 15:26:54,742:INFO:               scipy: 1.11.4
2024-09-23 15:26:54,742:INFO:              joblib: 1.2.0
2024-09-23 15:26:54,742:INFO:             sklearn: 1.4.2
2024-09-23 15:26:54,742:INFO:                pyod: 2.0.2
2024-09-23 15:26:54,742:INFO:            imblearn: 0.12.3
2024-09-23 15:26:54,742:INFO:   category_encoders: 2.6.3
2024-09-23 15:26:54,742:INFO:            lightgbm: 4.5.0
2024-09-23 15:26:54,743:INFO:               numba: 0.60.0
2024-09-23 15:26:54,743:INFO:            requests: 2.32.3
2024-09-23 15:26:54,743:INFO:          matplotlib: 3.9.2
2024-09-23 15:26:54,743:INFO:          scikitplot: 0.3.7
2024-09-23 15:26:54,743:INFO:         yellowbrick: 1.5
2024-09-23 15:26:54,743:INFO:              plotly: 5.24.1
2024-09-23 15:26:54,743:INFO:    plotly-resampler: Not installed
2024-09-23 15:26:54,743:INFO:             kaleido: 0.2.1
2024-09-23 15:26:54,743:INFO:           schemdraw: 0.15
2024-09-23 15:26:54,743:INFO:         statsmodels: 0.14.2
2024-09-23 15:26:54,743:INFO:              sktime: 0.26.0
2024-09-23 15:26:54,743:INFO:               tbats: 1.1.3
2024-09-23 15:26:54,743:INFO:            pmdarima: 2.0.4
2024-09-23 15:26:54,743:INFO:              psutil: 5.9.0
2024-09-23 15:26:54,744:INFO:          markupsafe: 2.1.3
2024-09-23 15:26:54,744:INFO:             pickle5: Not installed
2024-09-23 15:26:54,744:INFO:         cloudpickle: 3.0.0
2024-09-23 15:26:54,744:INFO:         deprecation: 2.1.0
2024-09-23 15:26:54,744:INFO:              xxhash: 2.0.2
2024-09-23 15:26:54,744:INFO:           wurlitzer: 3.1.1
2024-09-23 15:26:54,744:INFO:PyCaret optional dependencies:
2024-09-23 15:26:54,744:INFO:                shap: Not installed
2024-09-23 15:26:54,744:INFO:           interpret: Not installed
2024-09-23 15:26:54,744:INFO:                umap: Not installed
2024-09-23 15:26:54,744:INFO:     ydata_profiling: Not installed
2024-09-23 15:26:54,744:INFO:  explainerdashboard: Not installed
2024-09-23 15:26:54,744:INFO:             autoviz: Not installed
2024-09-23 15:26:54,744:INFO:           fairlearn: Not installed
2024-09-23 15:26:54,744:INFO:          deepchecks: Not installed
2024-09-23 15:26:54,744:INFO:             xgboost: 2.1.1
2024-09-23 15:26:54,744:INFO:            catboost: 1.2.7
2024-09-23 15:26:54,744:INFO:              kmodes: Not installed
2024-09-23 15:26:54,744:INFO:             mlxtend: Not installed
2024-09-23 15:26:54,744:INFO:       statsforecast: Not installed
2024-09-23 15:26:54,744:INFO:        tune_sklearn: Not installed
2024-09-23 15:26:54,744:INFO:                 ray: Not installed
2024-09-23 15:26:54,744:INFO:            hyperopt: Not installed
2024-09-23 15:26:54,744:INFO:              optuna: Not installed
2024-09-23 15:26:54,744:INFO:               skopt: Not installed
2024-09-23 15:26:54,744:INFO:              mlflow: Not installed
2024-09-23 15:26:54,744:INFO:              gradio: Not installed
2024-09-23 15:26:54,744:INFO:             fastapi: Not installed
2024-09-23 15:26:54,744:INFO:             uvicorn: Not installed
2024-09-23 15:26:54,744:INFO:              m2cgen: Not installed
2024-09-23 15:26:54,744:INFO:           evidently: Not installed
2024-09-23 15:26:54,744:INFO:               fugue: Not installed
2024-09-23 15:26:54,744:INFO:           streamlit: Not installed
2024-09-23 15:26:54,744:INFO:             prophet: Not installed
2024-09-23 15:26:54,744:INFO:None
2024-09-23 15:26:54,744:INFO:Set up data.
2024-09-23 15:26:54,744:INFO:Set up folding strategy.
2024-09-23 15:26:54,744:INFO:Set up train/test split.
2024-09-23 15:26:54,757:INFO:Set up index.
2024-09-23 15:26:54,757:INFO:Assigning column types.
2024-09-23 15:26:54,770:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:26:54,802:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:26:54,803:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:26:54,820:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:54,820:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:54,848:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:26:54,849:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:26:54,870:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:54,872:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:54,872:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:26:54,894:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:26:54,903:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:54,903:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:54,936:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:26:54,949:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:54,953:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:54,953:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:26:54,988:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:54,991:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:55,019:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:55,019:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:55,019:INFO:Preparing preprocessing pipeline...
2024-09-23 15:26:55,019:INFO:Set up simple imputation.
2024-09-23 15:26:55,019:INFO:Set up feature normalization.
2024-09-23 15:26:55,019:INFO:Set up column name cleaning.
2024-09-23 15:26:55,052:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:26:55,052:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pendapatan Keluarga Encode'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:26:55,052:INFO:Creating final display dataframe.
2024-09-23 15:26:55,089:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape        (30380, 2)
4        Transformed data shape        (30380, 2)
5   Transformed train set shape        (21266, 2)
6    Transformed test set shape         (9114, 2)
7              Numeric features                 1
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              ca48
2024-09-23 15:26:55,136:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:55,136:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:55,186:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:26:55,187:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:26:55,188:INFO:setup() successfully completed in 0.46s...............
2024-09-23 15:26:57,563:INFO:Initializing compare_models()
2024-09-23 15:26:57,563:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:26:57,563:INFO:Checking exceptions
2024-09-23 15:26:57,571:INFO:Preparing display monitor
2024-09-23 15:26:57,603:INFO:Initializing Logistic Regression
2024-09-23 15:26:57,603:INFO:Total runtime is 0.0 minutes
2024-09-23 15:26:57,603:INFO:SubProcess create_model() called ==================================
2024-09-23 15:26:57,603:INFO:Initializing create_model()
2024-09-23 15:26:57,603:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:26:57,603:INFO:Checking exceptions
2024-09-23 15:26:57,603:INFO:Importing libraries
2024-09-23 15:26:57,603:INFO:Copying training dataset
2024-09-23 15:26:57,603:INFO:Defining folds
2024-09-23 15:26:57,603:INFO:Declaring metric variables
2024-09-23 15:26:57,603:INFO:Importing untrained model
2024-09-23 15:26:57,620:INFO:Logistic Regression Imported successfully
2024-09-23 15:26:57,620:INFO:Starting cross validation
2024-09-23 15:26:57,620:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:00,164:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,170:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,219:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,219:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,234:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,250:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,250:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,250:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,281:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,281:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,281:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,281:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,312:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:00,312:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:00,312:INFO:Calculating mean and std
2024-09-23 15:27:00,328:INFO:Creating metrics dataframe
2024-09-23 15:27:00,328:INFO:Uploading results into container
2024-09-23 15:27:00,328:INFO:Uploading model into container now
2024-09-23 15:27:00,328:INFO:_master_model_container: 1
2024-09-23 15:27:00,328:INFO:_display_container: 2
2024-09-23 15:27:00,328:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:27:00,328:INFO:create_model() successfully completed......................................
2024-09-23 15:27:00,473:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:00,473:INFO:Creating metrics dataframe
2024-09-23 15:27:00,473:INFO:Initializing K Neighbors Classifier
2024-09-23 15:27:00,473:INFO:Total runtime is 0.04782873789469401 minutes
2024-09-23 15:27:00,473:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:00,473:INFO:Initializing create_model()
2024-09-23 15:27:00,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:00,473:INFO:Checking exceptions
2024-09-23 15:27:00,473:INFO:Importing libraries
2024-09-23 15:27:00,473:INFO:Copying training dataset
2024-09-23 15:27:00,489:INFO:Defining folds
2024-09-23 15:27:00,489:INFO:Declaring metric variables
2024-09-23 15:27:00,491:INFO:Importing untrained model
2024-09-23 15:27:00,493:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:27:00,495:INFO:Starting cross validation
2024-09-23 15:27:00,495:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:02,411:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,422:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,713:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,745:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,745:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,745:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,745:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,745:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,760:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,823:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:02,823:INFO:Calculating mean and std
2024-09-23 15:27:02,823:INFO:Creating metrics dataframe
2024-09-23 15:27:02,823:INFO:Uploading results into container
2024-09-23 15:27:02,823:INFO:Uploading model into container now
2024-09-23 15:27:02,823:INFO:_master_model_container: 2
2024-09-23 15:27:02,823:INFO:_display_container: 2
2024-09-23 15:27:02,823:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:27:02,823:INFO:create_model() successfully completed......................................
2024-09-23 15:27:02,935:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:02,935:INFO:Creating metrics dataframe
2024-09-23 15:27:02,953:INFO:Initializing Naive Bayes
2024-09-23 15:27:02,953:INFO:Total runtime is 0.08915992577870688 minutes
2024-09-23 15:27:02,954:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:02,954:INFO:Initializing create_model()
2024-09-23 15:27:02,954:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:02,954:INFO:Checking exceptions
2024-09-23 15:27:02,954:INFO:Importing libraries
2024-09-23 15:27:02,954:INFO:Copying training dataset
2024-09-23 15:27:02,959:INFO:Defining folds
2024-09-23 15:27:02,959:INFO:Declaring metric variables
2024-09-23 15:27:02,959:INFO:Importing untrained model
2024-09-23 15:27:02,959:INFO:Naive Bayes Imported successfully
2024-09-23 15:27:02,959:INFO:Starting cross validation
2024-09-23 15:27:02,959:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:03,006:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:03,009:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:03,011:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:03,012:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:03,012:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:03,014:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,347:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,347:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,355:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,363:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,367:INFO:Calculating mean and std
2024-09-23 15:27:04,368:INFO:Creating metrics dataframe
2024-09-23 15:27:04,371:INFO:Uploading results into container
2024-09-23 15:27:04,371:INFO:Uploading model into container now
2024-09-23 15:27:04,371:INFO:_master_model_container: 3
2024-09-23 15:27:04,371:INFO:_display_container: 2
2024-09-23 15:27:04,371:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:27:04,371:INFO:create_model() successfully completed......................................
2024-09-23 15:27:04,497:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:04,497:INFO:Creating metrics dataframe
2024-09-23 15:27:04,501:INFO:Initializing Decision Tree Classifier
2024-09-23 15:27:04,501:INFO:Total runtime is 0.1149609923362732 minutes
2024-09-23 15:27:04,504:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:04,505:INFO:Initializing create_model()
2024-09-23 15:27:04,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:04,505:INFO:Checking exceptions
2024-09-23 15:27:04,505:INFO:Importing libraries
2024-09-23 15:27:04,505:INFO:Copying training dataset
2024-09-23 15:27:04,509:INFO:Defining folds
2024-09-23 15:27:04,509:INFO:Declaring metric variables
2024-09-23 15:27:04,511:INFO:Importing untrained model
2024-09-23 15:27:04,513:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:27:04,516:INFO:Starting cross validation
2024-09-23 15:27:04,516:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:04,545:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,546:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,547:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,547:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,548:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,549:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,549:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,550:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,560:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,561:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,565:INFO:Calculating mean and std
2024-09-23 15:27:04,566:INFO:Creating metrics dataframe
2024-09-23 15:27:04,570:INFO:Uploading results into container
2024-09-23 15:27:04,570:INFO:Uploading model into container now
2024-09-23 15:27:04,570:INFO:_master_model_container: 4
2024-09-23 15:27:04,570:INFO:_display_container: 2
2024-09-23 15:27:04,570:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:27:04,570:INFO:create_model() successfully completed......................................
2024-09-23 15:27:04,709:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:04,709:INFO:Creating metrics dataframe
2024-09-23 15:27:04,714:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:27:04,714:INFO:Total runtime is 0.11851752996444703 minutes
2024-09-23 15:27:04,716:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:04,716:INFO:Initializing create_model()
2024-09-23 15:27:04,717:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:04,717:INFO:Checking exceptions
2024-09-23 15:27:04,717:INFO:Importing libraries
2024-09-23 15:27:04,717:INFO:Copying training dataset
2024-09-23 15:27:04,719:INFO:Defining folds
2024-09-23 15:27:04,719:INFO:Declaring metric variables
2024-09-23 15:27:04,722:INFO:Importing untrained model
2024-09-23 15:27:04,722:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:27:04,722:INFO:Starting cross validation
2024-09-23 15:27:04,722:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,849:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,852:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:04,852:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,854:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:04,858:INFO:Calculating mean and std
2024-09-23 15:27:04,859:INFO:Creating metrics dataframe
2024-09-23 15:27:04,861:INFO:Uploading results into container
2024-09-23 15:27:04,861:INFO:Uploading model into container now
2024-09-23 15:27:04,861:INFO:_master_model_container: 5
2024-09-23 15:27:04,862:INFO:_display_container: 2
2024-09-23 15:27:04,862:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:27:04,862:INFO:create_model() successfully completed......................................
2024-09-23 15:27:04,979:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:04,979:INFO:Creating metrics dataframe
2024-09-23 15:27:04,984:INFO:Initializing Ridge Classifier
2024-09-23 15:27:04,984:INFO:Total runtime is 0.12301987012227378 minutes
2024-09-23 15:27:04,987:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:04,987:INFO:Initializing create_model()
2024-09-23 15:27:04,987:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:04,987:INFO:Checking exceptions
2024-09-23 15:27:04,987:INFO:Importing libraries
2024-09-23 15:27:04,987:INFO:Copying training dataset
2024-09-23 15:27:04,988:INFO:Defining folds
2024-09-23 15:27:04,988:INFO:Declaring metric variables
2024-09-23 15:27:04,988:INFO:Importing untrained model
2024-09-23 15:27:04,988:INFO:Ridge Classifier Imported successfully
2024-09-23 15:27:04,988:INFO:Starting cross validation
2024-09-23 15:27:04,988:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:05,032:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,032:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,032:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,033:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,034:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,034:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,035:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,036:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,037:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,038:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,038:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,038:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,040:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,042:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,043:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,045:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,046:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,050:INFO:Calculating mean and std
2024-09-23 15:27:05,051:INFO:Creating metrics dataframe
2024-09-23 15:27:05,053:INFO:Uploading results into container
2024-09-23 15:27:05,053:INFO:Uploading model into container now
2024-09-23 15:27:05,053:INFO:_master_model_container: 6
2024-09-23 15:27:05,053:INFO:_display_container: 2
2024-09-23 15:27:05,053:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:27:05,053:INFO:create_model() successfully completed......................................
2024-09-23 15:27:05,184:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:05,184:INFO:Creating metrics dataframe
2024-09-23 15:27:05,184:INFO:Initializing Random Forest Classifier
2024-09-23 15:27:05,184:INFO:Total runtime is 0.12634944121042888 minutes
2024-09-23 15:27:05,200:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:05,200:INFO:Initializing create_model()
2024-09-23 15:27:05,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:05,200:INFO:Checking exceptions
2024-09-23 15:27:05,200:INFO:Importing libraries
2024-09-23 15:27:05,200:INFO:Copying training dataset
2024-09-23 15:27:05,206:INFO:Defining folds
2024-09-23 15:27:05,206:INFO:Declaring metric variables
2024-09-23 15:27:05,209:INFO:Importing untrained model
2024-09-23 15:27:05,210:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:27:05,214:INFO:Starting cross validation
2024-09-23 15:27:05,214:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,528:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,528:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,543:INFO:Calculating mean and std
2024-09-23 15:27:05,543:INFO:Creating metrics dataframe
2024-09-23 15:27:05,543:INFO:Uploading results into container
2024-09-23 15:27:05,543:INFO:Uploading model into container now
2024-09-23 15:27:05,543:INFO:_master_model_container: 7
2024-09-23 15:27:05,543:INFO:_display_container: 2
2024-09-23 15:27:05,543:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:27:05,543:INFO:create_model() successfully completed......................................
2024-09-23 15:27:05,652:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:05,652:INFO:Creating metrics dataframe
2024-09-23 15:27:05,652:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:27:05,652:INFO:Total runtime is 0.13414613008499146 minutes
2024-09-23 15:27:05,668:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:05,669:INFO:Initializing create_model()
2024-09-23 15:27:05,670:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:05,670:INFO:Checking exceptions
2024-09-23 15:27:05,670:INFO:Importing libraries
2024-09-23 15:27:05,670:INFO:Copying training dataset
2024-09-23 15:27:05,672:INFO:Defining folds
2024-09-23 15:27:05,672:INFO:Declaring metric variables
2024-09-23 15:27:05,674:INFO:Importing untrained model
2024-09-23 15:27:05,677:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:27:05,679:INFO:Starting cross validation
2024-09-23 15:27:05,679:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:05,707:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,707:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,708:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,710:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,711:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,712:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,712:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,713:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,713:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,713:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,715:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,715:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,716:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,716:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,719:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:05,724:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:05,730:INFO:Calculating mean and std
2024-09-23 15:27:05,731:INFO:Creating metrics dataframe
2024-09-23 15:27:05,732:INFO:Uploading results into container
2024-09-23 15:27:05,732:INFO:Uploading model into container now
2024-09-23 15:27:05,732:INFO:_master_model_container: 8
2024-09-23 15:27:05,733:INFO:_display_container: 2
2024-09-23 15:27:05,733:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:27:05,733:INFO:create_model() successfully completed......................................
2024-09-23 15:27:05,883:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:05,883:INFO:Creating metrics dataframe
2024-09-23 15:27:05,883:INFO:Initializing Ada Boost Classifier
2024-09-23 15:27:05,883:INFO:Total runtime is 0.1380066235860189 minutes
2024-09-23 15:27:05,897:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:05,897:INFO:Initializing create_model()
2024-09-23 15:27:05,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:05,897:INFO:Checking exceptions
2024-09-23 15:27:05,897:INFO:Importing libraries
2024-09-23 15:27:05,897:INFO:Copying training dataset
2024-09-23 15:27:05,901:INFO:Defining folds
2024-09-23 15:27:05,902:INFO:Declaring metric variables
2024-09-23 15:27:05,905:INFO:Importing untrained model
2024-09-23 15:27:05,905:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:27:05,910:INFO:Starting cross validation
2024-09-23 15:27:05,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:05,920:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,920:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,920:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,933:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,941:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,941:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,941:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,941:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:05,941:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:27:06,305:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,308:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,308:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,309:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,311:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,312:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,314:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,317:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,325:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,329:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,331:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,338:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,338:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,368:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,373:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,379:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,384:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,400:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:06,403:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:06,404:INFO:Calculating mean and std
2024-09-23 15:27:06,404:INFO:Creating metrics dataframe
2024-09-23 15:27:06,404:INFO:Uploading results into container
2024-09-23 15:27:06,404:INFO:Uploading model into container now
2024-09-23 15:27:06,404:INFO:_master_model_container: 9
2024-09-23 15:27:06,404:INFO:_display_container: 2
2024-09-23 15:27:06,404:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:27:06,404:INFO:create_model() successfully completed......................................
2024-09-23 15:27:06,526:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:06,526:INFO:Creating metrics dataframe
2024-09-23 15:27:06,532:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:27:06,532:INFO:Total runtime is 0.14881285826365154 minutes
2024-09-23 15:27:06,534:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:06,534:INFO:Initializing create_model()
2024-09-23 15:27:06,534:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:06,534:INFO:Checking exceptions
2024-09-23 15:27:06,534:INFO:Importing libraries
2024-09-23 15:27:06,534:INFO:Copying training dataset
2024-09-23 15:27:06,537:INFO:Defining folds
2024-09-23 15:27:06,537:INFO:Declaring metric variables
2024-09-23 15:27:06,539:INFO:Importing untrained model
2024-09-23 15:27:06,541:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:27:06,545:INFO:Starting cross validation
2024-09-23 15:27:06,545:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:08,728:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:08,731:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:08,752:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:08,752:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:08,783:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:08,783:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:08,978:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:08,978:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,010:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,010:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,025:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,025:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,119:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,119:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,119:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,119:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,228:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,228:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,244:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,244:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,244:INFO:Calculating mean and std
2024-09-23 15:27:09,244:INFO:Creating metrics dataframe
2024-09-23 15:27:09,244:INFO:Uploading results into container
2024-09-23 15:27:09,244:INFO:Uploading model into container now
2024-09-23 15:27:09,244:INFO:_master_model_container: 10
2024-09-23 15:27:09,244:INFO:_display_container: 2
2024-09-23 15:27:09,244:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:27:09,244:INFO:create_model() successfully completed......................................
2024-09-23 15:27:09,349:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:09,349:INFO:Creating metrics dataframe
2024-09-23 15:27:09,349:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:27:09,349:INFO:Total runtime is 0.19577078024546307 minutes
2024-09-23 15:27:09,349:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:09,349:INFO:Initializing create_model()
2024-09-23 15:27:09,349:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:09,349:INFO:Checking exceptions
2024-09-23 15:27:09,349:INFO:Importing libraries
2024-09-23 15:27:09,349:INFO:Copying training dataset
2024-09-23 15:27:09,366:INFO:Defining folds
2024-09-23 15:27:09,366:INFO:Declaring metric variables
2024-09-23 15:27:09,368:INFO:Importing untrained model
2024-09-23 15:27:09,369:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:27:09,373:INFO:Starting cross validation
2024-09-23 15:27:09,374:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:09,402:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,406:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,406:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,406:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,407:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,408:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,408:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,409:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,410:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,411:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,411:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,413:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,415:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,416:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,416:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,417:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:27:09,418:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,419:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,419:INFO:Calculating mean and std
2024-09-23 15:27:09,419:INFO:Creating metrics dataframe
2024-09-23 15:27:09,419:INFO:Uploading results into container
2024-09-23 15:27:09,419:INFO:Uploading model into container now
2024-09-23 15:27:09,419:INFO:_master_model_container: 11
2024-09-23 15:27:09,419:INFO:_display_container: 2
2024-09-23 15:27:09,419:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:27:09,419:INFO:create_model() successfully completed......................................
2024-09-23 15:27:09,553:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:09,553:INFO:Creating metrics dataframe
2024-09-23 15:27:09,568:INFO:Initializing Extra Trees Classifier
2024-09-23 15:27:09,568:INFO:Total runtime is 0.19942115942637129 minutes
2024-09-23 15:27:09,568:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:09,568:INFO:Initializing create_model()
2024-09-23 15:27:09,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:09,568:INFO:Checking exceptions
2024-09-23 15:27:09,568:INFO:Importing libraries
2024-09-23 15:27:09,568:INFO:Copying training dataset
2024-09-23 15:27:09,578:INFO:Defining folds
2024-09-23 15:27:09,578:INFO:Declaring metric variables
2024-09-23 15:27:09,581:INFO:Importing untrained model
2024-09-23 15:27:09,584:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:27:09,587:INFO:Starting cross validation
2024-09-23 15:27:09,588:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:09,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,819:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,835:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,840:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,841:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,846:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,849:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:09,855:INFO:Calculating mean and std
2024-09-23 15:27:09,856:INFO:Creating metrics dataframe
2024-09-23 15:27:09,857:INFO:Uploading results into container
2024-09-23 15:27:09,857:INFO:Uploading model into container now
2024-09-23 15:27:09,857:INFO:_master_model_container: 12
2024-09-23 15:27:09,858:INFO:_display_container: 2
2024-09-23 15:27:09,858:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:27:09,858:INFO:create_model() successfully completed......................................
2024-09-23 15:27:09,971:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:09,971:INFO:Creating metrics dataframe
2024-09-23 15:27:09,976:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:27:09,976:INFO:Total runtime is 0.20620946486790978 minutes
2024-09-23 15:27:09,979:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:09,980:INFO:Initializing create_model()
2024-09-23 15:27:09,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:09,980:INFO:Checking exceptions
2024-09-23 15:27:09,980:INFO:Importing libraries
2024-09-23 15:27:09,980:INFO:Copying training dataset
2024-09-23 15:27:09,983:INFO:Defining folds
2024-09-23 15:27:09,983:INFO:Declaring metric variables
2024-09-23 15:27:09,985:INFO:Importing untrained model
2024-09-23 15:27:09,987:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:27:09,990:INFO:Starting cross validation
2024-09-23 15:27:09,990:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:10,428:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,443:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,475:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,506:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,586:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,630:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,636:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:10,668:INFO:Calculating mean and std
2024-09-23 15:27:10,668:INFO:Creating metrics dataframe
2024-09-23 15:27:10,668:INFO:Uploading results into container
2024-09-23 15:27:10,668:INFO:Uploading model into container now
2024-09-23 15:27:10,668:INFO:_master_model_container: 13
2024-09-23 15:27:10,668:INFO:_display_container: 2
2024-09-23 15:27:10,668:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:27:10,668:INFO:create_model() successfully completed......................................
2024-09-23 15:27:10,789:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:10,789:INFO:Creating metrics dataframe
2024-09-23 15:27:10,804:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:27:10,804:INFO:Total runtime is 0.22001965443293256 minutes
2024-09-23 15:27:10,806:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:10,806:INFO:Initializing create_model()
2024-09-23 15:27:10,806:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:10,806:INFO:Checking exceptions
2024-09-23 15:27:10,806:INFO:Importing libraries
2024-09-23 15:27:10,806:INFO:Copying training dataset
2024-09-23 15:27:10,808:INFO:Defining folds
2024-09-23 15:27:10,808:INFO:Declaring metric variables
2024-09-23 15:27:10,808:INFO:Importing untrained model
2024-09-23 15:27:10,808:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:27:10,808:INFO:Starting cross validation
2024-09-23 15:27:10,808:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:13,447:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,467:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,490:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,518:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,574:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,598:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,622:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,624:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,629:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,638:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:13,643:INFO:Calculating mean and std
2024-09-23 15:27:13,644:INFO:Creating metrics dataframe
2024-09-23 15:27:13,646:INFO:Uploading results into container
2024-09-23 15:27:13,646:INFO:Uploading model into container now
2024-09-23 15:27:13,647:INFO:_master_model_container: 14
2024-09-23 15:27:13,647:INFO:_display_container: 2
2024-09-23 15:27:13,647:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:27:13,647:INFO:create_model() successfully completed......................................
2024-09-23 15:27:13,802:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:13,803:INFO:Creating metrics dataframe
2024-09-23 15:27:13,808:INFO:Initializing CatBoost Classifier
2024-09-23 15:27:13,808:INFO:Total runtime is 0.2700811584790548 minutes
2024-09-23 15:27:13,810:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:13,810:INFO:Initializing create_model()
2024-09-23 15:27:13,810:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:13,810:INFO:Checking exceptions
2024-09-23 15:27:13,810:INFO:Importing libraries
2024-09-23 15:27:13,810:INFO:Copying training dataset
2024-09-23 15:27:13,813:INFO:Defining folds
2024-09-23 15:27:13,814:INFO:Declaring metric variables
2024-09-23 15:27:13,816:INFO:Importing untrained model
2024-09-23 15:27:13,818:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:27:13,821:INFO:Starting cross validation
2024-09-23 15:27:13,822:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:28,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,668:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,777:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,793:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,824:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,871:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,887:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,902:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,918:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:28,934:INFO:Calculating mean and std
2024-09-23 15:27:28,934:INFO:Creating metrics dataframe
2024-09-23 15:27:28,934:INFO:Uploading results into container
2024-09-23 15:27:28,934:INFO:Uploading model into container now
2024-09-23 15:27:28,934:INFO:_master_model_container: 15
2024-09-23 15:27:28,934:INFO:_display_container: 2
2024-09-23 15:27:28,934:INFO:<catboost.core.CatBoostClassifier object at 0x0000020070B76F90>
2024-09-23 15:27:28,934:INFO:create_model() successfully completed......................................
2024-09-23 15:27:29,043:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:29,043:INFO:Creating metrics dataframe
2024-09-23 15:27:29,056:INFO:Initializing Dummy Classifier
2024-09-23 15:27:29,056:INFO:Total runtime is 0.5242243885993958 minutes
2024-09-23 15:27:29,060:INFO:SubProcess create_model() called ==================================
2024-09-23 15:27:29,060:INFO:Initializing create_model()
2024-09-23 15:27:29,060:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002006DA135D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:29,060:INFO:Checking exceptions
2024-09-23 15:27:29,060:INFO:Importing libraries
2024-09-23 15:27:29,061:INFO:Copying training dataset
2024-09-23 15:27:29,064:INFO:Defining folds
2024-09-23 15:27:29,064:INFO:Declaring metric variables
2024-09-23 15:27:29,066:INFO:Importing untrained model
2024-09-23 15:27:29,067:INFO:Dummy Classifier Imported successfully
2024-09-23 15:27:29,071:INFO:Starting cross validation
2024-09-23 15:27:29,071:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:27:29,109:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,110:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,110:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,111:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,113:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,114:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,116:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,117:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,120:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,120:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:27:29,120:INFO:Calculating mean and std
2024-09-23 15:27:29,120:INFO:Creating metrics dataframe
2024-09-23 15:27:29,120:INFO:Uploading results into container
2024-09-23 15:27:29,120:INFO:Uploading model into container now
2024-09-23 15:27:29,120:INFO:_master_model_container: 16
2024-09-23 15:27:29,135:INFO:_display_container: 2
2024-09-23 15:27:29,135:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:27:29,135:INFO:create_model() successfully completed......................................
2024-09-23 15:27:29,280:INFO:SubProcess create_model() end ==================================
2024-09-23 15:27:29,280:INFO:Creating metrics dataframe
2024-09-23 15:27:29,298:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:27:29,303:INFO:Initializing create_model()
2024-09-23 15:27:29,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020073EAA750>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:27:29,303:INFO:Checking exceptions
2024-09-23 15:27:29,304:INFO:Importing libraries
2024-09-23 15:27:29,304:INFO:Copying training dataset
2024-09-23 15:27:29,307:INFO:Defining folds
2024-09-23 15:27:29,307:INFO:Declaring metric variables
2024-09-23 15:27:29,307:INFO:Importing untrained model
2024-09-23 15:27:29,307:INFO:Declaring custom model
2024-09-23 15:27:29,307:INFO:Logistic Regression Imported successfully
2024-09-23 15:27:29,307:INFO:Cross validation set to False
2024-09-23 15:27:29,307:INFO:Fitting Model
2024-09-23 15:27:29,425:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:27:29,425:INFO:create_model() successfully completed......................................
2024-09-23 15:27:29,568:INFO:_master_model_container: 16
2024-09-23 15:27:29,569:INFO:_display_container: 2
2024-09-23 15:27:29,570:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:27:29,570:INFO:compare_models() successfully completed......................................
2024-09-23 15:28:06,751:INFO:PyCaret ClassificationExperiment
2024-09-23 15:28:06,751:INFO:Logging name: clf-default-name
2024-09-23 15:28:06,751:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:28:06,751:INFO:version 3.3.1
2024-09-23 15:28:06,751:INFO:Initializing setup()
2024-09-23 15:28:06,751:INFO:self.USI: 7349
2024-09-23 15:28:06,751:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:28:06,751:INFO:Checking environment
2024-09-23 15:28:06,751:INFO:python_version: 3.11.9
2024-09-23 15:28:06,751:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:28:06,751:INFO:machine: AMD64
2024-09-23 15:28:06,751:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:28:06,751:INFO:Memory: svmem(total=16853458944, available=4292386816, percent=74.5, used=12561072128, free=4292386816)
2024-09-23 15:28:06,751:INFO:Physical Core: 16
2024-09-23 15:28:06,751:INFO:Logical Core: 24
2024-09-23 15:28:06,751:INFO:Checking libraries
2024-09-23 15:28:06,752:INFO:System:
2024-09-23 15:28:06,752:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:28:06,752:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:28:06,752:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:28:06,752:INFO:PyCaret required dependencies:
2024-09-23 15:28:06,752:INFO:                 pip: 24.2
2024-09-23 15:28:06,753:INFO:          setuptools: 75.1.0
2024-09-23 15:28:06,753:INFO:             pycaret: 3.3.1
2024-09-23 15:28:06,753:INFO:             IPython: 8.27.0
2024-09-23 15:28:06,753:INFO:          ipywidgets: 8.1.2
2024-09-23 15:28:06,753:INFO:                tqdm: 4.66.5
2024-09-23 15:28:06,753:INFO:               numpy: 1.26.4
2024-09-23 15:28:06,753:INFO:              pandas: 2.1.4
2024-09-23 15:28:06,753:INFO:              jinja2: 3.1.4
2024-09-23 15:28:06,753:INFO:               scipy: 1.11.4
2024-09-23 15:28:06,753:INFO:              joblib: 1.2.0
2024-09-23 15:28:06,753:INFO:             sklearn: 1.4.2
2024-09-23 15:28:06,753:INFO:                pyod: 2.0.2
2024-09-23 15:28:06,753:INFO:            imblearn: 0.12.3
2024-09-23 15:28:06,753:INFO:   category_encoders: 2.6.3
2024-09-23 15:28:06,753:INFO:            lightgbm: 4.5.0
2024-09-23 15:28:06,753:INFO:               numba: 0.60.0
2024-09-23 15:28:06,753:INFO:            requests: 2.32.3
2024-09-23 15:28:06,753:INFO:          matplotlib: 3.9.2
2024-09-23 15:28:06,753:INFO:          scikitplot: 0.3.7
2024-09-23 15:28:06,753:INFO:         yellowbrick: 1.5
2024-09-23 15:28:06,754:INFO:              plotly: 5.24.1
2024-09-23 15:28:06,754:INFO:    plotly-resampler: Not installed
2024-09-23 15:28:06,754:INFO:             kaleido: 0.2.1
2024-09-23 15:28:06,754:INFO:           schemdraw: 0.15
2024-09-23 15:28:06,754:INFO:         statsmodels: 0.14.2
2024-09-23 15:28:06,754:INFO:              sktime: 0.26.0
2024-09-23 15:28:06,754:INFO:               tbats: 1.1.3
2024-09-23 15:28:06,754:INFO:            pmdarima: 2.0.4
2024-09-23 15:28:06,754:INFO:              psutil: 5.9.0
2024-09-23 15:28:06,754:INFO:          markupsafe: 2.1.3
2024-09-23 15:28:06,754:INFO:             pickle5: Not installed
2024-09-23 15:28:06,754:INFO:         cloudpickle: 3.0.0
2024-09-23 15:28:06,754:INFO:         deprecation: 2.1.0
2024-09-23 15:28:06,754:INFO:              xxhash: 2.0.2
2024-09-23 15:28:06,754:INFO:           wurlitzer: 3.1.1
2024-09-23 15:28:06,754:INFO:PyCaret optional dependencies:
2024-09-23 15:28:06,754:INFO:                shap: Not installed
2024-09-23 15:28:06,754:INFO:           interpret: Not installed
2024-09-23 15:28:06,755:INFO:                umap: Not installed
2024-09-23 15:28:06,755:INFO:     ydata_profiling: Not installed
2024-09-23 15:28:06,755:INFO:  explainerdashboard: Not installed
2024-09-23 15:28:06,755:INFO:             autoviz: Not installed
2024-09-23 15:28:06,755:INFO:           fairlearn: Not installed
2024-09-23 15:28:06,755:INFO:          deepchecks: Not installed
2024-09-23 15:28:06,755:INFO:             xgboost: 2.1.1
2024-09-23 15:28:06,755:INFO:            catboost: 1.2.7
2024-09-23 15:28:06,755:INFO:              kmodes: Not installed
2024-09-23 15:28:06,755:INFO:             mlxtend: Not installed
2024-09-23 15:28:06,755:INFO:       statsforecast: Not installed
2024-09-23 15:28:06,755:INFO:        tune_sklearn: Not installed
2024-09-23 15:28:06,755:INFO:                 ray: Not installed
2024-09-23 15:28:06,755:INFO:            hyperopt: Not installed
2024-09-23 15:28:06,755:INFO:              optuna: Not installed
2024-09-23 15:28:06,755:INFO:               skopt: Not installed
2024-09-23 15:28:06,755:INFO:              mlflow: Not installed
2024-09-23 15:28:06,755:INFO:              gradio: Not installed
2024-09-23 15:28:06,755:INFO:             fastapi: Not installed
2024-09-23 15:28:06,755:INFO:             uvicorn: Not installed
2024-09-23 15:28:06,755:INFO:              m2cgen: Not installed
2024-09-23 15:28:06,755:INFO:           evidently: Not installed
2024-09-23 15:28:06,755:INFO:               fugue: Not installed
2024-09-23 15:28:06,756:INFO:           streamlit: Not installed
2024-09-23 15:28:06,756:INFO:             prophet: Not installed
2024-09-23 15:28:06,756:INFO:None
2024-09-23 15:28:06,756:INFO:Set up data.
2024-09-23 15:28:06,773:INFO:Set up folding strategy.
2024-09-23 15:28:06,773:INFO:Set up train/test split.
2024-09-23 15:28:06,787:INFO:Set up index.
2024-09-23 15:28:06,787:INFO:Assigning column types.
2024-09-23 15:28:06,804:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:28:06,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:28:06,853:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:28:06,871:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:06,873:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:06,896:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:28:06,896:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:28:06,903:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:06,903:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:06,903:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:28:06,918:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:28:06,933:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:06,933:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:06,965:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:28:06,980:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:06,982:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:06,982:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:28:07,020:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:07,020:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:07,067:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:07,067:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:07,067:INFO:Preparing preprocessing pipeline...
2024-09-23 15:28:07,067:INFO:Set up simple imputation.
2024-09-23 15:28:07,067:INFO:Set up feature normalization.
2024-09-23 15:28:07,067:INFO:Set up column name cleaning.
2024-09-23 15:28:07,114:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:28:07,130:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?'...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:28:07,130:INFO:Creating final display dataframe.
2024-09-23 15:28:07,274:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 25)
4        Transformed data shape       (30380, 25)
5   Transformed train set shape       (21266, 25)
6    Transformed test set shape        (9114, 25)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              7349
2024-09-23 15:28:07,322:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:07,323:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:07,361:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:28:07,363:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:28:07,363:INFO:setup() successfully completed in 0.61s...............
2024-09-23 15:28:14,256:INFO:Initializing compare_models()
2024-09-23 15:28:14,269:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:28:14,269:INFO:Checking exceptions
2024-09-23 15:28:14,280:INFO:Preparing display monitor
2024-09-23 15:28:14,286:INFO:Initializing Logistic Regression
2024-09-23 15:28:14,286:INFO:Total runtime is 0.0 minutes
2024-09-23 15:28:14,286:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:14,286:INFO:Initializing create_model()
2024-09-23 15:28:14,286:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:14,286:INFO:Checking exceptions
2024-09-23 15:28:14,286:INFO:Importing libraries
2024-09-23 15:28:14,286:INFO:Copying training dataset
2024-09-23 15:28:14,303:INFO:Defining folds
2024-09-23 15:28:14,303:INFO:Declaring metric variables
2024-09-23 15:28:14,303:INFO:Importing untrained model
2024-09-23 15:28:14,303:INFO:Logistic Regression Imported successfully
2024-09-23 15:28:14,303:INFO:Starting cross validation
2024-09-23 15:28:14,303:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:15,020:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,020:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,050:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,050:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,050:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,050:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,160:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,175:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,191:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,191:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,207:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,222:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,238:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,238:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:15,238:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:15,238:INFO:Calculating mean and std
2024-09-23 15:28:15,238:INFO:Creating metrics dataframe
2024-09-23 15:28:15,254:INFO:Uploading results into container
2024-09-23 15:28:15,254:INFO:Uploading model into container now
2024-09-23 15:28:15,254:INFO:_master_model_container: 1
2024-09-23 15:28:15,254:INFO:_display_container: 2
2024-09-23 15:28:15,254:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:28:15,254:INFO:create_model() successfully completed......................................
2024-09-23 15:28:15,366:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:15,366:INFO:Creating metrics dataframe
2024-09-23 15:28:15,366:INFO:Initializing K Neighbors Classifier
2024-09-23 15:28:15,366:INFO:Total runtime is 0.017994312445322673 minutes
2024-09-23 15:28:15,366:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:15,366:INFO:Initializing create_model()
2024-09-23 15:28:15,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:15,366:INFO:Checking exceptions
2024-09-23 15:28:15,366:INFO:Importing libraries
2024-09-23 15:28:15,366:INFO:Copying training dataset
2024-09-23 15:28:15,397:INFO:Defining folds
2024-09-23 15:28:15,397:INFO:Declaring metric variables
2024-09-23 15:28:15,399:INFO:Importing untrained model
2024-09-23 15:28:15,401:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:28:15,404:INFO:Starting cross validation
2024-09-23 15:28:15,405:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:15,897:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:16,006:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:16,068:INFO:Calculating mean and std
2024-09-23 15:28:16,068:INFO:Creating metrics dataframe
2024-09-23 15:28:16,068:INFO:Uploading results into container
2024-09-23 15:28:16,068:INFO:Uploading model into container now
2024-09-23 15:28:16,068:INFO:_master_model_container: 2
2024-09-23 15:28:16,068:INFO:_display_container: 2
2024-09-23 15:28:16,068:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:28:16,068:INFO:create_model() successfully completed......................................
2024-09-23 15:28:16,181:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:16,182:INFO:Creating metrics dataframe
2024-09-23 15:28:16,189:INFO:Initializing Naive Bayes
2024-09-23 15:28:16,189:INFO:Total runtime is 0.031709134578704834 minutes
2024-09-23 15:28:16,191:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:16,192:INFO:Initializing create_model()
2024-09-23 15:28:16,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:16,192:INFO:Checking exceptions
2024-09-23 15:28:16,192:INFO:Importing libraries
2024-09-23 15:28:16,192:INFO:Copying training dataset
2024-09-23 15:28:16,203:INFO:Defining folds
2024-09-23 15:28:16,203:INFO:Declaring metric variables
2024-09-23 15:28:16,205:INFO:Importing untrained model
2024-09-23 15:28:16,206:INFO:Naive Bayes Imported successfully
2024-09-23 15:28:16,210:INFO:Starting cross validation
2024-09-23 15:28:16,211:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:16,310:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:16,382:INFO:Calculating mean and std
2024-09-23 15:28:16,382:INFO:Creating metrics dataframe
2024-09-23 15:28:16,382:INFO:Uploading results into container
2024-09-23 15:28:16,382:INFO:Uploading model into container now
2024-09-23 15:28:16,382:INFO:_master_model_container: 3
2024-09-23 15:28:16,382:INFO:_display_container: 2
2024-09-23 15:28:16,382:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:28:16,382:INFO:create_model() successfully completed......................................
2024-09-23 15:28:16,498:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:16,498:INFO:Creating metrics dataframe
2024-09-23 15:28:16,498:INFO:Initializing Decision Tree Classifier
2024-09-23 15:28:16,498:INFO:Total runtime is 0.036851958433787024 minutes
2024-09-23 15:28:16,498:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:16,498:INFO:Initializing create_model()
2024-09-23 15:28:16,498:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:16,498:INFO:Checking exceptions
2024-09-23 15:28:16,498:INFO:Importing libraries
2024-09-23 15:28:16,498:INFO:Copying training dataset
2024-09-23 15:28:16,520:INFO:Defining folds
2024-09-23 15:28:16,520:INFO:Declaring metric variables
2024-09-23 15:28:16,523:INFO:Importing untrained model
2024-09-23 15:28:16,523:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:28:16,527:INFO:Starting cross validation
2024-09-23 15:28:16,527:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:16,888:INFO:Calculating mean and std
2024-09-23 15:28:16,888:INFO:Creating metrics dataframe
2024-09-23 15:28:16,888:INFO:Uploading results into container
2024-09-23 15:28:16,888:INFO:Uploading model into container now
2024-09-23 15:28:16,888:INFO:_master_model_container: 4
2024-09-23 15:28:16,888:INFO:_display_container: 2
2024-09-23 15:28:16,888:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:28:16,888:INFO:create_model() successfully completed......................................
2024-09-23 15:28:17,045:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:17,045:INFO:Creating metrics dataframe
2024-09-23 15:28:17,053:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:28:17,053:INFO:Total runtime is 0.04610912005106608 minutes
2024-09-23 15:28:17,055:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:17,056:INFO:Initializing create_model()
2024-09-23 15:28:17,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:17,056:INFO:Checking exceptions
2024-09-23 15:28:17,056:INFO:Importing libraries
2024-09-23 15:28:17,056:INFO:Copying training dataset
2024-09-23 15:28:17,070:INFO:Defining folds
2024-09-23 15:28:17,072:INFO:Declaring metric variables
2024-09-23 15:28:17,073:INFO:Importing untrained model
2024-09-23 15:28:17,075:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:28:17,079:INFO:Starting cross validation
2024-09-23 15:28:17,079:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:17,350:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,397:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,413:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,413:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,413:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,413:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,429:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,429:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,429:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,444:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,460:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,460:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,476:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,476:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,523:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,523:INFO:Calculating mean and std
2024-09-23 15:28:17,523:INFO:Creating metrics dataframe
2024-09-23 15:28:17,523:INFO:Uploading results into container
2024-09-23 15:28:17,523:INFO:Uploading model into container now
2024-09-23 15:28:17,523:INFO:_master_model_container: 5
2024-09-23 15:28:17,523:INFO:_display_container: 2
2024-09-23 15:28:17,523:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:28:17,523:INFO:create_model() successfully completed......................................
2024-09-23 15:28:17,632:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:17,632:INFO:Creating metrics dataframe
2024-09-23 15:28:17,646:INFO:Initializing Ridge Classifier
2024-09-23 15:28:17,646:INFO:Total runtime is 0.05599467754364013 minutes
2024-09-23 15:28:17,648:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:17,648:INFO:Initializing create_model()
2024-09-23 15:28:17,648:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:17,648:INFO:Checking exceptions
2024-09-23 15:28:17,648:INFO:Importing libraries
2024-09-23 15:28:17,648:INFO:Copying training dataset
2024-09-23 15:28:17,660:INFO:Defining folds
2024-09-23 15:28:17,660:INFO:Declaring metric variables
2024-09-23 15:28:17,662:INFO:Importing untrained model
2024-09-23 15:28:17,663:INFO:Ridge Classifier Imported successfully
2024-09-23 15:28:17,666:INFO:Starting cross validation
2024-09-23 15:28:17,667:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:17,773:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,774:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,778:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,779:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,782:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,783:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,786:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,786:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,802:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,802:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,818:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,818:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,818:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,818:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,818:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:17,833:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:17,849:INFO:Calculating mean and std
2024-09-23 15:28:17,849:INFO:Creating metrics dataframe
2024-09-23 15:28:17,849:INFO:Uploading results into container
2024-09-23 15:28:17,849:INFO:Uploading model into container now
2024-09-23 15:28:17,849:INFO:_master_model_container: 6
2024-09-23 15:28:17,849:INFO:_display_container: 2
2024-09-23 15:28:17,849:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:28:17,849:INFO:create_model() successfully completed......................................
2024-09-23 15:28:18,011:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:18,011:INFO:Creating metrics dataframe
2024-09-23 15:28:18,011:INFO:Initializing Random Forest Classifier
2024-09-23 15:28:18,011:INFO:Total runtime is 0.06207458178202311 minutes
2024-09-23 15:28:18,011:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:18,011:INFO:Initializing create_model()
2024-09-23 15:28:18,011:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:18,011:INFO:Checking exceptions
2024-09-23 15:28:18,011:INFO:Importing libraries
2024-09-23 15:28:18,011:INFO:Copying training dataset
2024-09-23 15:28:18,031:INFO:Defining folds
2024-09-23 15:28:18,031:INFO:Declaring metric variables
2024-09-23 15:28:18,036:INFO:Importing untrained model
2024-09-23 15:28:18,039:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:28:18,040:INFO:Starting cross validation
2024-09-23 15:28:18,040:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:19,843:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,843:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,843:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,843:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,843:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,843:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,885:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,900:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:19,994:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:20,010:INFO:Calculating mean and std
2024-09-23 15:28:20,010:INFO:Creating metrics dataframe
2024-09-23 15:28:20,010:INFO:Uploading results into container
2024-09-23 15:28:20,010:INFO:Uploading model into container now
2024-09-23 15:28:20,010:INFO:_master_model_container: 7
2024-09-23 15:28:20,010:INFO:_display_container: 2
2024-09-23 15:28:20,010:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:28:20,010:INFO:create_model() successfully completed......................................
2024-09-23 15:28:20,133:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:20,133:INFO:Creating metrics dataframe
2024-09-23 15:28:20,133:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:28:20,133:INFO:Total runtime is 0.0974406083424886 minutes
2024-09-23 15:28:20,144:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:20,144:INFO:Initializing create_model()
2024-09-23 15:28:20,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:20,144:INFO:Checking exceptions
2024-09-23 15:28:20,144:INFO:Importing libraries
2024-09-23 15:28:20,144:INFO:Copying training dataset
2024-09-23 15:28:20,155:INFO:Defining folds
2024-09-23 15:28:20,155:INFO:Declaring metric variables
2024-09-23 15:28:20,155:INFO:Importing untrained model
2024-09-23 15:28:20,155:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:28:20,155:INFO:Starting cross validation
2024-09-23 15:28:20,155:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:20,228:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,228:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,271:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,285:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,286:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,304:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,308:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,310:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,321:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,322:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,324:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,327:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,330:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:28:20,336:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,352:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:20,352:INFO:Calculating mean and std
2024-09-23 15:28:20,352:INFO:Creating metrics dataframe
2024-09-23 15:28:20,352:INFO:Uploading results into container
2024-09-23 15:28:20,352:INFO:Uploading model into container now
2024-09-23 15:28:20,352:INFO:_master_model_container: 8
2024-09-23 15:28:20,352:INFO:_display_container: 2
2024-09-23 15:28:20,352:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:28:20,352:INFO:create_model() successfully completed......................................
2024-09-23 15:28:20,481:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:20,481:INFO:Creating metrics dataframe
2024-09-23 15:28:20,481:INFO:Initializing Ada Boost Classifier
2024-09-23 15:28:20,481:INFO:Total runtime is 0.10324958960215251 minutes
2024-09-23 15:28:20,496:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:20,496:INFO:Initializing create_model()
2024-09-23 15:28:20,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:20,496:INFO:Checking exceptions
2024-09-23 15:28:20,496:INFO:Importing libraries
2024-09-23 15:28:20,496:INFO:Copying training dataset
2024-09-23 15:28:20,505:INFO:Defining folds
2024-09-23 15:28:20,505:INFO:Declaring metric variables
2024-09-23 15:28:20,505:INFO:Importing untrained model
2024-09-23 15:28:20,505:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:28:20,522:INFO:Starting cross validation
2024-09-23 15:28:20,522:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:20,585:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,585:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,607:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,613:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,616:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,619:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,636:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,637:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,637:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:20,653:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:28:21,508:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,539:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,554:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,570:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,570:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,586:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,648:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,727:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,758:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,758:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:21,758:INFO:Calculating mean and std
2024-09-23 15:28:21,758:INFO:Creating metrics dataframe
2024-09-23 15:28:21,758:INFO:Uploading results into container
2024-09-23 15:28:21,758:INFO:Uploading model into container now
2024-09-23 15:28:21,758:INFO:_master_model_container: 9
2024-09-23 15:28:21,758:INFO:_display_container: 2
2024-09-23 15:28:21,758:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:28:21,758:INFO:create_model() successfully completed......................................
2024-09-23 15:28:21,899:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:21,899:INFO:Creating metrics dataframe
2024-09-23 15:28:21,912:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:28:21,912:INFO:Total runtime is 0.12708668311436971 minutes
2024-09-23 15:28:21,914:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:21,914:INFO:Initializing create_model()
2024-09-23 15:28:21,914:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:21,914:INFO:Checking exceptions
2024-09-23 15:28:21,914:INFO:Importing libraries
2024-09-23 15:28:21,914:INFO:Copying training dataset
2024-09-23 15:28:21,925:INFO:Defining folds
2024-09-23 15:28:21,925:INFO:Declaring metric variables
2024-09-23 15:28:21,925:INFO:Importing untrained model
2024-09-23 15:28:21,925:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:28:21,925:INFO:Starting cross validation
2024-09-23 15:28:21,925:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:50,632:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:51,864:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:51,871:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:52,464:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:52,915:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:53,403:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:54,724:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:54,740:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:54,906:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:54,919:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:55,261:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,261:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:55,277:INFO:Calculating mean and std
2024-09-23 15:28:55,277:INFO:Creating metrics dataframe
2024-09-23 15:28:55,277:INFO:Uploading results into container
2024-09-23 15:28:55,277:INFO:Uploading model into container now
2024-09-23 15:28:55,277:INFO:_master_model_container: 10
2024-09-23 15:28:55,277:INFO:_display_container: 2
2024-09-23 15:28:55,277:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:28:55,277:INFO:create_model() successfully completed......................................
2024-09-23 15:28:55,398:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:55,398:INFO:Creating metrics dataframe
2024-09-23 15:28:55,403:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:28:55,403:INFO:Total runtime is 0.6852754275004069 minutes
2024-09-23 15:28:55,406:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:55,406:INFO:Initializing create_model()
2024-09-23 15:28:55,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:55,406:INFO:Checking exceptions
2024-09-23 15:28:55,406:INFO:Importing libraries
2024-09-23 15:28:55,406:INFO:Copying training dataset
2024-09-23 15:28:55,409:INFO:Defining folds
2024-09-23 15:28:55,409:INFO:Declaring metric variables
2024-09-23 15:28:55,420:INFO:Importing untrained model
2024-09-23 15:28:55,420:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:28:55,420:INFO:Starting cross validation
2024-09-23 15:28:55,420:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:55,538:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,568:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,568:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,573:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:55,574:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:55,581:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,582:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,586:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:55,586:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,601:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,601:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,618:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,619:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:28:55,621:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:55,630:INFO:Calculating mean and std
2024-09-23 15:28:55,631:INFO:Creating metrics dataframe
2024-09-23 15:28:55,632:INFO:Uploading results into container
2024-09-23 15:28:55,633:INFO:Uploading model into container now
2024-09-23 15:28:55,633:INFO:_master_model_container: 11
2024-09-23 15:28:55,634:INFO:_display_container: 2
2024-09-23 15:28:55,634:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:28:55,634:INFO:create_model() successfully completed......................................
2024-09-23 15:28:55,745:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:55,745:INFO:Creating metrics dataframe
2024-09-23 15:28:55,745:INFO:Initializing Extra Trees Classifier
2024-09-23 15:28:55,745:INFO:Total runtime is 0.6909837166468302 minutes
2024-09-23 15:28:55,745:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:55,745:INFO:Initializing create_model()
2024-09-23 15:28:55,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:55,745:INFO:Checking exceptions
2024-09-23 15:28:55,745:INFO:Importing libraries
2024-09-23 15:28:55,745:INFO:Copying training dataset
2024-09-23 15:28:55,773:INFO:Defining folds
2024-09-23 15:28:55,773:INFO:Declaring metric variables
2024-09-23 15:28:55,776:INFO:Importing untrained model
2024-09-23 15:28:55,777:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:28:55,780:INFO:Starting cross validation
2024-09-23 15:28:55,781:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:28:57,966:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,045:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,046:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,050:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,069:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,069:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,085:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,101:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,144:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:28:58,153:INFO:Calculating mean and std
2024-09-23 15:28:58,154:INFO:Creating metrics dataframe
2024-09-23 15:28:58,156:INFO:Uploading results into container
2024-09-23 15:28:58,156:INFO:Uploading model into container now
2024-09-23 15:28:58,156:INFO:_master_model_container: 12
2024-09-23 15:28:58,157:INFO:_display_container: 2
2024-09-23 15:28:58,157:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:28:58,157:INFO:create_model() successfully completed......................................
2024-09-23 15:28:58,280:INFO:SubProcess create_model() end ==================================
2024-09-23 15:28:58,280:INFO:Creating metrics dataframe
2024-09-23 15:28:58,285:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:28:58,285:INFO:Total runtime is 0.733305037021637 minutes
2024-09-23 15:28:58,287:INFO:SubProcess create_model() called ==================================
2024-09-23 15:28:58,288:INFO:Initializing create_model()
2024-09-23 15:28:58,288:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:28:58,288:INFO:Checking exceptions
2024-09-23 15:28:58,288:INFO:Importing libraries
2024-09-23 15:28:58,288:INFO:Copying training dataset
2024-09-23 15:28:58,289:INFO:Defining folds
2024-09-23 15:28:58,289:INFO:Declaring metric variables
2024-09-23 15:28:58,289:INFO:Importing untrained model
2024-09-23 15:28:58,303:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:28:58,307:INFO:Starting cross validation
2024-09-23 15:28:58,308:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:29:01,257:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,313:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,351:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,435:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,441:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,549:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,637:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,723:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,756:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:01,756:INFO:Calculating mean and std
2024-09-23 15:29:01,756:INFO:Creating metrics dataframe
2024-09-23 15:29:01,756:INFO:Uploading results into container
2024-09-23 15:29:01,756:INFO:Uploading model into container now
2024-09-23 15:29:01,756:INFO:_master_model_container: 13
2024-09-23 15:29:01,756:INFO:_display_container: 2
2024-09-23 15:29:01,756:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:29:01,756:INFO:create_model() successfully completed......................................
2024-09-23 15:29:01,877:INFO:SubProcess create_model() end ==================================
2024-09-23 15:29:01,877:INFO:Creating metrics dataframe
2024-09-23 15:29:01,884:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:29:01,884:INFO:Total runtime is 0.7933013677597046 minutes
2024-09-23 15:29:01,887:INFO:SubProcess create_model() called ==================================
2024-09-23 15:29:01,888:INFO:Initializing create_model()
2024-09-23 15:29:01,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:29:01,888:INFO:Checking exceptions
2024-09-23 15:29:01,888:INFO:Importing libraries
2024-09-23 15:29:01,888:INFO:Copying training dataset
2024-09-23 15:29:01,889:INFO:Defining folds
2024-09-23 15:29:01,889:INFO:Declaring metric variables
2024-09-23 15:29:01,889:INFO:Importing untrained model
2024-09-23 15:29:01,904:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:29:01,906:INFO:Starting cross validation
2024-09-23 15:29:01,906:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:29:19,016:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:19,063:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:19,250:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:19,297:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:21,149:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:21,181:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:29:21,274:INFO:Calculating mean and std
2024-09-23 15:29:21,274:INFO:Creating metrics dataframe
2024-09-23 15:29:21,274:INFO:Uploading results into container
2024-09-23 15:29:21,274:INFO:Uploading model into container now
2024-09-23 15:29:21,274:INFO:_master_model_container: 14
2024-09-23 15:29:21,274:INFO:_display_container: 2
2024-09-23 15:29:21,274:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:29:21,274:INFO:create_model() successfully completed......................................
2024-09-23 15:29:21,447:INFO:SubProcess create_model() end ==================================
2024-09-23 15:29:21,447:INFO:Creating metrics dataframe
2024-09-23 15:29:21,453:INFO:Initializing CatBoost Classifier
2024-09-23 15:29:21,453:INFO:Total runtime is 1.1194433848063152 minutes
2024-09-23 15:29:21,459:INFO:SubProcess create_model() called ==================================
2024-09-23 15:29:21,459:INFO:Initializing create_model()
2024-09-23 15:29:21,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:29:21,459:INFO:Checking exceptions
2024-09-23 15:29:21,459:INFO:Importing libraries
2024-09-23 15:29:21,459:INFO:Copying training dataset
2024-09-23 15:29:21,475:INFO:Defining folds
2024-09-23 15:29:21,475:INFO:Declaring metric variables
2024-09-23 15:29:21,477:INFO:Importing untrained model
2024-09-23 15:29:21,479:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:29:21,482:INFO:Starting cross validation
2024-09-23 15:29:21,483:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:30:20,520:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:21,999:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:26,508:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:26,552:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:26,650:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:26,994:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:27,636:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:27,715:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:27,845:INFO:Calculating mean and std
2024-09-23 15:30:27,846:INFO:Creating metrics dataframe
2024-09-23 15:30:27,847:INFO:Uploading results into container
2024-09-23 15:30:27,847:INFO:Uploading model into container now
2024-09-23 15:30:27,847:INFO:_master_model_container: 15
2024-09-23 15:30:27,847:INFO:_display_container: 2
2024-09-23 15:30:27,848:INFO:<catboost.core.CatBoostClassifier object at 0x0000020070A144D0>
2024-09-23 15:30:27,848:INFO:create_model() successfully completed......................................
2024-09-23 15:30:28,026:INFO:SubProcess create_model() end ==================================
2024-09-23 15:30:28,026:INFO:Creating metrics dataframe
2024-09-23 15:30:28,041:INFO:Initializing Dummy Classifier
2024-09-23 15:30:28,041:INFO:Total runtime is 2.2292469143867493 minutes
2024-09-23 15:30:28,041:INFO:SubProcess create_model() called ==================================
2024-09-23 15:30:28,041:INFO:Initializing create_model()
2024-09-23 15:30:28,041:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020073D13110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:30:28,041:INFO:Checking exceptions
2024-09-23 15:30:28,041:INFO:Importing libraries
2024-09-23 15:30:28,041:INFO:Copying training dataset
2024-09-23 15:30:28,056:INFO:Defining folds
2024-09-23 15:30:28,056:INFO:Declaring metric variables
2024-09-23 15:30:28,056:INFO:Importing untrained model
2024-09-23 15:30:28,056:INFO:Dummy Classifier Imported successfully
2024-09-23 15:30:28,056:INFO:Starting cross validation
2024-09-23 15:30:28,056:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:30:28,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,179:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,186:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,186:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:30:28,217:INFO:Calculating mean and std
2024-09-23 15:30:28,217:INFO:Creating metrics dataframe
2024-09-23 15:30:28,217:INFO:Uploading results into container
2024-09-23 15:30:28,217:INFO:Uploading model into container now
2024-09-23 15:30:28,217:INFO:_master_model_container: 16
2024-09-23 15:30:28,217:INFO:_display_container: 2
2024-09-23 15:30:28,217:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:30:28,217:INFO:create_model() successfully completed......................................
2024-09-23 15:30:28,350:INFO:SubProcess create_model() end ==================================
2024-09-23 15:30:28,350:INFO:Creating metrics dataframe
2024-09-23 15:30:28,357:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:30:28,362:INFO:Initializing create_model()
2024-09-23 15:30:28,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002006D937A10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:30:28,362:INFO:Checking exceptions
2024-09-23 15:30:28,364:INFO:Importing libraries
2024-09-23 15:30:28,364:INFO:Copying training dataset
2024-09-23 15:30:28,374:INFO:Defining folds
2024-09-23 15:30:28,374:INFO:Declaring metric variables
2024-09-23 15:30:28,374:INFO:Importing untrained model
2024-09-23 15:30:28,374:INFO:Declaring custom model
2024-09-23 15:30:28,374:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:30:28,374:INFO:Cross validation set to False
2024-09-23 15:30:28,374:INFO:Fitting Model
2024-09-23 15:30:54,637:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:30:54,637:INFO:create_model() successfully completed......................................
2024-09-23 15:30:54,787:INFO:_master_model_container: 16
2024-09-23 15:30:54,787:INFO:_display_container: 2
2024-09-23 15:30:54,787:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:30:54,787:INFO:compare_models() successfully completed......................................
2024-09-23 15:41:31,598:INFO:PyCaret ClassificationExperiment
2024-09-23 15:41:31,598:INFO:Logging name: clf-default-name
2024-09-23 15:41:31,598:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 15:41:31,598:INFO:version 3.3.1
2024-09-23 15:41:31,598:INFO:Initializing setup()
2024-09-23 15:41:31,598:INFO:self.USI: 89b2
2024-09-23 15:41:31,598:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 15:41:31,598:INFO:Checking environment
2024-09-23 15:41:31,598:INFO:python_version: 3.11.9
2024-09-23 15:41:31,598:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 15:41:31,598:INFO:machine: AMD64
2024-09-23 15:41:31,598:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 15:41:31,598:INFO:Memory: svmem(total=16853458944, available=7796826112, percent=53.7, used=9056632832, free=7796826112)
2024-09-23 15:41:31,599:INFO:Physical Core: 16
2024-09-23 15:41:31,599:INFO:Logical Core: 24
2024-09-23 15:41:31,599:INFO:Checking libraries
2024-09-23 15:41:31,599:INFO:System:
2024-09-23 15:41:31,599:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 15:41:31,599:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 15:41:31,599:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 15:41:31,599:INFO:PyCaret required dependencies:
2024-09-23 15:41:31,599:INFO:                 pip: 24.2
2024-09-23 15:41:31,599:INFO:          setuptools: 75.1.0
2024-09-23 15:41:31,599:INFO:             pycaret: 3.3.1
2024-09-23 15:41:31,599:INFO:             IPython: 8.27.0
2024-09-23 15:41:31,599:INFO:          ipywidgets: 8.1.2
2024-09-23 15:41:31,599:INFO:                tqdm: 4.66.5
2024-09-23 15:41:31,599:INFO:               numpy: 1.26.4
2024-09-23 15:41:31,599:INFO:              pandas: 2.1.4
2024-09-23 15:41:31,599:INFO:              jinja2: 3.1.4
2024-09-23 15:41:31,599:INFO:               scipy: 1.11.4
2024-09-23 15:41:31,599:INFO:              joblib: 1.2.0
2024-09-23 15:41:31,599:INFO:             sklearn: 1.4.2
2024-09-23 15:41:31,599:INFO:                pyod: 2.0.2
2024-09-23 15:41:31,599:INFO:            imblearn: 0.12.3
2024-09-23 15:41:31,599:INFO:   category_encoders: 2.6.3
2024-09-23 15:41:31,599:INFO:            lightgbm: 4.5.0
2024-09-23 15:41:31,599:INFO:               numba: 0.60.0
2024-09-23 15:41:31,599:INFO:            requests: 2.32.3
2024-09-23 15:41:31,599:INFO:          matplotlib: 3.9.2
2024-09-23 15:41:31,599:INFO:          scikitplot: 0.3.7
2024-09-23 15:41:31,599:INFO:         yellowbrick: 1.5
2024-09-23 15:41:31,599:INFO:              plotly: 5.24.1
2024-09-23 15:41:31,599:INFO:    plotly-resampler: Not installed
2024-09-23 15:41:31,599:INFO:             kaleido: 0.2.1
2024-09-23 15:41:31,599:INFO:           schemdraw: 0.15
2024-09-23 15:41:31,599:INFO:         statsmodels: 0.14.2
2024-09-23 15:41:31,599:INFO:              sktime: 0.26.0
2024-09-23 15:41:31,599:INFO:               tbats: 1.1.3
2024-09-23 15:41:31,599:INFO:            pmdarima: 2.0.4
2024-09-23 15:41:31,599:INFO:              psutil: 5.9.0
2024-09-23 15:41:31,599:INFO:          markupsafe: 2.1.3
2024-09-23 15:41:31,599:INFO:             pickle5: Not installed
2024-09-23 15:41:31,599:INFO:         cloudpickle: 3.0.0
2024-09-23 15:41:31,599:INFO:         deprecation: 2.1.0
2024-09-23 15:41:31,599:INFO:              xxhash: 2.0.2
2024-09-23 15:41:31,599:INFO:           wurlitzer: 3.1.1
2024-09-23 15:41:31,599:INFO:PyCaret optional dependencies:
2024-09-23 15:41:31,599:INFO:                shap: Not installed
2024-09-23 15:41:31,599:INFO:           interpret: Not installed
2024-09-23 15:41:31,599:INFO:                umap: Not installed
2024-09-23 15:41:31,599:INFO:     ydata_profiling: Not installed
2024-09-23 15:41:31,599:INFO:  explainerdashboard: Not installed
2024-09-23 15:41:31,599:INFO:             autoviz: Not installed
2024-09-23 15:41:31,599:INFO:           fairlearn: Not installed
2024-09-23 15:41:31,599:INFO:          deepchecks: Not installed
2024-09-23 15:41:31,599:INFO:             xgboost: 2.1.1
2024-09-23 15:41:31,599:INFO:            catboost: 1.2.7
2024-09-23 15:41:31,599:INFO:              kmodes: Not installed
2024-09-23 15:41:31,599:INFO:             mlxtend: Not installed
2024-09-23 15:41:31,600:INFO:       statsforecast: Not installed
2024-09-23 15:41:31,600:INFO:        tune_sklearn: Not installed
2024-09-23 15:41:31,600:INFO:                 ray: Not installed
2024-09-23 15:41:31,600:INFO:            hyperopt: Not installed
2024-09-23 15:41:31,600:INFO:              optuna: Not installed
2024-09-23 15:41:31,600:INFO:               skopt: Not installed
2024-09-23 15:41:31,600:INFO:              mlflow: Not installed
2024-09-23 15:41:31,600:INFO:              gradio: Not installed
2024-09-23 15:41:31,600:INFO:             fastapi: Not installed
2024-09-23 15:41:31,600:INFO:             uvicorn: Not installed
2024-09-23 15:41:31,600:INFO:              m2cgen: Not installed
2024-09-23 15:41:31,600:INFO:           evidently: Not installed
2024-09-23 15:41:31,600:INFO:               fugue: Not installed
2024-09-23 15:41:31,600:INFO:           streamlit: Not installed
2024-09-23 15:41:31,600:INFO:             prophet: Not installed
2024-09-23 15:41:31,600:INFO:None
2024-09-23 15:41:31,600:INFO:Set up data.
2024-09-23 15:41:31,607:INFO:Set up folding strategy.
2024-09-23 15:41:31,607:INFO:Set up train/test split.
2024-09-23 15:41:31,619:INFO:Set up index.
2024-09-23 15:41:31,619:INFO:Assigning column types.
2024-09-23 15:41:31,619:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 15:41:31,652:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:41:31,652:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:41:31,670:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:31,670:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:31,686:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 15:41:31,686:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:41:31,708:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:31,709:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:31,710:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 15:41:31,733:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:41:31,746:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:31,747:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:31,767:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 15:41:31,767:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:31,767:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:31,767:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 15:41:31,814:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:31,814:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:31,853:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:31,853:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:31,853:INFO:Preparing preprocessing pipeline...
2024-09-23 15:41:31,853:INFO:Set up simple imputation.
2024-09-23 15:41:31,853:INFO:Set up imbalanced handling.
2024-09-23 15:41:31,853:INFO:Set up feature normalization.
2024-09-23 15:41:31,853:INFO:Set up column name cleaning.
2024-09-23 15:41:32,325:INFO:Finished creating preprocessing pipeline.
2024-09-23 15:41:32,325:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?'...
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=123,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 15:41:32,325:INFO:Creating final display dataframe.
2024-09-23 15:41:32,823:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 25)
4        Transformed data shape       (46170, 25)
5   Transformed train set shape       (37056, 25)
6    Transformed test set shape        (9114, 25)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              89b2
2024-09-23 15:41:32,869:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:32,870:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:32,904:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 15:41:32,904:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 15:41:32,904:INFO:setup() successfully completed in 1.31s...............
2024-09-23 15:41:39,069:INFO:Initializing compare_models()
2024-09-23 15:41:39,069:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 15:41:39,069:INFO:Checking exceptions
2024-09-23 15:41:39,080:INFO:Preparing display monitor
2024-09-23 15:41:39,087:INFO:Initializing Logistic Regression
2024-09-23 15:41:39,087:INFO:Total runtime is 0.0 minutes
2024-09-23 15:41:39,087:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:39,087:INFO:Initializing create_model()
2024-09-23 15:41:39,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:39,087:INFO:Checking exceptions
2024-09-23 15:41:39,087:INFO:Importing libraries
2024-09-23 15:41:39,087:INFO:Copying training dataset
2024-09-23 15:41:39,103:INFO:Defining folds
2024-09-23 15:41:39,103:INFO:Declaring metric variables
2024-09-23 15:41:39,103:INFO:Importing untrained model
2024-09-23 15:41:39,103:INFO:Logistic Regression Imported successfully
2024-09-23 15:41:39,103:INFO:Starting cross validation
2024-09-23 15:41:39,103:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:43,296:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,328:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,390:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,453:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,484:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,515:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,547:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,578:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,578:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,640:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:43,640:INFO:Calculating mean and std
2024-09-23 15:41:43,656:INFO:Creating metrics dataframe
2024-09-23 15:41:43,656:INFO:Uploading results into container
2024-09-23 15:41:43,656:INFO:Uploading model into container now
2024-09-23 15:41:43,656:INFO:_master_model_container: 1
2024-09-23 15:41:43,656:INFO:_display_container: 2
2024-09-23 15:41:43,656:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 15:41:43,656:INFO:create_model() successfully completed......................................
2024-09-23 15:41:43,828:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:43,828:INFO:Creating metrics dataframe
2024-09-23 15:41:43,828:INFO:Initializing K Neighbors Classifier
2024-09-23 15:41:43,828:INFO:Total runtime is 0.07901448011398315 minutes
2024-09-23 15:41:43,828:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:43,828:INFO:Initializing create_model()
2024-09-23 15:41:43,828:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:43,828:INFO:Checking exceptions
2024-09-23 15:41:43,828:INFO:Importing libraries
2024-09-23 15:41:43,828:INFO:Copying training dataset
2024-09-23 15:41:43,853:INFO:Defining folds
2024-09-23 15:41:43,853:INFO:Declaring metric variables
2024-09-23 15:41:43,854:INFO:Importing untrained model
2024-09-23 15:41:43,854:INFO:K Neighbors Classifier Imported successfully
2024-09-23 15:41:43,858:INFO:Starting cross validation
2024-09-23 15:41:43,858:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:46,362:INFO:Calculating mean and std
2024-09-23 15:41:46,362:INFO:Creating metrics dataframe
2024-09-23 15:41:46,362:INFO:Uploading results into container
2024-09-23 15:41:46,362:INFO:Uploading model into container now
2024-09-23 15:41:46,362:INFO:_master_model_container: 2
2024-09-23 15:41:46,362:INFO:_display_container: 2
2024-09-23 15:41:46,362:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 15:41:46,362:INFO:create_model() successfully completed......................................
2024-09-23 15:41:46,524:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:46,524:INFO:Creating metrics dataframe
2024-09-23 15:41:46,524:INFO:Initializing Naive Bayes
2024-09-23 15:41:46,524:INFO:Total runtime is 0.12394702831904093 minutes
2024-09-23 15:41:46,524:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:46,524:INFO:Initializing create_model()
2024-09-23 15:41:46,524:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:46,524:INFO:Checking exceptions
2024-09-23 15:41:46,524:INFO:Importing libraries
2024-09-23 15:41:46,524:INFO:Copying training dataset
2024-09-23 15:41:46,543:INFO:Defining folds
2024-09-23 15:41:46,543:INFO:Declaring metric variables
2024-09-23 15:41:46,543:INFO:Importing untrained model
2024-09-23 15:41:46,543:INFO:Naive Bayes Imported successfully
2024-09-23 15:41:46,543:INFO:Starting cross validation
2024-09-23 15:41:46,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:48,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:41:48,233:INFO:Calculating mean and std
2024-09-23 15:41:48,233:INFO:Creating metrics dataframe
2024-09-23 15:41:48,233:INFO:Uploading results into container
2024-09-23 15:41:48,249:INFO:Uploading model into container now
2024-09-23 15:41:48,249:INFO:_master_model_container: 3
2024-09-23 15:41:48,249:INFO:_display_container: 2
2024-09-23 15:41:48,249:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 15:41:48,249:INFO:create_model() successfully completed......................................
2024-09-23 15:41:48,358:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:48,358:INFO:Creating metrics dataframe
2024-09-23 15:41:48,381:INFO:Initializing Decision Tree Classifier
2024-09-23 15:41:48,381:INFO:Total runtime is 0.15489368438720702 minutes
2024-09-23 15:41:48,382:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:48,382:INFO:Initializing create_model()
2024-09-23 15:41:48,382:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:48,382:INFO:Checking exceptions
2024-09-23 15:41:48,382:INFO:Importing libraries
2024-09-23 15:41:48,382:INFO:Copying training dataset
2024-09-23 15:41:48,389:INFO:Defining folds
2024-09-23 15:41:48,389:INFO:Declaring metric variables
2024-09-23 15:41:48,389:INFO:Importing untrained model
2024-09-23 15:41:48,389:INFO:Decision Tree Classifier Imported successfully
2024-09-23 15:41:48,402:INFO:Starting cross validation
2024-09-23 15:41:48,403:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:49,432:INFO:Calculating mean and std
2024-09-23 15:41:49,432:INFO:Creating metrics dataframe
2024-09-23 15:41:49,433:INFO:Uploading results into container
2024-09-23 15:41:49,434:INFO:Uploading model into container now
2024-09-23 15:41:49,436:INFO:_master_model_container: 4
2024-09-23 15:41:49,436:INFO:_display_container: 2
2024-09-23 15:41:49,436:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 15:41:49,436:INFO:create_model() successfully completed......................................
2024-09-23 15:41:49,538:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:49,538:INFO:Creating metrics dataframe
2024-09-23 15:41:49,554:INFO:Initializing SVM - Linear Kernel
2024-09-23 15:41:49,554:INFO:Total runtime is 0.17444470326105752 minutes
2024-09-23 15:41:49,554:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:49,554:INFO:Initializing create_model()
2024-09-23 15:41:49,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:49,554:INFO:Checking exceptions
2024-09-23 15:41:49,554:INFO:Importing libraries
2024-09-23 15:41:49,554:INFO:Copying training dataset
2024-09-23 15:41:49,554:INFO:Defining folds
2024-09-23 15:41:49,554:INFO:Declaring metric variables
2024-09-23 15:41:49,569:INFO:Importing untrained model
2024-09-23 15:41:49,571:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 15:41:49,574:INFO:Starting cross validation
2024-09-23 15:41:49,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:50,115:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,308:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,371:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,407:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,434:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,449:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,449:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,449:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,465:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,512:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:50,527:INFO:Calculating mean and std
2024-09-23 15:41:50,527:INFO:Creating metrics dataframe
2024-09-23 15:41:50,527:INFO:Uploading results into container
2024-09-23 15:41:50,527:INFO:Uploading model into container now
2024-09-23 15:41:50,527:INFO:_master_model_container: 5
2024-09-23 15:41:50,527:INFO:_display_container: 2
2024-09-23 15:41:50,527:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 15:41:50,527:INFO:create_model() successfully completed......................................
2024-09-23 15:41:50,674:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:50,674:INFO:Creating metrics dataframe
2024-09-23 15:41:50,680:INFO:Initializing Ridge Classifier
2024-09-23 15:41:50,681:INFO:Total runtime is 0.1932262063026428 minutes
2024-09-23 15:41:50,682:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:50,682:INFO:Initializing create_model()
2024-09-23 15:41:50,682:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:50,683:INFO:Checking exceptions
2024-09-23 15:41:50,683:INFO:Importing libraries
2024-09-23 15:41:50,683:INFO:Copying training dataset
2024-09-23 15:41:50,693:INFO:Defining folds
2024-09-23 15:41:50,693:INFO:Declaring metric variables
2024-09-23 15:41:50,693:INFO:Importing untrained model
2024-09-23 15:41:50,702:INFO:Ridge Classifier Imported successfully
2024-09-23 15:41:50,704:INFO:Starting cross validation
2024-09-23 15:41:50,707:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:51,003:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,051:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,052:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,054:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,080:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,083:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,095:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,096:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,122:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,137:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:51,149:INFO:Calculating mean and std
2024-09-23 15:41:51,150:INFO:Creating metrics dataframe
2024-09-23 15:41:51,151:INFO:Uploading results into container
2024-09-23 15:41:51,153:INFO:Uploading model into container now
2024-09-23 15:41:51,153:INFO:_master_model_container: 6
2024-09-23 15:41:51,153:INFO:_display_container: 2
2024-09-23 15:41:51,154:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 15:41:51,154:INFO:create_model() successfully completed......................................
2024-09-23 15:41:51,284:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:51,284:INFO:Creating metrics dataframe
2024-09-23 15:41:51,284:INFO:Initializing Random Forest Classifier
2024-09-23 15:41:51,284:INFO:Total runtime is 0.20327681303024292 minutes
2024-09-23 15:41:51,299:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:51,299:INFO:Initializing create_model()
2024-09-23 15:41:51,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:51,299:INFO:Checking exceptions
2024-09-23 15:41:51,299:INFO:Importing libraries
2024-09-23 15:41:51,299:INFO:Copying training dataset
2024-09-23 15:41:51,307:INFO:Defining folds
2024-09-23 15:41:51,307:INFO:Declaring metric variables
2024-09-23 15:41:51,320:INFO:Importing untrained model
2024-09-23 15:41:51,320:INFO:Random Forest Classifier Imported successfully
2024-09-23 15:41:51,320:INFO:Starting cross validation
2024-09-23 15:41:51,320:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:56,085:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:41:56,136:INFO:Calculating mean and std
2024-09-23 15:41:56,136:INFO:Creating metrics dataframe
2024-09-23 15:41:56,136:INFO:Uploading results into container
2024-09-23 15:41:56,136:INFO:Uploading model into container now
2024-09-23 15:41:56,136:INFO:_master_model_container: 7
2024-09-23 15:41:56,136:INFO:_display_container: 2
2024-09-23 15:41:56,136:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 15:41:56,136:INFO:create_model() successfully completed......................................
2024-09-23 15:41:56,332:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:56,332:INFO:Creating metrics dataframe
2024-09-23 15:41:56,335:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 15:41:56,335:INFO:Total runtime is 0.28747481902440386 minutes
2024-09-23 15:41:56,335:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:56,335:INFO:Initializing create_model()
2024-09-23 15:41:56,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:56,335:INFO:Checking exceptions
2024-09-23 15:41:56,335:INFO:Importing libraries
2024-09-23 15:41:56,335:INFO:Copying training dataset
2024-09-23 15:41:56,351:INFO:Defining folds
2024-09-23 15:41:56,351:INFO:Declaring metric variables
2024-09-23 15:41:56,354:INFO:Importing untrained model
2024-09-23 15:41:56,356:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 15:41:56,357:INFO:Starting cross validation
2024-09-23 15:41:56,357:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:56,610:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,626:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,626:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,642:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,642:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,642:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,657:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,657:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,673:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,689:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,689:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,704:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,720:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,735:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,735:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,751:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,751:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,767:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 15:41:56,782:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,782:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:41:56,798:INFO:Calculating mean and std
2024-09-23 15:41:56,798:INFO:Creating metrics dataframe
2024-09-23 15:41:56,798:INFO:Uploading results into container
2024-09-23 15:41:56,798:INFO:Uploading model into container now
2024-09-23 15:41:56,798:INFO:_master_model_container: 8
2024-09-23 15:41:56,798:INFO:_display_container: 2
2024-09-23 15:41:56,798:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 15:41:56,798:INFO:create_model() successfully completed......................................
2024-09-23 15:41:56,947:INFO:SubProcess create_model() end ==================================
2024-09-23 15:41:56,947:INFO:Creating metrics dataframe
2024-09-23 15:41:56,947:INFO:Initializing Ada Boost Classifier
2024-09-23 15:41:56,947:INFO:Total runtime is 0.2976607282956441 minutes
2024-09-23 15:41:56,947:INFO:SubProcess create_model() called ==================================
2024-09-23 15:41:56,947:INFO:Initializing create_model()
2024-09-23 15:41:56,947:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:41:56,947:INFO:Checking exceptions
2024-09-23 15:41:56,947:INFO:Importing libraries
2024-09-23 15:41:56,947:INFO:Copying training dataset
2024-09-23 15:41:56,970:INFO:Defining folds
2024-09-23 15:41:56,970:INFO:Declaring metric variables
2024-09-23 15:41:56,970:INFO:Importing untrained model
2024-09-23 15:41:56,970:INFO:Ada Boost Classifier Imported successfully
2024-09-23 15:41:56,970:INFO:Starting cross validation
2024-09-23 15:41:56,970:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:41:57,201:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,202:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,233:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,265:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,327:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,358:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,374:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,390:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,390:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:41:57,408:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 15:42:00,307:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,322:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,494:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,670:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,688:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,768:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,785:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,883:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,930:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,986:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:42:00,987:INFO:Calculating mean and std
2024-09-23 15:42:00,987:INFO:Creating metrics dataframe
2024-09-23 15:42:00,987:INFO:Uploading results into container
2024-09-23 15:42:00,987:INFO:Uploading model into container now
2024-09-23 15:42:00,987:INFO:_master_model_container: 9
2024-09-23 15:42:00,987:INFO:_display_container: 2
2024-09-23 15:42:00,987:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 15:42:00,987:INFO:create_model() successfully completed......................................
2024-09-23 15:42:01,144:INFO:SubProcess create_model() end ==================================
2024-09-23 15:42:01,144:INFO:Creating metrics dataframe
2024-09-23 15:42:01,151:INFO:Initializing Gradient Boosting Classifier
2024-09-23 15:42:01,151:INFO:Total runtime is 0.36772984663645425 minutes
2024-09-23 15:42:01,153:INFO:SubProcess create_model() called ==================================
2024-09-23 15:42:01,153:INFO:Initializing create_model()
2024-09-23 15:42:01,153:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:42:01,153:INFO:Checking exceptions
2024-09-23 15:42:01,153:INFO:Importing libraries
2024-09-23 15:42:01,153:INFO:Copying training dataset
2024-09-23 15:42:01,167:INFO:Defining folds
2024-09-23 15:42:01,167:INFO:Declaring metric variables
2024-09-23 15:42:01,170:INFO:Importing untrained model
2024-09-23 15:42:01,171:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:42:01,173:INFO:Starting cross validation
2024-09-23 15:42:01,178:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:44:08,135:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:08,464:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:09,340:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:11,110:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:11,166:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:11,686:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:12,306:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:12,351:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:12,764:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:15,497:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:15,504:INFO:Calculating mean and std
2024-09-23 15:44:15,505:INFO:Creating metrics dataframe
2024-09-23 15:44:15,506:INFO:Uploading results into container
2024-09-23 15:44:15,506:INFO:Uploading model into container now
2024-09-23 15:44:15,507:INFO:_master_model_container: 10
2024-09-23 15:44:15,507:INFO:_display_container: 2
2024-09-23 15:44:15,507:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:44:15,507:INFO:create_model() successfully completed......................................
2024-09-23 15:44:15,642:INFO:SubProcess create_model() end ==================================
2024-09-23 15:44:15,642:INFO:Creating metrics dataframe
2024-09-23 15:44:15,648:INFO:Initializing Linear Discriminant Analysis
2024-09-23 15:44:15,648:INFO:Total runtime is 2.609358282883962 minutes
2024-09-23 15:44:15,651:INFO:SubProcess create_model() called ==================================
2024-09-23 15:44:15,651:INFO:Initializing create_model()
2024-09-23 15:44:15,651:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:44:15,651:INFO:Checking exceptions
2024-09-23 15:44:15,651:INFO:Importing libraries
2024-09-23 15:44:15,651:INFO:Copying training dataset
2024-09-23 15:44:15,664:INFO:Defining folds
2024-09-23 15:44:15,664:INFO:Declaring metric variables
2024-09-23 15:44:15,667:INFO:Importing untrained model
2024-09-23 15:44:15,669:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 15:44:15,671:INFO:Starting cross validation
2024-09-23 15:44:15,671:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:44:16,011:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,052:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,072:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,084:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,084:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,084:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,111:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,151:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,167:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 15:44:16,183:INFO:Calculating mean and std
2024-09-23 15:44:16,183:INFO:Creating metrics dataframe
2024-09-23 15:44:16,183:INFO:Uploading results into container
2024-09-23 15:44:16,183:INFO:Uploading model into container now
2024-09-23 15:44:16,183:INFO:_master_model_container: 11
2024-09-23 15:44:16,183:INFO:_display_container: 2
2024-09-23 15:44:16,183:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 15:44:16,183:INFO:create_model() successfully completed......................................
2024-09-23 15:44:16,298:INFO:SubProcess create_model() end ==================================
2024-09-23 15:44:16,298:INFO:Creating metrics dataframe
2024-09-23 15:44:16,298:INFO:Initializing Extra Trees Classifier
2024-09-23 15:44:16,298:INFO:Total runtime is 2.620186944802602 minutes
2024-09-23 15:44:16,298:INFO:SubProcess create_model() called ==================================
2024-09-23 15:44:16,298:INFO:Initializing create_model()
2024-09-23 15:44:16,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:44:16,298:INFO:Checking exceptions
2024-09-23 15:44:16,298:INFO:Importing libraries
2024-09-23 15:44:16,298:INFO:Copying training dataset
2024-09-23 15:44:16,323:INFO:Defining folds
2024-09-23 15:44:16,323:INFO:Declaring metric variables
2024-09-23 15:44:16,326:INFO:Importing untrained model
2024-09-23 15:44:16,328:INFO:Extra Trees Classifier Imported successfully
2024-09-23 15:44:16,331:INFO:Starting cross validation
2024-09-23 15:44:16,332:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:44:20,579:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:20,579:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:20,623:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:20,685:INFO:Calculating mean and std
2024-09-23 15:44:20,685:INFO:Creating metrics dataframe
2024-09-23 15:44:20,685:INFO:Uploading results into container
2024-09-23 15:44:20,685:INFO:Uploading model into container now
2024-09-23 15:44:20,685:INFO:_master_model_container: 12
2024-09-23 15:44:20,685:INFO:_display_container: 2
2024-09-23 15:44:20,685:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 15:44:20,685:INFO:create_model() successfully completed......................................
2024-09-23 15:44:20,851:INFO:SubProcess create_model() end ==================================
2024-09-23 15:44:20,851:INFO:Creating metrics dataframe
2024-09-23 15:44:20,851:INFO:Initializing Extreme Gradient Boosting
2024-09-23 15:44:20,851:INFO:Total runtime is 2.696060514450073 minutes
2024-09-23 15:44:20,851:INFO:SubProcess create_model() called ==================================
2024-09-23 15:44:20,851:INFO:Initializing create_model()
2024-09-23 15:44:20,851:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:44:20,851:INFO:Checking exceptions
2024-09-23 15:44:20,851:INFO:Importing libraries
2024-09-23 15:44:20,851:INFO:Copying training dataset
2024-09-23 15:44:20,871:INFO:Defining folds
2024-09-23 15:44:20,871:INFO:Declaring metric variables
2024-09-23 15:44:20,871:INFO:Importing untrained model
2024-09-23 15:44:20,871:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 15:44:20,871:INFO:Starting cross validation
2024-09-23 15:44:20,871:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:44:26,432:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:27,400:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:28,057:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:28,129:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:28,167:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:28,323:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:28,323:INFO:Calculating mean and std
2024-09-23 15:44:28,323:INFO:Creating metrics dataframe
2024-09-23 15:44:28,323:INFO:Uploading results into container
2024-09-23 15:44:28,323:INFO:Uploading model into container now
2024-09-23 15:44:28,323:INFO:_master_model_container: 13
2024-09-23 15:44:28,323:INFO:_display_container: 2
2024-09-23 15:44:28,323:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 15:44:28,323:INFO:create_model() successfully completed......................................
2024-09-23 15:44:28,469:INFO:SubProcess create_model() end ==================================
2024-09-23 15:44:28,469:INFO:Creating metrics dataframe
2024-09-23 15:44:28,469:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 15:44:28,469:INFO:Total runtime is 2.8230301380157465 minutes
2024-09-23 15:44:28,469:INFO:SubProcess create_model() called ==================================
2024-09-23 15:44:28,469:INFO:Initializing create_model()
2024-09-23 15:44:28,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:44:28,479:INFO:Checking exceptions
2024-09-23 15:44:28,479:INFO:Importing libraries
2024-09-23 15:44:28,479:INFO:Copying training dataset
2024-09-23 15:44:28,489:INFO:Defining folds
2024-09-23 15:44:28,489:INFO:Declaring metric variables
2024-09-23 15:44:28,489:INFO:Importing untrained model
2024-09-23 15:44:28,489:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 15:44:28,502:INFO:Starting cross validation
2024-09-23 15:44:28,504:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:44:48,796:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:49,031:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:49,165:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:44:49,352:INFO:Calculating mean and std
2024-09-23 15:44:49,352:INFO:Creating metrics dataframe
2024-09-23 15:44:49,352:INFO:Uploading results into container
2024-09-23 15:44:49,352:INFO:Uploading model into container now
2024-09-23 15:44:49,352:INFO:_master_model_container: 14
2024-09-23 15:44:49,352:INFO:_display_container: 2
2024-09-23 15:44:49,352:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 15:44:49,352:INFO:create_model() successfully completed......................................
2024-09-23 15:44:49,565:INFO:SubProcess create_model() end ==================================
2024-09-23 15:44:49,565:INFO:Creating metrics dataframe
2024-09-23 15:44:49,565:INFO:Initializing CatBoost Classifier
2024-09-23 15:44:49,565:INFO:Total runtime is 3.1746273994445797 minutes
2024-09-23 15:44:49,580:INFO:SubProcess create_model() called ==================================
2024-09-23 15:44:49,580:INFO:Initializing create_model()
2024-09-23 15:44:49,580:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:44:49,580:INFO:Checking exceptions
2024-09-23 15:44:49,580:INFO:Importing libraries
2024-09-23 15:44:49,580:INFO:Copying training dataset
2024-09-23 15:44:49,592:INFO:Defining folds
2024-09-23 15:44:49,592:INFO:Declaring metric variables
2024-09-23 15:44:49,592:INFO:Importing untrained model
2024-09-23 15:44:49,592:INFO:CatBoost Classifier Imported successfully
2024-09-23 15:44:49,602:INFO:Starting cross validation
2024-09-23 15:44:49,602:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:46:43,136:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:43,557:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:43,604:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:43,620:INFO:Calculating mean and std
2024-09-23 15:46:43,620:INFO:Creating metrics dataframe
2024-09-23 15:46:43,620:INFO:Uploading results into container
2024-09-23 15:46:43,620:INFO:Uploading model into container now
2024-09-23 15:46:43,620:INFO:_master_model_container: 15
2024-09-23 15:46:43,620:INFO:_display_container: 2
2024-09-23 15:46:43,620:INFO:<catboost.core.CatBoostClassifier object at 0x000002007098BF90>
2024-09-23 15:46:43,620:INFO:create_model() successfully completed......................................
2024-09-23 15:46:43,769:INFO:SubProcess create_model() end ==================================
2024-09-23 15:46:43,769:INFO:Creating metrics dataframe
2024-09-23 15:46:43,769:INFO:Initializing Dummy Classifier
2024-09-23 15:46:43,769:INFO:Total runtime is 5.0780294338862095 minutes
2024-09-23 15:46:43,769:INFO:SubProcess create_model() called ==================================
2024-09-23 15:46:43,769:INFO:Initializing create_model()
2024-09-23 15:46:43,769:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020067E3AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:46:43,769:INFO:Checking exceptions
2024-09-23 15:46:43,769:INFO:Importing libraries
2024-09-23 15:46:43,769:INFO:Copying training dataset
2024-09-23 15:46:43,794:INFO:Defining folds
2024-09-23 15:46:43,794:INFO:Declaring metric variables
2024-09-23 15:46:43,795:INFO:Importing untrained model
2024-09-23 15:46:43,795:INFO:Dummy Classifier Imported successfully
2024-09-23 15:46:43,802:INFO:Starting cross validation
2024-09-23 15:46:43,803:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 15:46:44,182:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,198:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,229:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,229:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,245:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,261:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,276:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 15:46:44,276:INFO:Calculating mean and std
2024-09-23 15:46:44,276:INFO:Creating metrics dataframe
2024-09-23 15:46:44,276:INFO:Uploading results into container
2024-09-23 15:46:44,276:INFO:Uploading model into container now
2024-09-23 15:46:44,276:INFO:_master_model_container: 16
2024-09-23 15:46:44,276:INFO:_display_container: 2
2024-09-23 15:46:44,276:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 15:46:44,276:INFO:create_model() successfully completed......................................
2024-09-23 15:46:44,413:INFO:SubProcess create_model() end ==================================
2024-09-23 15:46:44,413:INFO:Creating metrics dataframe
2024-09-23 15:46:44,428:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 15:46:44,437:INFO:Initializing create_model()
2024-09-23 15:46:44,437:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020076CA6FD0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 15:46:44,437:INFO:Checking exceptions
2024-09-23 15:46:44,437:INFO:Importing libraries
2024-09-23 15:46:44,438:INFO:Copying training dataset
2024-09-23 15:46:44,443:INFO:Defining folds
2024-09-23 15:46:44,443:INFO:Declaring metric variables
2024-09-23 15:46:44,443:INFO:Importing untrained model
2024-09-23 15:46:44,443:INFO:Declaring custom model
2024-09-23 15:46:44,443:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 15:46:44,452:INFO:Cross validation set to False
2024-09-23 15:46:44,452:INFO:Fitting Model
2024-09-23 15:48:19,950:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:48:19,950:INFO:create_model() successfully completed......................................
2024-09-23 15:48:20,104:INFO:_master_model_container: 16
2024-09-23 15:48:20,104:INFO:_display_container: 2
2024-09-23 15:48:20,104:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 15:48:20,104:INFO:compare_models() successfully completed......................................
2024-09-23 16:21:41,759:INFO:PyCaret ClassificationExperiment
2024-09-23 16:21:41,759:INFO:Logging name: clf-default-name
2024-09-23 16:21:41,759:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 16:21:41,759:INFO:version 3.3.1
2024-09-23 16:21:41,759:INFO:Initializing setup()
2024-09-23 16:21:41,759:INFO:self.USI: f240
2024-09-23 16:21:41,760:INFO:self._variable_keys: {'y', '_available_plots', 'gpu_param', 'idx', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'html_param', 'pipeline', 'fold_groups_param', 'y_train', 'logging_param', 'exp_name_log', 'gpu_n_jobs_param', 'X_train', 'memory', 'y_test', 'n_jobs_param', 'fold_generator', 'is_multiclass', 'USI', 'data', 'X', 'X_test', 'target_param', 'fold_shuffle_param', 'exp_id', 'seed'}
2024-09-23 16:21:41,760:INFO:Checking environment
2024-09-23 16:21:41,760:INFO:python_version: 3.11.9
2024-09-23 16:21:41,760:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 16:21:41,760:INFO:machine: AMD64
2024-09-23 16:21:41,760:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 16:21:41,761:INFO:Memory: svmem(total=16853458944, available=6891569152, percent=59.1, used=9961889792, free=6891569152)
2024-09-23 16:21:41,761:INFO:Physical Core: 16
2024-09-23 16:21:41,761:INFO:Logical Core: 24
2024-09-23 16:21:41,761:INFO:Checking libraries
2024-09-23 16:21:41,761:INFO:System:
2024-09-23 16:21:41,761:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 16:21:41,761:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 16:21:41,761:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 16:21:41,761:INFO:PyCaret required dependencies:
2024-09-23 16:21:41,761:INFO:                 pip: 24.2
2024-09-23 16:21:41,761:INFO:          setuptools: 75.1.0
2024-09-23 16:21:41,761:INFO:             pycaret: 3.3.1
2024-09-23 16:21:41,761:INFO:             IPython: 8.27.0
2024-09-23 16:21:41,761:INFO:          ipywidgets: 8.1.2
2024-09-23 16:21:41,761:INFO:                tqdm: 4.66.5
2024-09-23 16:21:41,761:INFO:               numpy: 1.26.4
2024-09-23 16:21:41,761:INFO:              pandas: 2.1.4
2024-09-23 16:21:41,761:INFO:              jinja2: 3.1.4
2024-09-23 16:21:41,761:INFO:               scipy: 1.11.4
2024-09-23 16:21:41,761:INFO:              joblib: 1.2.0
2024-09-23 16:21:41,762:INFO:             sklearn: 1.4.2
2024-09-23 16:21:41,762:INFO:                pyod: 2.0.2
2024-09-23 16:21:41,762:INFO:            imblearn: 0.12.3
2024-09-23 16:21:41,762:INFO:   category_encoders: 2.6.3
2024-09-23 16:21:41,762:INFO:            lightgbm: 4.5.0
2024-09-23 16:21:41,762:INFO:               numba: 0.60.0
2024-09-23 16:21:41,762:INFO:            requests: 2.32.3
2024-09-23 16:21:41,762:INFO:          matplotlib: 3.9.2
2024-09-23 16:21:41,762:INFO:          scikitplot: 0.3.7
2024-09-23 16:21:41,762:INFO:         yellowbrick: 1.5
2024-09-23 16:21:41,762:INFO:              plotly: 5.24.1
2024-09-23 16:21:41,762:INFO:    plotly-resampler: Not installed
2024-09-23 16:21:41,762:INFO:             kaleido: 0.2.1
2024-09-23 16:21:41,762:INFO:           schemdraw: 0.15
2024-09-23 16:21:41,762:INFO:         statsmodels: 0.14.2
2024-09-23 16:21:41,762:INFO:              sktime: 0.26.0
2024-09-23 16:21:41,762:INFO:               tbats: 1.1.3
2024-09-23 16:21:41,762:INFO:            pmdarima: 2.0.4
2024-09-23 16:21:41,762:INFO:              psutil: 5.9.0
2024-09-23 16:21:41,762:INFO:          markupsafe: 2.1.3
2024-09-23 16:21:41,762:INFO:             pickle5: Not installed
2024-09-23 16:21:41,762:INFO:         cloudpickle: 3.0.0
2024-09-23 16:21:41,763:INFO:         deprecation: 2.1.0
2024-09-23 16:21:41,763:INFO:              xxhash: 2.0.2
2024-09-23 16:21:41,763:INFO:           wurlitzer: 3.1.1
2024-09-23 16:21:41,763:INFO:PyCaret optional dependencies:
2024-09-23 16:21:41,763:INFO:                shap: Not installed
2024-09-23 16:21:41,763:INFO:           interpret: Not installed
2024-09-23 16:21:41,763:INFO:                umap: Not installed
2024-09-23 16:21:41,763:INFO:     ydata_profiling: Not installed
2024-09-23 16:21:41,763:INFO:  explainerdashboard: Not installed
2024-09-23 16:21:41,763:INFO:             autoviz: Not installed
2024-09-23 16:21:41,764:INFO:           fairlearn: Not installed
2024-09-23 16:21:41,764:INFO:          deepchecks: Not installed
2024-09-23 16:21:41,764:INFO:             xgboost: 2.1.1
2024-09-23 16:21:41,764:INFO:            catboost: 1.2.7
2024-09-23 16:21:41,764:INFO:              kmodes: Not installed
2024-09-23 16:21:41,764:INFO:             mlxtend: Not installed
2024-09-23 16:21:41,764:INFO:       statsforecast: Not installed
2024-09-23 16:21:41,764:INFO:        tune_sklearn: Not installed
2024-09-23 16:21:41,764:INFO:                 ray: Not installed
2024-09-23 16:21:41,765:INFO:            hyperopt: Not installed
2024-09-23 16:21:41,765:INFO:              optuna: Not installed
2024-09-23 16:21:41,765:INFO:               skopt: Not installed
2024-09-23 16:21:41,765:INFO:              mlflow: Not installed
2024-09-23 16:21:41,765:INFO:              gradio: Not installed
2024-09-23 16:21:41,765:INFO:             fastapi: Not installed
2024-09-23 16:21:41,765:INFO:             uvicorn: Not installed
2024-09-23 16:21:41,765:INFO:              m2cgen: Not installed
2024-09-23 16:21:41,765:INFO:           evidently: Not installed
2024-09-23 16:21:41,765:INFO:               fugue: Not installed
2024-09-23 16:21:41,765:INFO:           streamlit: Not installed
2024-09-23 16:21:41,765:INFO:             prophet: Not installed
2024-09-23 16:21:41,765:INFO:None
2024-09-23 16:21:41,765:INFO:Set up data.
2024-09-23 16:21:41,769:INFO:Set up folding strategy.
2024-09-23 16:21:41,769:INFO:Set up train/test split.
2024-09-23 16:21:41,801:INFO:Set up index.
2024-09-23 16:21:41,801:INFO:Assigning column types.
2024-09-23 16:21:41,818:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 16:21:41,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 16:21:41,853:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:21:41,866:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:41,867:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:41,890:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 16:21:41,890:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:21:41,903:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:41,904:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:41,904:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 16:21:41,917:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:21:41,932:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:41,932:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:41,948:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:21:41,986:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:41,987:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:41,987:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 16:21:42,018:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:42,018:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:42,061:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:42,062:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:42,063:INFO:Preparing preprocessing pipeline...
2024-09-23 16:21:42,065:INFO:Set up simple imputation.
2024-09-23 16:21:42,065:INFO:Set up imbalanced handling.
2024-09-23 16:21:42,065:INFO:Set up feature normalization.
2024-09-23 16:21:42,066:INFO:Set up column name cleaning.
2024-09-23 16:21:42,458:INFO:Finished creating preprocessing pipeline.
2024-09-23 16:21:42,458:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hi...
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=123,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 16:21:42,458:INFO:Creating final display dataframe.
2024-09-23 16:21:42,941:INFO:Setup _display_container:         Description             Value
0        Session id               123
1            Target      Golongan UKT
..              ...               ...
21  Experiment Name  clf-default-name
22              USI              f240

[23 rows x 2 columns]
2024-09-23 16:21:42,998:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:43,000:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:43,034:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:21:43,034:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:21:43,034:INFO:setup() successfully completed in 1.28s...............
2024-09-23 16:21:50,319:INFO:Initializing create_model()
2024-09-23 16:21:50,319:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:21:50,319:INFO:Checking exceptions
2024-09-23 16:21:50,358:INFO:Importing libraries
2024-09-23 16:21:50,358:INFO:Copying training dataset
2024-09-23 16:21:50,384:INFO:Defining folds
2024-09-23 16:21:50,384:INFO:Declaring metric variables
2024-09-23 16:21:50,386:INFO:Importing untrained model
2024-09-23 16:21:50,387:INFO:CatBoost Classifier Imported successfully
2024-09-23 16:21:50,387:INFO:Starting cross validation
2024-09-23 16:21:50,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:23:54,017:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:24:11,502:INFO:Initializing create_model()
2024-09-23 16:24:11,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:24:11,502:INFO:Checking exceptions
2024-09-23 16:24:11,529:INFO:Importing libraries
2024-09-23 16:24:11,529:INFO:Copying training dataset
2024-09-23 16:24:11,542:INFO:Defining folds
2024-09-23 16:24:11,542:INFO:Declaring metric variables
2024-09-23 16:24:11,544:INFO:Importing untrained model
2024-09-23 16:24:11,546:INFO:CatBoost Classifier Imported successfully
2024-09-23 16:24:11,549:INFO:Starting cross validation
2024-09-23 16:24:11,550:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:26:15,609:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:26:15,785:INFO:Calculating mean and std
2024-09-23 16:26:15,785:INFO:Creating metrics dataframe
2024-09-23 16:26:15,785:INFO:Finalizing model
2024-09-23 16:26:33,048:INFO:Uploading results into container
2024-09-23 16:26:33,048:INFO:Uploading model into container now
2024-09-23 16:26:33,057:INFO:_master_model_container: 1
2024-09-23 16:26:33,057:INFO:_display_container: 2
2024-09-23 16:26:33,057:INFO:<catboost.core.CatBoostClassifier object at 0x00000200067E0E90>
2024-09-23 16:26:33,057:INFO:create_model() successfully completed......................................
2024-09-23 16:28:34,386:INFO:PyCaret ClassificationExperiment
2024-09-23 16:28:34,386:INFO:Logging name: clf-default-name
2024-09-23 16:28:34,386:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 16:28:34,386:INFO:version 3.3.1
2024-09-23 16:28:34,386:INFO:Initializing setup()
2024-09-23 16:28:34,386:INFO:self.USI: 9124
2024-09-23 16:28:34,386:INFO:self._variable_keys: {'_ml_usecase', 'y_test', 'X_test', 'is_multiclass', 'target_param', 'USI', 'exp_name_log', 'logging_param', 'idx', 'gpu_n_jobs_param', 'pipeline', 'fold_generator', 'seed', 'html_param', 'data', 'y', 'fold_groups_param', 'X_train', 'log_plots_param', 'fix_imbalance', 'exp_id', 'fold_shuffle_param', 'memory', 'n_jobs_param', 'y_train', '_available_plots', 'X', 'gpu_param'}
2024-09-23 16:28:34,386:INFO:Checking environment
2024-09-23 16:28:34,386:INFO:python_version: 3.11.9
2024-09-23 16:28:34,386:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 16:28:34,386:INFO:machine: AMD64
2024-09-23 16:28:34,386:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 16:28:34,386:INFO:Memory: svmem(total=16853458944, available=4990005248, percent=70.4, used=11863453696, free=4990005248)
2024-09-23 16:28:34,386:INFO:Physical Core: 16
2024-09-23 16:28:34,386:INFO:Logical Core: 24
2024-09-23 16:28:34,386:INFO:Checking libraries
2024-09-23 16:28:34,386:INFO:System:
2024-09-23 16:28:34,386:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 16:28:34,386:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 16:28:34,386:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 16:28:34,386:INFO:PyCaret required dependencies:
2024-09-23 16:28:34,386:INFO:                 pip: 24.2
2024-09-23 16:28:34,386:INFO:          setuptools: 75.1.0
2024-09-23 16:28:34,386:INFO:             pycaret: 3.3.1
2024-09-23 16:28:34,386:INFO:             IPython: 8.27.0
2024-09-23 16:28:34,386:INFO:          ipywidgets: 8.1.2
2024-09-23 16:28:34,386:INFO:                tqdm: 4.66.5
2024-09-23 16:28:34,386:INFO:               numpy: 1.26.4
2024-09-23 16:28:34,386:INFO:              pandas: 2.1.4
2024-09-23 16:28:34,386:INFO:              jinja2: 3.1.4
2024-09-23 16:28:34,386:INFO:               scipy: 1.11.4
2024-09-23 16:28:34,386:INFO:              joblib: 1.2.0
2024-09-23 16:28:34,386:INFO:             sklearn: 1.4.2
2024-09-23 16:28:34,386:INFO:                pyod: 2.0.2
2024-09-23 16:28:34,386:INFO:            imblearn: 0.12.3
2024-09-23 16:28:34,386:INFO:   category_encoders: 2.6.3
2024-09-23 16:28:34,386:INFO:            lightgbm: 4.5.0
2024-09-23 16:28:34,386:INFO:               numba: 0.60.0
2024-09-23 16:28:34,386:INFO:            requests: 2.32.3
2024-09-23 16:28:34,386:INFO:          matplotlib: 3.9.2
2024-09-23 16:28:34,386:INFO:          scikitplot: 0.3.7
2024-09-23 16:28:34,386:INFO:         yellowbrick: 1.5
2024-09-23 16:28:34,386:INFO:              plotly: 5.24.1
2024-09-23 16:28:34,386:INFO:    plotly-resampler: Not installed
2024-09-23 16:28:34,386:INFO:             kaleido: 0.2.1
2024-09-23 16:28:34,386:INFO:           schemdraw: 0.15
2024-09-23 16:28:34,386:INFO:         statsmodels: 0.14.2
2024-09-23 16:28:34,386:INFO:              sktime: 0.26.0
2024-09-23 16:28:34,386:INFO:               tbats: 1.1.3
2024-09-23 16:28:34,386:INFO:            pmdarima: 2.0.4
2024-09-23 16:28:34,386:INFO:              psutil: 5.9.0
2024-09-23 16:28:34,386:INFO:          markupsafe: 2.1.3
2024-09-23 16:28:34,386:INFO:             pickle5: Not installed
2024-09-23 16:28:34,386:INFO:         cloudpickle: 3.0.0
2024-09-23 16:28:34,386:INFO:         deprecation: 2.1.0
2024-09-23 16:28:34,386:INFO:              xxhash: 2.0.2
2024-09-23 16:28:34,386:INFO:           wurlitzer: 3.1.1
2024-09-23 16:28:34,386:INFO:PyCaret optional dependencies:
2024-09-23 16:28:34,386:INFO:                shap: Not installed
2024-09-23 16:28:34,386:INFO:           interpret: Not installed
2024-09-23 16:28:34,386:INFO:                umap: Not installed
2024-09-23 16:28:34,386:INFO:     ydata_profiling: Not installed
2024-09-23 16:28:34,386:INFO:  explainerdashboard: Not installed
2024-09-23 16:28:34,386:INFO:             autoviz: Not installed
2024-09-23 16:28:34,386:INFO:           fairlearn: Not installed
2024-09-23 16:28:34,386:INFO:          deepchecks: Not installed
2024-09-23 16:28:34,386:INFO:             xgboost: 2.1.1
2024-09-23 16:28:34,386:INFO:            catboost: 1.2.7
2024-09-23 16:28:34,386:INFO:              kmodes: Not installed
2024-09-23 16:28:34,386:INFO:             mlxtend: Not installed
2024-09-23 16:28:34,386:INFO:       statsforecast: Not installed
2024-09-23 16:28:34,386:INFO:        tune_sklearn: Not installed
2024-09-23 16:28:34,386:INFO:                 ray: Not installed
2024-09-23 16:28:34,386:INFO:            hyperopt: Not installed
2024-09-23 16:28:34,386:INFO:              optuna: Not installed
2024-09-23 16:28:34,386:INFO:               skopt: Not installed
2024-09-23 16:28:34,386:INFO:              mlflow: Not installed
2024-09-23 16:28:34,386:INFO:              gradio: Not installed
2024-09-23 16:28:34,386:INFO:             fastapi: Not installed
2024-09-23 16:28:34,386:INFO:             uvicorn: Not installed
2024-09-23 16:28:34,386:INFO:              m2cgen: Not installed
2024-09-23 16:28:34,386:INFO:           evidently: Not installed
2024-09-23 16:28:34,386:INFO:               fugue: Not installed
2024-09-23 16:28:34,386:INFO:           streamlit: Not installed
2024-09-23 16:28:34,386:INFO:             prophet: Not installed
2024-09-23 16:28:34,386:INFO:None
2024-09-23 16:28:34,386:INFO:Set up data.
2024-09-23 16:28:34,434:INFO:Set up folding strategy.
2024-09-23 16:28:34,434:INFO:Set up train/test split.
2024-09-23 16:28:34,434:INFO:Set up data.
2024-09-23 16:33:26,300:INFO:Initializing tune_model()
2024-09-23 16:33:26,301:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x00000200067E0E90>, fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-23 16:33:26,301:INFO:Checking exceptions
2024-09-23 16:33:26,334:INFO:Copying training dataset
2024-09-23 16:33:26,353:INFO:Checking base model
2024-09-23 16:33:26,353:INFO:Base model : CatBoost Classifier
2024-09-23 16:33:26,354:INFO:Declaring metric variables
2024-09-23 16:33:26,354:INFO:Defining Hyperparameters
2024-09-23 16:33:26,568:INFO:Tuning with n_jobs=-1
2024-09-23 16:33:26,568:INFO:Initializing RandomizedSearchCV
2024-09-23 16:36:14,008:INFO:best_params: {'actual_estimator__random_strength': 0.7, 'actual_estimator__n_estimators': 250, 'actual_estimator__l2_leaf_reg': 50, 'actual_estimator__eta': 0.15, 'actual_estimator__depth': 3}
2024-09-23 16:36:14,008:INFO:Hyperparameter search completed
2024-09-23 16:36:14,008:INFO:SubProcess create_model() called ==================================
2024-09-23 16:36:14,008:INFO:Initializing create_model()
2024-09-23 16:36:14,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020076D3A1D0>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000200061AC110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'random_strength': 0.7, 'n_estimators': 250, 'l2_leaf_reg': 50, 'eta': 0.15, 'depth': 3})
2024-09-23 16:36:14,008:INFO:Checking exceptions
2024-09-23 16:36:14,008:INFO:Importing libraries
2024-09-23 16:36:14,008:INFO:Copying training dataset
2024-09-23 16:36:14,033:INFO:Defining folds
2024-09-23 16:36:14,033:INFO:Declaring metric variables
2024-09-23 16:36:14,036:INFO:Importing untrained model
2024-09-23 16:36:14,036:INFO:Declaring custom model
2024-09-23 16:36:14,036:INFO:CatBoost Classifier Imported successfully
2024-09-23 16:36:14,036:INFO:Starting cross validation
2024-09-23 16:36:14,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:36:23,790:INFO:Calculating mean and std
2024-09-23 16:36:23,790:INFO:Creating metrics dataframe
2024-09-23 16:36:23,805:INFO:Finalizing model
2024-09-23 16:36:25,666:INFO:Uploading results into container
2024-09-23 16:36:25,666:INFO:Uploading model into container now
2024-09-23 16:36:25,666:INFO:_master_model_container: 2
2024-09-23 16:36:25,666:INFO:_display_container: 3
2024-09-23 16:36:25,666:INFO:<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>
2024-09-23 16:36:25,666:INFO:create_model() successfully completed......................................
2024-09-23 16:36:25,889:INFO:SubProcess create_model() end ==================================
2024-09-23 16:36:25,889:INFO:choose_better activated
2024-09-23 16:36:25,905:INFO:SubProcess create_model() called ==================================
2024-09-23 16:36:25,905:INFO:Initializing create_model()
2024-09-23 16:36:25,905:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x00000200067E0E90>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:36:25,905:INFO:Checking exceptions
2024-09-23 16:36:25,905:INFO:Importing libraries
2024-09-23 16:36:25,905:INFO:Copying training dataset
2024-09-23 16:36:25,932:INFO:Defining folds
2024-09-23 16:36:25,932:INFO:Declaring metric variables
2024-09-23 16:36:25,932:INFO:Importing untrained model
2024-09-23 16:36:25,932:INFO:Declaring custom model
2024-09-23 16:36:25,933:INFO:CatBoost Classifier Imported successfully
2024-09-23 16:36:25,934:INFO:Starting cross validation
2024-09-23 16:36:25,934:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:38:28,429:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:38:28,445:INFO:Calculating mean and std
2024-09-23 16:38:28,445:INFO:Creating metrics dataframe
2024-09-23 16:38:28,445:INFO:Finalizing model
2024-09-23 16:38:45,149:INFO:Uploading results into container
2024-09-23 16:38:45,149:INFO:Uploading model into container now
2024-09-23 16:38:45,149:INFO:_master_model_container: 3
2024-09-23 16:38:45,149:INFO:_display_container: 4
2024-09-23 16:38:45,149:INFO:<catboost.core.CatBoostClassifier object at 0x0000020076E73ED0>
2024-09-23 16:38:45,149:INFO:create_model() successfully completed......................................
2024-09-23 16:38:45,382:INFO:SubProcess create_model() end ==================================
2024-09-23 16:38:45,382:INFO:<catboost.core.CatBoostClassifier object at 0x0000020076E73ED0> result for Accuracy is 0.5421
2024-09-23 16:38:45,382:INFO:<catboost.core.CatBoostClassifier object at 0x0000020009443FD0> result for Accuracy is 0.5443
2024-09-23 16:38:45,382:INFO:<catboost.core.CatBoostClassifier object at 0x0000020009443FD0> is best model
2024-09-23 16:38:45,382:INFO:choose_better completed
2024-09-23 16:38:45,396:INFO:_master_model_container: 3
2024-09-23 16:38:45,396:INFO:_display_container: 3
2024-09-23 16:38:45,396:INFO:<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>
2024-09-23 16:38:45,396:INFO:tune_model() successfully completed......................................
2024-09-23 16:39:12,572:INFO:Initializing evaluate_model()
2024-09-23 16:39:12,572:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 16:39:12,597:INFO:Initializing plot_model()
2024-09-23 16:39:12,597:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 16:39:12,597:INFO:Checking exceptions
2024-09-23 16:39:12,605:INFO:Preloading libraries
2024-09-23 16:39:12,605:INFO:Copying training dataset
2024-09-23 16:39:12,605:INFO:Plot type: pipeline
2024-09-23 16:39:12,717:INFO:Visual Rendered Successfully
2024-09-23 16:39:12,895:INFO:plot_model() successfully completed......................................
2024-09-23 16:39:16,770:INFO:Initializing plot_model()
2024-09-23 16:39:16,783:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, plot=class_report, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 16:39:16,783:INFO:Checking exceptions
2024-09-23 16:39:16,785:INFO:Preloading libraries
2024-09-23 16:39:16,800:INFO:Copying training dataset
2024-09-23 16:39:16,800:INFO:Plot type: class_report
2024-09-23 16:39:16,915:INFO:Fitting Model
2024-09-23 16:39:16,915:INFO:Scoring test/hold-out set
2024-09-23 16:39:17,053:INFO:Visual Rendered Successfully
2024-09-23 16:39:17,239:INFO:plot_model() successfully completed......................................
2024-09-23 16:39:52,067:INFO:Initializing plot_model()
2024-09-23 16:39:52,067:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 16:39:52,067:INFO:Checking exceptions
2024-09-23 16:39:55,588:INFO:Initializing plot_model()
2024-09-23 16:39:55,588:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 16:39:55,588:INFO:Checking exceptions
2024-09-23 16:39:55,604:INFO:Preloading libraries
2024-09-23 16:39:55,604:INFO:Copying training dataset
2024-09-23 16:39:55,604:INFO:Plot type: feature
2024-09-23 16:39:55,604:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 16:39:55,717:INFO:Visual Rendered Successfully
2024-09-23 16:39:55,897:INFO:plot_model() successfully completed......................................
2024-09-23 16:39:57,693:INFO:Initializing plot_model()
2024-09-23 16:39:57,694:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 16:39:57,694:INFO:Checking exceptions
2024-09-23 16:39:57,701:INFO:Preloading libraries
2024-09-23 16:39:57,701:INFO:Copying training dataset
2024-09-23 16:39:57,701:INFO:Plot type: feature_all
2024-09-23 16:39:57,741:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 16:39:57,892:INFO:Visual Rendered Successfully
2024-09-23 16:39:58,076:INFO:plot_model() successfully completed......................................
2024-09-23 16:39:59,300:INFO:Initializing plot_model()
2024-09-23 16:39:59,300:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002000939F050>, estimator=<catboost.core.CatBoostClassifier object at 0x0000020009443FD0>, plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 16:39:59,300:INFO:Checking exceptions
2024-09-23 16:39:59,321:INFO:Preloading libraries
2024-09-23 16:39:59,321:INFO:Copying training dataset
2024-09-23 16:39:59,321:INFO:Plot type: feature
2024-09-23 16:39:59,321:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 16:39:59,429:INFO:Visual Rendered Successfully
2024-09-23 16:39:59,630:INFO:plot_model() successfully completed......................................
2024-09-23 16:52:12,632:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 16:52:12,632:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 16:52:12,632:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 16:52:12,632:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-23 16:52:38,032:INFO:PyCaret ClassificationExperiment
2024-09-23 16:52:38,033:INFO:Logging name: clf-default-name
2024-09-23 16:52:38,033:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-23 16:52:38,033:INFO:version 3.3.1
2024-09-23 16:52:38,033:INFO:Initializing setup()
2024-09-23 16:52:38,033:INFO:self.USI: ca4f
2024-09-23 16:52:38,033:INFO:self._variable_keys: {'n_jobs_param', 'pipeline', 'fold_groups_param', 'html_param', 'exp_name_log', 'X_train', 'fix_imbalance', 'gpu_param', 'target_param', 'y_test', '_ml_usecase', 'fold_generator', 'data', 'logging_param', 'seed', 'y', 'y_train', 'fold_shuffle_param', 'idx', 'X', 'memory', '_available_plots', 'USI', 'is_multiclass', 'exp_id', 'gpu_n_jobs_param', 'log_plots_param', 'X_test'}
2024-09-23 16:52:38,033:INFO:Checking environment
2024-09-23 16:52:38,033:INFO:python_version: 3.11.9
2024-09-23 16:52:38,033:INFO:python_build: ('main', 'Apr 19 2024 16:40:41')
2024-09-23 16:52:38,033:INFO:machine: AMD64
2024-09-23 16:52:38,033:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-23 16:52:38,033:INFO:Memory: svmem(total=16853458944, available=6981881856, percent=58.6, used=9871577088, free=6981881856)
2024-09-23 16:52:38,033:INFO:Physical Core: 16
2024-09-23 16:52:38,034:INFO:Logical Core: 24
2024-09-23 16:52:38,034:INFO:Checking libraries
2024-09-23 16:52:38,034:INFO:System:
2024-09-23 16:52:38,034:INFO:    python: 3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]
2024-09-23 16:52:38,034:INFO:executable: C:\Users\ICT\anaconda3\envs\my_env\python.exe
2024-09-23 16:52:38,034:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-23 16:52:38,034:INFO:PyCaret required dependencies:
2024-09-23 16:52:38,072:INFO:                 pip: 24.2
2024-09-23 16:52:38,072:INFO:          setuptools: 75.1.0
2024-09-23 16:52:38,072:INFO:             pycaret: 3.3.1
2024-09-23 16:52:38,072:INFO:             IPython: 8.27.0
2024-09-23 16:52:38,072:INFO:          ipywidgets: 8.1.2
2024-09-23 16:52:38,072:INFO:                tqdm: 4.66.5
2024-09-23 16:52:38,072:INFO:               numpy: 1.26.4
2024-09-23 16:52:38,072:INFO:              pandas: 2.1.4
2024-09-23 16:52:38,072:INFO:              jinja2: 3.1.4
2024-09-23 16:52:38,072:INFO:               scipy: 1.11.4
2024-09-23 16:52:38,072:INFO:              joblib: 1.2.0
2024-09-23 16:52:38,072:INFO:             sklearn: 1.4.2
2024-09-23 16:52:38,072:INFO:                pyod: 2.0.2
2024-09-23 16:52:38,072:INFO:            imblearn: 0.12.3
2024-09-23 16:52:38,072:INFO:   category_encoders: 2.6.3
2024-09-23 16:52:38,072:INFO:            lightgbm: 4.5.0
2024-09-23 16:52:38,072:INFO:               numba: 0.60.0
2024-09-23 16:52:38,072:INFO:            requests: 2.32.3
2024-09-23 16:52:38,072:INFO:          matplotlib: 3.9.2
2024-09-23 16:52:38,072:INFO:          scikitplot: 0.3.7
2024-09-23 16:52:38,072:INFO:         yellowbrick: 1.5
2024-09-23 16:52:38,072:INFO:              plotly: 5.24.1
2024-09-23 16:52:38,072:INFO:    plotly-resampler: Not installed
2024-09-23 16:52:38,072:INFO:             kaleido: 0.2.1
2024-09-23 16:52:38,072:INFO:           schemdraw: 0.15
2024-09-23 16:52:38,072:INFO:         statsmodels: 0.14.2
2024-09-23 16:52:38,072:INFO:              sktime: 0.26.0
2024-09-23 16:52:38,072:INFO:               tbats: 1.1.3
2024-09-23 16:52:38,072:INFO:            pmdarima: 2.0.4
2024-09-23 16:52:38,072:INFO:              psutil: 5.9.0
2024-09-23 16:52:38,072:INFO:          markupsafe: 2.1.3
2024-09-23 16:52:38,072:INFO:             pickle5: Not installed
2024-09-23 16:52:38,072:INFO:         cloudpickle: 3.0.0
2024-09-23 16:52:38,072:INFO:         deprecation: 2.1.0
2024-09-23 16:52:38,072:INFO:              xxhash: 2.0.2
2024-09-23 16:52:38,072:INFO:           wurlitzer: 3.1.1
2024-09-23 16:52:38,072:INFO:PyCaret optional dependencies:
2024-09-23 16:52:38,108:INFO:                shap: Not installed
2024-09-23 16:52:38,108:INFO:           interpret: Not installed
2024-09-23 16:52:38,108:INFO:                umap: Not installed
2024-09-23 16:52:38,108:INFO:     ydata_profiling: Not installed
2024-09-23 16:52:38,108:INFO:  explainerdashboard: Not installed
2024-09-23 16:52:38,108:INFO:             autoviz: Not installed
2024-09-23 16:52:38,108:INFO:           fairlearn: Not installed
2024-09-23 16:52:38,108:INFO:          deepchecks: Not installed
2024-09-23 16:52:38,108:INFO:             xgboost: 2.1.1
2024-09-23 16:52:38,108:INFO:            catboost: 1.2.7
2024-09-23 16:52:38,108:INFO:              kmodes: Not installed
2024-09-23 16:52:38,108:INFO:             mlxtend: Not installed
2024-09-23 16:52:38,108:INFO:       statsforecast: Not installed
2024-09-23 16:52:38,108:INFO:        tune_sklearn: Not installed
2024-09-23 16:52:38,108:INFO:                 ray: Not installed
2024-09-23 16:52:38,108:INFO:            hyperopt: Not installed
2024-09-23 16:52:38,108:INFO:              optuna: Not installed
2024-09-23 16:52:38,108:INFO:               skopt: Not installed
2024-09-23 16:52:38,108:INFO:              mlflow: Not installed
2024-09-23 16:52:38,108:INFO:              gradio: Not installed
2024-09-23 16:52:38,108:INFO:             fastapi: Not installed
2024-09-23 16:52:38,108:INFO:             uvicorn: Not installed
2024-09-23 16:52:38,108:INFO:              m2cgen: Not installed
2024-09-23 16:52:38,108:INFO:           evidently: Not installed
2024-09-23 16:52:38,108:INFO:               fugue: Not installed
2024-09-23 16:52:38,108:INFO:           streamlit: Not installed
2024-09-23 16:52:38,108:INFO:             prophet: Not installed
2024-09-23 16:52:38,108:INFO:None
2024-09-23 16:52:38,108:INFO:Set up data.
2024-09-23 16:52:38,120:INFO:Set up folding strategy.
2024-09-23 16:52:38,120:INFO:Set up train/test split.
2024-09-23 16:52:38,133:INFO:Set up index.
2024-09-23 16:52:38,133:INFO:Assigning column types.
2024-09-23 16:52:38,133:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-23 16:52:38,170:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 16:52:38,174:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:52:38,198:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:38,198:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:38,300:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-23 16:52:38,300:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:52:38,315:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:38,315:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:38,315:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-23 16:52:38,378:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:52:38,394:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:38,394:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:38,428:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-23 16:52:38,432:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:38,432:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:38,432:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-23 16:52:38,465:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:38,480:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:38,512:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:38,512:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:38,512:INFO:Preparing preprocessing pipeline...
2024-09-23 16:52:38,527:INFO:Set up simple imputation.
2024-09-23 16:52:38,527:INFO:Set up imbalanced handling.
2024-09-23 16:52:38,527:INFO:Set up feature normalization.
2024-09-23 16:52:38,528:INFO:Set up column name cleaning.
2024-09-23 16:52:39,070:INFO:Finished creating preprocessing pipeline.
2024-09-23 16:52:39,070:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ICT\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Jumlah Tanggungan dalam Keluarga',
                                             'Pekerjaan Orang Tua/Wali',
                                             'Pendidikan Orang Tua/Wali',
                                             'Kepemilikan Aset',
                                             'Lokasi Tempat Tinggal',
                                             'Pengeluaran Bulanan Keluarga',
                                             'Riwayat Beasiswa atau Bantuan '
                                             'Pendidikan',
                                             'Ayah Hidup?'...
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              n_jobs=None,
                                                                              random_state=123,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-23 16:52:39,070:INFO:Creating final display dataframe.
2024-09-23 16:52:39,607:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target      Golongan UKT
2                   Target type        Multiclass
3           Original data shape       (30380, 25)
4        Transformed data shape       (46170, 25)
5   Transformed train set shape       (37056, 25)
6    Transformed test set shape        (9114, 25)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             SMOTE
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              ca4f
2024-09-23 16:52:39,654:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:39,655:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:39,683:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-23 16:52:39,683:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-23 16:52:39,683:INFO:setup() successfully completed in 1.66s...............
2024-09-23 16:52:48,902:INFO:Initializing compare_models()
2024-09-23 16:52:48,903:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-23 16:52:48,903:INFO:Checking exceptions
2024-09-23 16:52:48,924:INFO:Preparing display monitor
2024-09-23 16:52:48,934:INFO:Initializing Logistic Regression
2024-09-23 16:52:48,934:INFO:Total runtime is 0.0 minutes
2024-09-23 16:52:48,934:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:48,934:INFO:Initializing create_model()
2024-09-23 16:52:48,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:48,934:INFO:Checking exceptions
2024-09-23 16:52:48,934:INFO:Importing libraries
2024-09-23 16:52:48,934:INFO:Copying training dataset
2024-09-23 16:52:48,966:INFO:Defining folds
2024-09-23 16:52:48,966:INFO:Declaring metric variables
2024-09-23 16:52:48,966:INFO:Importing untrained model
2024-09-23 16:52:48,966:INFO:Logistic Regression Imported successfully
2024-09-23 16:52:48,966:INFO:Starting cross validation
2024-09-23 16:52:48,966:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:52:52,108:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,186:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,233:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,280:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,280:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,311:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,343:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,343:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,358:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,421:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:52,436:INFO:Calculating mean and std
2024-09-23 16:52:52,436:INFO:Creating metrics dataframe
2024-09-23 16:52:52,436:INFO:Uploading results into container
2024-09-23 16:52:52,436:INFO:Uploading model into container now
2024-09-23 16:52:52,436:INFO:_master_model_container: 1
2024-09-23 16:52:52,436:INFO:_display_container: 2
2024-09-23 16:52:52,436:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-23 16:52:52,436:INFO:create_model() successfully completed......................................
2024-09-23 16:52:52,494:INFO:SubProcess create_model() end ==================================
2024-09-23 16:52:52,494:INFO:Creating metrics dataframe
2024-09-23 16:52:52,498:INFO:Initializing K Neighbors Classifier
2024-09-23 16:52:52,498:INFO:Total runtime is 0.05939671595891317 minutes
2024-09-23 16:52:52,500:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:52,500:INFO:Initializing create_model()
2024-09-23 16:52:52,500:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:52,500:INFO:Checking exceptions
2024-09-23 16:52:52,500:INFO:Importing libraries
2024-09-23 16:52:52,500:INFO:Copying training dataset
2024-09-23 16:52:52,508:INFO:Defining folds
2024-09-23 16:52:52,508:INFO:Declaring metric variables
2024-09-23 16:52:52,508:INFO:Importing untrained model
2024-09-23 16:52:52,516:INFO:K Neighbors Classifier Imported successfully
2024-09-23 16:52:52,518:INFO:Starting cross validation
2024-09-23 16:52:52,518:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:52:55,060:INFO:Calculating mean and std
2024-09-23 16:52:55,060:INFO:Creating metrics dataframe
2024-09-23 16:52:55,060:INFO:Uploading results into container
2024-09-23 16:52:55,060:INFO:Uploading model into container now
2024-09-23 16:52:55,060:INFO:_master_model_container: 2
2024-09-23 16:52:55,060:INFO:_display_container: 2
2024-09-23 16:52:55,060:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-23 16:52:55,060:INFO:create_model() successfully completed......................................
2024-09-23 16:52:55,122:INFO:SubProcess create_model() end ==================================
2024-09-23 16:52:55,123:INFO:Creating metrics dataframe
2024-09-23 16:52:55,127:INFO:Initializing Naive Bayes
2024-09-23 16:52:55,127:INFO:Total runtime is 0.10321704546610513 minutes
2024-09-23 16:52:55,129:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:55,129:INFO:Initializing create_model()
2024-09-23 16:52:55,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:55,129:INFO:Checking exceptions
2024-09-23 16:52:55,129:INFO:Importing libraries
2024-09-23 16:52:55,129:INFO:Copying training dataset
2024-09-23 16:52:55,140:INFO:Defining folds
2024-09-23 16:52:55,140:INFO:Declaring metric variables
2024-09-23 16:52:55,140:INFO:Importing untrained model
2024-09-23 16:52:55,150:INFO:Naive Bayes Imported successfully
2024-09-23 16:52:55,152:INFO:Starting cross validation
2024-09-23 16:52:55,155:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:52:55,483:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:52:55,483:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:52:56,819:INFO:Calculating mean and std
2024-09-23 16:52:56,819:INFO:Creating metrics dataframe
2024-09-23 16:52:56,819:INFO:Uploading results into container
2024-09-23 16:52:56,819:INFO:Uploading model into container now
2024-09-23 16:52:56,819:INFO:_master_model_container: 3
2024-09-23 16:52:56,819:INFO:_display_container: 2
2024-09-23 16:52:56,834:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-23 16:52:56,834:INFO:create_model() successfully completed......................................
2024-09-23 16:52:56,883:INFO:SubProcess create_model() end ==================================
2024-09-23 16:52:56,883:INFO:Creating metrics dataframe
2024-09-23 16:52:56,899:INFO:Initializing Decision Tree Classifier
2024-09-23 16:52:56,899:INFO:Total runtime is 0.13274452686309812 minutes
2024-09-23 16:52:56,899:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:56,899:INFO:Initializing create_model()
2024-09-23 16:52:56,899:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:56,899:INFO:Checking exceptions
2024-09-23 16:52:56,899:INFO:Importing libraries
2024-09-23 16:52:56,899:INFO:Copying training dataset
2024-09-23 16:52:56,933:INFO:Defining folds
2024-09-23 16:52:56,933:INFO:Declaring metric variables
2024-09-23 16:52:56,935:INFO:Importing untrained model
2024-09-23 16:52:56,937:INFO:Decision Tree Classifier Imported successfully
2024-09-23 16:52:56,938:INFO:Starting cross validation
2024-09-23 16:52:56,938:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:52:57,751:INFO:Calculating mean and std
2024-09-23 16:52:57,751:INFO:Creating metrics dataframe
2024-09-23 16:52:57,751:INFO:Uploading results into container
2024-09-23 16:52:57,751:INFO:Uploading model into container now
2024-09-23 16:52:57,751:INFO:_master_model_container: 4
2024-09-23 16:52:57,751:INFO:_display_container: 2
2024-09-23 16:52:57,751:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-23 16:52:57,751:INFO:create_model() successfully completed......................................
2024-09-23 16:52:57,799:INFO:SubProcess create_model() end ==================================
2024-09-23 16:52:57,799:INFO:Creating metrics dataframe
2024-09-23 16:52:57,799:INFO:Initializing SVM - Linear Kernel
2024-09-23 16:52:57,799:INFO:Total runtime is 0.14775415658950802 minutes
2024-09-23 16:52:57,799:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:57,799:INFO:Initializing create_model()
2024-09-23 16:52:57,799:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:57,799:INFO:Checking exceptions
2024-09-23 16:52:57,799:INFO:Importing libraries
2024-09-23 16:52:57,799:INFO:Copying training dataset
2024-09-23 16:52:57,820:INFO:Defining folds
2024-09-23 16:52:57,820:INFO:Declaring metric variables
2024-09-23 16:52:57,820:INFO:Importing untrained model
2024-09-23 16:52:57,820:INFO:SVM - Linear Kernel Imported successfully
2024-09-23 16:52:57,833:INFO:Starting cross validation
2024-09-23 16:52:57,833:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:52:58,531:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,531:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,594:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,594:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,626:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,626:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,626:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,626:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,704:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,720:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:58,720:INFO:Calculating mean and std
2024-09-23 16:52:58,720:INFO:Creating metrics dataframe
2024-09-23 16:52:58,720:INFO:Uploading results into container
2024-09-23 16:52:58,720:INFO:Uploading model into container now
2024-09-23 16:52:58,720:INFO:_master_model_container: 5
2024-09-23 16:52:58,735:INFO:_display_container: 2
2024-09-23 16:52:58,735:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-23 16:52:58,735:INFO:create_model() successfully completed......................................
2024-09-23 16:52:58,783:INFO:SubProcess create_model() end ==================================
2024-09-23 16:52:58,783:INFO:Creating metrics dataframe
2024-09-23 16:52:58,783:INFO:Initializing Ridge Classifier
2024-09-23 16:52:58,783:INFO:Total runtime is 0.16415713230768836 minutes
2024-09-23 16:52:58,783:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:58,783:INFO:Initializing create_model()
2024-09-23 16:52:58,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:58,799:INFO:Checking exceptions
2024-09-23 16:52:58,799:INFO:Importing libraries
2024-09-23 16:52:58,799:INFO:Copying training dataset
2024-09-23 16:52:58,816:INFO:Defining folds
2024-09-23 16:52:58,816:INFO:Declaring metric variables
2024-09-23 16:52:58,818:INFO:Importing untrained model
2024-09-23 16:52:58,818:INFO:Ridge Classifier Imported successfully
2024-09-23 16:52:58,818:INFO:Starting cross validation
2024-09-23 16:52:58,818:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:52:59,076:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,091:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,138:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,185:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,201:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,217:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,232:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,232:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,248:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,248:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:52:59,263:INFO:Calculating mean and std
2024-09-23 16:52:59,263:INFO:Creating metrics dataframe
2024-09-23 16:52:59,263:INFO:Uploading results into container
2024-09-23 16:52:59,263:INFO:Uploading model into container now
2024-09-23 16:52:59,263:INFO:_master_model_container: 6
2024-09-23 16:52:59,263:INFO:_display_container: 2
2024-09-23 16:52:59,263:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-23 16:52:59,263:INFO:create_model() successfully completed......................................
2024-09-23 16:52:59,299:INFO:SubProcess create_model() end ==================================
2024-09-23 16:52:59,299:INFO:Creating metrics dataframe
2024-09-23 16:52:59,314:INFO:Initializing Random Forest Classifier
2024-09-23 16:52:59,314:INFO:Total runtime is 0.17300471862157182 minutes
2024-09-23 16:52:59,314:INFO:SubProcess create_model() called ==================================
2024-09-23 16:52:59,314:INFO:Initializing create_model()
2024-09-23 16:52:59,314:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:52:59,314:INFO:Checking exceptions
2024-09-23 16:52:59,314:INFO:Importing libraries
2024-09-23 16:52:59,314:INFO:Copying training dataset
2024-09-23 16:52:59,334:INFO:Defining folds
2024-09-23 16:52:59,334:INFO:Declaring metric variables
2024-09-23 16:52:59,334:INFO:Importing untrained model
2024-09-23 16:52:59,334:INFO:Random Forest Classifier Imported successfully
2024-09-23 16:52:59,351:INFO:Starting cross validation
2024-09-23 16:52:59,351:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:53:02,758:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:02,773:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:02,805:INFO:Calculating mean and std
2024-09-23 16:53:02,805:INFO:Creating metrics dataframe
2024-09-23 16:53:02,805:INFO:Uploading results into container
2024-09-23 16:53:02,805:INFO:Uploading model into container now
2024-09-23 16:53:02,805:INFO:_master_model_container: 7
2024-09-23 16:53:02,805:INFO:_display_container: 2
2024-09-23 16:53:02,805:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-23 16:53:02,805:INFO:create_model() successfully completed......................................
2024-09-23 16:53:02,916:INFO:SubProcess create_model() end ==================================
2024-09-23 16:53:02,916:INFO:Creating metrics dataframe
2024-09-23 16:53:02,920:INFO:Initializing Quadratic Discriminant Analysis
2024-09-23 16:53:02,920:INFO:Total runtime is 0.23310441176096594 minutes
2024-09-23 16:53:02,922:INFO:SubProcess create_model() called ==================================
2024-09-23 16:53:02,922:INFO:Initializing create_model()
2024-09-23 16:53:02,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:53:02,922:INFO:Checking exceptions
2024-09-23 16:53:02,922:INFO:Importing libraries
2024-09-23 16:53:02,922:INFO:Copying training dataset
2024-09-23 16:53:02,953:INFO:Defining folds
2024-09-23 16:53:02,953:INFO:Declaring metric variables
2024-09-23 16:53:02,955:INFO:Importing untrained model
2024-09-23 16:53:02,956:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-23 16:53:02,959:INFO:Starting cross validation
2024-09-23 16:53:02,960:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:53:03,222:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,222:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,222:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:03,254:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,269:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,269:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:03,285:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,301:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,301:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,316:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,332:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,332:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,332:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:03,348:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,348:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,348:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:03,363:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,363:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-23 16:53:03,379:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,379:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,394:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:03,394:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:53:03,394:INFO:Calculating mean and std
2024-09-23 16:53:03,394:INFO:Creating metrics dataframe
2024-09-23 16:53:03,410:INFO:Uploading results into container
2024-09-23 16:53:03,410:INFO:Uploading model into container now
2024-09-23 16:53:03,410:INFO:_master_model_container: 8
2024-09-23 16:53:03,410:INFO:_display_container: 2
2024-09-23 16:53:03,410:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-23 16:53:03,410:INFO:create_model() successfully completed......................................
2024-09-23 16:53:03,465:INFO:SubProcess create_model() end ==================================
2024-09-23 16:53:03,465:INFO:Creating metrics dataframe
2024-09-23 16:53:03,483:INFO:Initializing Ada Boost Classifier
2024-09-23 16:53:03,483:INFO:Total runtime is 0.24248232841491696 minutes
2024-09-23 16:53:03,485:INFO:SubProcess create_model() called ==================================
2024-09-23 16:53:03,485:INFO:Initializing create_model()
2024-09-23 16:53:03,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:53:03,486:INFO:Checking exceptions
2024-09-23 16:53:03,486:INFO:Importing libraries
2024-09-23 16:53:03,486:INFO:Copying training dataset
2024-09-23 16:53:03,501:INFO:Defining folds
2024-09-23 16:53:03,501:INFO:Declaring metric variables
2024-09-23 16:53:03,501:INFO:Importing untrained model
2024-09-23 16:53:03,501:INFO:Ada Boost Classifier Imported successfully
2024-09-23 16:53:03,501:INFO:Starting cross validation
2024-09-23 16:53:03,501:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:53:03,768:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,800:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,800:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,831:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,846:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,846:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,862:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,878:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,909:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:03,909:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-23 16:53:05,667:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:05,836:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:05,882:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:05,897:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:05,929:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:05,947:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:05,969:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:06,030:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:06,108:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:06,154:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:53:06,163:INFO:Calculating mean and std
2024-09-23 16:53:06,163:INFO:Creating metrics dataframe
2024-09-23 16:53:06,165:INFO:Uploading results into container
2024-09-23 16:53:06,165:INFO:Uploading model into container now
2024-09-23 16:53:06,166:INFO:_master_model_container: 9
2024-09-23 16:53:06,166:INFO:_display_container: 2
2024-09-23 16:53:06,166:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-23 16:53:06,166:INFO:create_model() successfully completed......................................
2024-09-23 16:53:06,212:INFO:SubProcess create_model() end ==================================
2024-09-23 16:53:06,212:INFO:Creating metrics dataframe
2024-09-23 16:53:06,218:INFO:Initializing Gradient Boosting Classifier
2024-09-23 16:53:06,218:INFO:Total runtime is 0.2880693912506103 minutes
2024-09-23 16:53:06,220:INFO:SubProcess create_model() called ==================================
2024-09-23 16:53:06,220:INFO:Initializing create_model()
2024-09-23 16:53:06,220:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:53:06,220:INFO:Checking exceptions
2024-09-23 16:53:06,221:INFO:Importing libraries
2024-09-23 16:53:06,221:INFO:Copying training dataset
2024-09-23 16:53:06,234:INFO:Defining folds
2024-09-23 16:53:06,234:INFO:Declaring metric variables
2024-09-23 16:53:06,235:INFO:Importing untrained model
2024-09-23 16:53:06,236:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 16:53:06,236:INFO:Starting cross validation
2024-09-23 16:53:06,236:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:54:24,544:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:24,763:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:25,185:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:25,310:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:25,685:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:25,841:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:26,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,012:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,231:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,481:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,481:INFO:Calculating mean and std
2024-09-23 16:54:27,481:INFO:Creating metrics dataframe
2024-09-23 16:54:27,481:INFO:Uploading results into container
2024-09-23 16:54:27,481:INFO:Uploading model into container now
2024-09-23 16:54:27,481:INFO:_master_model_container: 10
2024-09-23 16:54:27,481:INFO:_display_container: 2
2024-09-23 16:54:27,497:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 16:54:27,497:INFO:create_model() successfully completed......................................
2024-09-23 16:54:27,558:INFO:SubProcess create_model() end ==================================
2024-09-23 16:54:27,558:INFO:Creating metrics dataframe
2024-09-23 16:54:27,563:INFO:Initializing Linear Discriminant Analysis
2024-09-23 16:54:27,564:INFO:Total runtime is 1.6438366969426472 minutes
2024-09-23 16:54:27,566:INFO:SubProcess create_model() called ==================================
2024-09-23 16:54:27,567:INFO:Initializing create_model()
2024-09-23 16:54:27,567:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:54:27,567:INFO:Checking exceptions
2024-09-23 16:54:27,567:INFO:Importing libraries
2024-09-23 16:54:27,567:INFO:Copying training dataset
2024-09-23 16:54:27,585:INFO:Defining folds
2024-09-23 16:54:27,585:INFO:Declaring metric variables
2024-09-23 16:54:27,585:INFO:Importing untrained model
2024-09-23 16:54:27,585:INFO:Linear Discriminant Analysis Imported successfully
2024-09-23 16:54:27,585:INFO:Starting cross validation
2024-09-23 16:54:27,585:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:54:27,867:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,883:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,883:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,914:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:27,974:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:28,030:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:28,030:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:28,045:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:28,061:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:28,076:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:54:28,076:INFO:Calculating mean and std
2024-09-23 16:54:28,076:INFO:Creating metrics dataframe
2024-09-23 16:54:28,076:INFO:Uploading results into container
2024-09-23 16:54:28,076:INFO:Uploading model into container now
2024-09-23 16:54:28,076:INFO:_master_model_container: 11
2024-09-23 16:54:28,076:INFO:_display_container: 2
2024-09-23 16:54:28,076:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-23 16:54:28,076:INFO:create_model() successfully completed......................................
2024-09-23 16:54:28,139:INFO:SubProcess create_model() end ==================================
2024-09-23 16:54:28,139:INFO:Creating metrics dataframe
2024-09-23 16:54:28,150:INFO:Initializing Extra Trees Classifier
2024-09-23 16:54:28,150:INFO:Total runtime is 1.653594692548116 minutes
2024-09-23 16:54:28,152:INFO:SubProcess create_model() called ==================================
2024-09-23 16:54:28,152:INFO:Initializing create_model()
2024-09-23 16:54:28,152:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:54:28,152:INFO:Checking exceptions
2024-09-23 16:54:28,153:INFO:Importing libraries
2024-09-23 16:54:28,153:INFO:Copying training dataset
2024-09-23 16:54:28,165:INFO:Defining folds
2024-09-23 16:54:28,165:INFO:Declaring metric variables
2024-09-23 16:54:28,167:INFO:Importing untrained model
2024-09-23 16:54:28,168:INFO:Extra Trees Classifier Imported successfully
2024-09-23 16:54:28,168:INFO:Starting cross validation
2024-09-23 16:54:28,168:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:54:32,810:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:54:32,877:INFO:Calculating mean and std
2024-09-23 16:54:32,882:INFO:Creating metrics dataframe
2024-09-23 16:54:32,885:INFO:Uploading results into container
2024-09-23 16:54:32,885:INFO:Uploading model into container now
2024-09-23 16:54:32,886:INFO:_master_model_container: 12
2024-09-23 16:54:32,886:INFO:_display_container: 2
2024-09-23 16:54:32,887:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-23 16:54:32,887:INFO:create_model() successfully completed......................................
2024-09-23 16:54:32,985:INFO:SubProcess create_model() end ==================================
2024-09-23 16:54:32,985:INFO:Creating metrics dataframe
2024-09-23 16:54:32,985:INFO:Initializing Extreme Gradient Boosting
2024-09-23 16:54:32,985:INFO:Total runtime is 1.7341817299524942 minutes
2024-09-23 16:54:33,000:INFO:SubProcess create_model() called ==================================
2024-09-23 16:54:33,000:INFO:Initializing create_model()
2024-09-23 16:54:33,000:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:54:33,001:INFO:Checking exceptions
2024-09-23 16:54:33,001:INFO:Importing libraries
2024-09-23 16:54:33,001:INFO:Copying training dataset
2024-09-23 16:54:33,019:INFO:Defining folds
2024-09-23 16:54:33,019:INFO:Declaring metric variables
2024-09-23 16:54:33,019:INFO:Importing untrained model
2024-09-23 16:54:33,033:INFO:Extreme Gradient Boosting Imported successfully
2024-09-23 16:54:33,037:INFO:Starting cross validation
2024-09-23 16:54:33,039:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:54:41,154:INFO:Calculating mean and std
2024-09-23 16:54:41,154:INFO:Creating metrics dataframe
2024-09-23 16:54:41,154:INFO:Uploading results into container
2024-09-23 16:54:41,154:INFO:Uploading model into container now
2024-09-23 16:54:41,154:INFO:_master_model_container: 13
2024-09-23 16:54:41,154:INFO:_display_container: 2
2024-09-23 16:54:41,154:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-23 16:54:41,154:INFO:create_model() successfully completed......................................
2024-09-23 16:54:41,201:INFO:SubProcess create_model() end ==================================
2024-09-23 16:54:41,201:INFO:Creating metrics dataframe
2024-09-23 16:54:41,216:INFO:Initializing Light Gradient Boosting Machine
2024-09-23 16:54:41,216:INFO:Total runtime is 1.871365948518117 minutes
2024-09-23 16:54:41,218:INFO:SubProcess create_model() called ==================================
2024-09-23 16:54:41,218:INFO:Initializing create_model()
2024-09-23 16:54:41,218:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:54:41,218:INFO:Checking exceptions
2024-09-23 16:54:41,218:INFO:Importing libraries
2024-09-23 16:54:41,218:INFO:Copying training dataset
2024-09-23 16:54:41,231:INFO:Defining folds
2024-09-23 16:54:41,231:INFO:Declaring metric variables
2024-09-23 16:54:41,234:INFO:Importing untrained model
2024-09-23 16:54:41,235:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-23 16:54:41,235:INFO:Starting cross validation
2024-09-23 16:54:41,235:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:55:01,918:INFO:Calculating mean and std
2024-09-23 16:55:01,918:INFO:Creating metrics dataframe
2024-09-23 16:55:01,918:INFO:Uploading results into container
2024-09-23 16:55:01,918:INFO:Uploading model into container now
2024-09-23 16:55:01,918:INFO:_master_model_container: 14
2024-09-23 16:55:01,934:INFO:_display_container: 2
2024-09-23 16:55:01,934:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-23 16:55:01,934:INFO:create_model() successfully completed......................................
2024-09-23 16:55:02,013:INFO:SubProcess create_model() end ==================================
2024-09-23 16:55:02,013:INFO:Creating metrics dataframe
2024-09-23 16:55:02,029:INFO:Initializing CatBoost Classifier
2024-09-23 16:55:02,029:INFO:Total runtime is 2.2182519833246865 minutes
2024-09-23 16:55:02,029:INFO:SubProcess create_model() called ==================================
2024-09-23 16:55:02,029:INFO:Initializing create_model()
2024-09-23 16:55:02,029:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:55:02,029:INFO:Checking exceptions
2024-09-23 16:55:02,029:INFO:Importing libraries
2024-09-23 16:55:02,029:INFO:Copying training dataset
2024-09-23 16:55:02,066:INFO:Defining folds
2024-09-23 16:55:02,066:INFO:Declaring metric variables
2024-09-23 16:55:02,066:INFO:Importing untrained model
2024-09-23 16:55:02,066:INFO:CatBoost Classifier Imported successfully
2024-09-23 16:55:02,066:INFO:Starting cross validation
2024-09-23 16:55:02,066:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:56:57,686:INFO:Calculating mean and std
2024-09-23 16:56:57,686:INFO:Creating metrics dataframe
2024-09-23 16:56:57,686:INFO:Uploading results into container
2024-09-23 16:56:57,686:INFO:Uploading model into container now
2024-09-23 16:56:57,686:INFO:_master_model_container: 15
2024-09-23 16:56:57,686:INFO:_display_container: 2
2024-09-23 16:56:57,686:INFO:<catboost.core.CatBoostClassifier object at 0x0000020CE304D390>
2024-09-23 16:56:57,686:INFO:create_model() successfully completed......................................
2024-09-23 16:56:57,747:INFO:SubProcess create_model() end ==================================
2024-09-23 16:56:57,747:INFO:Creating metrics dataframe
2024-09-23 16:56:57,747:INFO:Initializing Dummy Classifier
2024-09-23 16:56:57,747:INFO:Total runtime is 4.146889698505402 minutes
2024-09-23 16:56:57,747:INFO:SubProcess create_model() called ==================================
2024-09-23 16:56:57,747:INFO:Initializing create_model()
2024-09-23 16:56:57,747:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000020CE2FF4250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:56:57,747:INFO:Checking exceptions
2024-09-23 16:56:57,747:INFO:Importing libraries
2024-09-23 16:56:57,747:INFO:Copying training dataset
2024-09-23 16:56:57,784:INFO:Defining folds
2024-09-23 16:56:57,784:INFO:Declaring metric variables
2024-09-23 16:56:57,784:INFO:Importing untrained model
2024-09-23 16:56:57,784:INFO:Dummy Classifier Imported successfully
2024-09-23 16:56:57,784:INFO:Starting cross validation
2024-09-23 16:56:57,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:56:58,169:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,200:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,262:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,262:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,278:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,294:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,309:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,325:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,325:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,340:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-23 16:56:58,340:INFO:Calculating mean and std
2024-09-23 16:56:58,340:INFO:Creating metrics dataframe
2024-09-23 16:56:58,356:INFO:Uploading results into container
2024-09-23 16:56:58,356:INFO:Uploading model into container now
2024-09-23 16:56:58,356:INFO:_master_model_container: 16
2024-09-23 16:56:58,358:INFO:_display_container: 2
2024-09-23 16:56:58,358:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-23 16:56:58,358:INFO:create_model() successfully completed......................................
2024-09-23 16:56:58,413:INFO:SubProcess create_model() end ==================================
2024-09-23 16:56:58,413:INFO:Creating metrics dataframe
2024-09-23 16:56:58,432:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-23 16:56:58,437:INFO:Initializing create_model()
2024-09-23 16:56:58,437:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:56:58,437:INFO:Checking exceptions
2024-09-23 16:56:58,438:INFO:Importing libraries
2024-09-23 16:56:58,438:INFO:Copying training dataset
2024-09-23 16:56:58,450:INFO:Defining folds
2024-09-23 16:56:58,450:INFO:Declaring metric variables
2024-09-23 16:56:58,450:INFO:Importing untrained model
2024-09-23 16:56:58,450:INFO:Declaring custom model
2024-09-23 16:56:58,450:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 16:56:58,450:INFO:Cross validation set to False
2024-09-23 16:56:58,450:INFO:Fitting Model
2024-09-23 16:57:56,648:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 16:57:56,648:INFO:create_model() successfully completed......................................
2024-09-23 16:57:56,717:INFO:_master_model_container: 16
2024-09-23 16:57:56,717:INFO:_display_container: 2
2024-09-23 16:57:56,717:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 16:57:56,717:INFO:compare_models() successfully completed......................................
2024-09-23 16:58:13,526:INFO:Initializing create_model()
2024-09-23 16:58:13,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-23 16:58:13,526:INFO:Checking exceptions
2024-09-23 16:58:13,534:INFO:Importing libraries
2024-09-23 16:58:13,534:INFO:Copying training dataset
2024-09-23 16:58:13,557:INFO:Defining folds
2024-09-23 16:58:13,557:INFO:Declaring metric variables
2024-09-23 16:58:13,557:INFO:Importing untrained model
2024-09-23 16:58:13,557:INFO:Declaring custom model
2024-09-23 16:58:13,557:INFO:Gradient Boosting Classifier Imported successfully
2024-09-23 16:58:13,571:INFO:Starting cross validation
2024-09-23 16:58:13,572:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-23 16:59:24,193:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:25,334:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:25,427:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:25,927:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:26,021:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:26,240:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:26,271:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:26,411:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:27,240:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:28,614:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py:196: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 188, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\pycaret\internal\metrics.py", line 136, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-23 16:59:28,630:INFO:Calculating mean and std
2024-09-23 16:59:28,630:INFO:Creating metrics dataframe
2024-09-23 16:59:28,630:INFO:Finalizing model
2024-09-23 17:00:29,924:INFO:Uploading results into container
2024-09-23 17:00:29,925:INFO:Uploading model into container now
2024-09-23 17:00:29,930:INFO:_master_model_container: 17
2024-09-23 17:00:29,930:INFO:_display_container: 3
2024-09-23 17:00:29,930:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-23 17:00:29,931:INFO:create_model() successfully completed......................................
2024-09-23 17:00:57,337:INFO:Initializing tune_model()
2024-09-23 17:00:57,337:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-09-23 17:00:57,337:INFO:Checking exceptions
2024-09-23 17:00:57,350:INFO:Copying training dataset
2024-09-23 17:00:57,366:INFO:Checking base model
2024-09-23 17:00:57,366:INFO:Base model : Gradient Boosting Classifier
2024-09-23 17:00:57,381:INFO:Declaring metric variables
2024-09-23 17:00:57,383:INFO:Defining Hyperparameters
2024-09-23 17:00:57,448:INFO:Tuning with n_jobs=-1
2024-09-23 17:00:57,448:INFO:Initializing RandomizedSearchCV
2024-09-23 17:10:02,284:INFO:Initializing evaluate_model()
2024-09-23 17:10:02,284:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 17:10:02,315:INFO:Initializing plot_model()
2024-09-23 17:10:02,315:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:10:02,315:INFO:Checking exceptions
2024-09-23 17:10:02,320:INFO:Preloading libraries
2024-09-23 17:10:02,368:INFO:Copying training dataset
2024-09-23 17:10:02,368:INFO:Plot type: pipeline
2024-09-23 17:10:02,524:INFO:Visual Rendered Successfully
2024-09-23 17:10:02,611:INFO:plot_model() successfully completed......................................
2024-09-23 17:10:05,827:INFO:Initializing plot_model()
2024-09-23 17:10:05,827:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=class_report, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:10:05,827:INFO:Checking exceptions
2024-09-23 17:10:05,836:INFO:Preloading libraries
2024-09-23 17:10:05,891:INFO:Copying training dataset
2024-09-23 17:10:05,891:INFO:Plot type: class_report
2024-09-23 17:10:06,001:INFO:Fitting Model
2024-09-23 17:10:06,001:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 17:10:06,001:INFO:Scoring test/hold-out set
2024-09-23 17:10:06,263:INFO:Visual Rendered Successfully
2024-09-23 17:10:06,341:INFO:plot_model() successfully completed......................................
2024-09-23 17:10:46,683:INFO:Initializing plot_model()
2024-09-23 17:10:46,683:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:10:46,683:INFO:Checking exceptions
2024-09-23 17:10:47,641:INFO:Initializing plot_model()
2024-09-23 17:10:47,641:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:10:47,641:INFO:Checking exceptions
2024-09-23 17:10:47,651:INFO:Preloading libraries
2024-09-23 17:10:47,686:INFO:Copying training dataset
2024-09-23 17:10:47,687:INFO:Plot type: learning
2024-09-23 17:10:47,779:INFO:Fitting Model
2024-09-23 17:11:02,946:INFO:Initializing plot_model()
2024-09-23 17:11:02,946:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:11:02,946:INFO:Checking exceptions
2024-09-23 17:11:13,900:INFO:Initializing plot_model()
2024-09-23 17:11:13,900:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:11:13,900:INFO:Checking exceptions
2024-09-23 17:11:14,055:INFO:Preloading libraries
2024-09-23 17:11:14,069:INFO:Copying training dataset
2024-09-23 17:11:14,069:INFO:Plot type: feature
2024-09-23 17:11:14,069:WARNING:No coef_ found. Trying feature_importances_
2024-09-23 17:11:14,201:INFO:Visual Rendered Successfully
2024-09-23 17:11:14,302:INFO:plot_model() successfully completed......................................
2024-09-23 17:32:22,182:INFO:Initializing evaluate_model()
2024-09-23 17:32:22,182:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181B85D2990>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 17:35:07,915:INFO:Initializing evaluate_model()
2024-09-23 17:35:07,915:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181B85D2990>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 17:35:40,618:INFO:Initializing evaluate_model()
2024-09-23 17:35:40,618:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181B85D2990>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 17:39:22,916:INFO:Initializing evaluate_model()
2024-09-23 17:39:22,917:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 17:39:22,933:INFO:Initializing plot_model()
2024-09-23 17:39:22,933:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:39:22,933:INFO:Checking exceptions
2024-09-23 17:39:22,948:INFO:Preloading libraries
2024-09-23 17:39:22,989:INFO:Copying training dataset
2024-09-23 17:39:22,989:INFO:Plot type: pipeline
2024-09-23 17:39:23,047:INFO:Visual Rendered Successfully
2024-09-23 17:39:23,228:INFO:plot_model() successfully completed......................................
2024-09-23 17:39:26,435:INFO:Initializing plot_model()
2024-09-23 17:39:26,435:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020CE06CA190>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-23 17:39:26,435:INFO:Checking exceptions
2024-09-23 17:39:26,451:INFO:Preloading libraries
2024-09-23 17:39:26,498:INFO:Copying training dataset
2024-09-23 17:39:26,498:INFO:Plot type: confusion_matrix
2024-09-23 17:39:26,628:INFO:Fitting Model
2024-09-23 17:39:26,629:WARNING:C:\Users\ICT\anaconda3\envs\my_env\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names
  warnings.warn(

2024-09-23 17:39:26,629:INFO:Scoring test/hold-out set
2024-09-23 17:39:26,863:INFO:Visual Rendered Successfully
2024-09-23 17:39:27,067:INFO:plot_model() successfully completed......................................
2024-09-23 17:40:12,837:INFO:Initializing evaluate_model()
2024-09-23 17:40:12,837:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181B85D2990>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-23 17:40:33,618:INFO:Initializing evaluate_model()
2024-09-23 17:40:33,618:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000181B85D2990>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
